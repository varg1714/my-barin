### 2.4. Redis 实现分布式锁

#### 2.4.1. 基于单节点的锁

- 使用 `setnx` / `del` 添加和释放锁，可带生效时间设定。
- 删除锁时判断线程 ID 是否是自己，防止误删除。
- 重入性使用 state 值来判断。
- 判断锁释放可以通过轮询或者基于 Redis 的发布订阅机制实现。

`Redisson` 使用的部分 Redis 命令如下：

```lua
# 加锁：keys[1] = 原始锁名称，argv[1] = 锁持有时间，argv[2] = 带线程ID的key
if ((redis.call('exists', KEYS[1]) == 0) or (redis.call('hexists', KEYS[1], ARGV[2]) == 1))
    then
	    redis.call('hincrby', KEYS[1], ARGV[2], 1);
	    redis.call('pexpire', KEYS[1], ARGV[1]);
	    return nil;
   end;
return redis.call('pttl', KEYS[1]);
```

#### 2.4.2. 基于多节点的锁-RedLock

为了避免 Redis 实例故障而导致的锁无法工作的问题，Redis 的开发者 Antirez 提出了分布式锁算法 Redlock。

Redlock 算法的基本思路是让客户端和多个独立的 Redis 实例依次请求加锁，**如果客户端能够和半数以上的实例成功地完成加锁操作**，那么我们就认为，客户端成功地获得分布式锁了；否则加锁失败。这样一来，即使有单个 Redis 实例发生故障，因为锁变量在其它实例上也有保存，所以，客户端仍然可以正常地进行锁操作，锁变量并不会丢失。

RedLock 的加锁步骤如下：

1. 客户端获取当前时间。
2. 客户端按顺序依次向 N 个 Redis 实例执行加锁操作。
    这里的加锁操作和在单实例上执行的加锁操作一样，使用 `SET` 命令，带上 `NX`，`EX` / `PX` 选项，以及带上客户端的唯一标识。

    当然，如果某个 Redis 实例发生故障了，为了保证在这种情况下 Redlock 算法能够继续运行，我们需要给加锁操作设置一个超时时间。如果客户端在和一个 Redis 实例请求加锁时，一直到超时都没有成功，那么此时，客户端会和下一个 Redis 实例继续请求加锁。加锁操作的超时时间需要远远地小于锁的有效时间，一般也就是设置为几十毫秒。

3. 一旦客户端完成了和所有 Redis 实例的加锁操作，客户端就要计算整个加锁过程的总耗时。

客户端只有在满足下面的这两个条件时，才能认为是加锁成功：

1. 客户端从超过半数（ `>= N/2+1`）的 Redis 实例上成功获取到了锁。
2. 客户端获取锁的总耗时没有超过锁的有效时间。

在满足了这两个条件后，需要重新计算这把锁的有效时间，计算的结果是锁的最初有效时间减去客户端为获取锁的总耗时。如果锁的有效时间已经来不及完成共享数据的操作了，我们可以释放锁，以免出现还没完成数据操作锁就过期了的情况。当然，如果客户端在和所有实例执行完加锁操作后没能同时满足这两个条件，那么客户端向所有 Redis 节点发起释放锁的操作。

在 Redlock 算法中，释放锁的操作和在单实例上释放锁的操作一样，只要执行释放锁的 Lua 脚本就可以了。释放锁时，要对所有节点释放（即使某个节点加锁失败了），因为加锁时可能发生服务端加锁成功，由于网络问题，给客户端回复网络包失败的情况，所以需要把所有节点可能存的锁都释放掉。这样一来，只要 N 个 Redis 实例中的半数以上实例能正常工作，就能保证分布式锁的正常工作了。

#### 2.4.3. RedLock 的思考

##### 2.4.3.1. RedLock 失效场景之节点崩溃

假设一共有 5 个 Redis 节点：A, B, C, D, E。设想发生了如下的事件序列：

1. 客户端 1 成功锁住了 A, B, C，获取锁成功（但 D 和 E 没有锁住）。
2. 节点 C 崩溃重启了，但客户端 1 在 C 上加的锁没有持久化下来，丢失了。
3. 节点 C 重启后，客户端 2 锁住了 C, D, E，获取锁成功。

这样，客户端 1 和客户端 2 同时获得了锁（针对同一资源）。

针对这一情况，可以采取**延迟重启**（delayed restarts）的概念。也就是说，一个节点崩溃后，先不立即重启它，而是等待一段时间再重启，这段时间应该大于锁的有效时间 (lock validity time)。这样的话，这个节点在重启前所参与的锁都会过期，它在重启后就不会对现有的锁造成影响。

但是这个延迟时间需要根据锁的时间来控制，这个通常来说不可控，且延迟重启对集群也会有影响。

##### 2.4.3.2. RedLock 失效场景之时钟跳跃

Martin Kleppmann 在 2016-02-08 这一天发表了一篇 blog，名字叫” [How to do distributed locking ](https://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html) “，Martin 在这篇文章中谈及了分布式系统的很多基础性的问题（特别是分布式计算的异步模型），对分布式系统的从业者来说非常值得一读。其中部分篇幅是对 Redlock 本身的批评。Martin 指出，由于 Redlock 本质上是建立在一个同步模型之上，对系统的记时假设 (timing assumption) 有很强的要求，因此本身的安全性是不够的。Redis 的作者对此进行了[回复](http://antirez.com/news/101)，双方对此进行了许多探讨，许多人也参与到这个探讨中。

在失效场景之节点崩溃中，我们设想了节点发生崩溃未持久化数据而导致的锁失效问题。这是由于节点数据丢失导致的，还有另一种情况会产生类似的效果：时钟跳跃导致锁提前失效。

如果节点 C 上的时钟发生了向前跳跃，导致它上面维护的锁快速过期，那么另一个客户端就能获取到锁，从而导致互斥资源保护锁失效。

针对这一情况，就需要我们确保[[数据密集型系统设计4：分布式的挑战#3 不可靠的时钟|时钟混乱]] 的场景不会发生。

##### 2.4.3.3. RedLock 失效场景之客户端延迟

**即使我们拥有一个完美实现的分布式锁（带自动过期功能），在没有共享资源参与进来提供某种 fencing 机制的前提下，我们仍然不可能获得足够的安全性。**

![](https://r2.129870.xyz/img/20220521192441.png)

在上面的时序图中，假设锁服务本身是没有问题的，它总是能保证任一时刻最多只有一个客户端获得锁。上图中出现的 lease 这个词可以暂且认为就等同于一个带有自动过期功能的锁。

假设这样一个场景：客户端 1 在获得锁之后发生了很长时间的 GC pause，在此期间，它获得的锁过期了，而客户端 2 获得了锁。当客户端 1 从 GC pause 中恢复过来的时候，它不知道自己持有的锁已经过期了，它依然向共享资源（上图中是一个存储服务）发起了写数据请求，而这时锁实际上被客户端 2 持有，因此两个客户端的写请求就有可能冲突（锁的互斥作用失效了）。

对于 GC 来说是一种客户端延迟，**系统环境太复杂，仍然有很多原因导致进程的 pause**。比如虚存造成的缺页故障 (page fault)，再比如 CPU 资源的竞争。即使不考虑进程 pause 的情况，网络延迟也仍然会造成类似的结果。

Martin 给出了一种方法，称为 `fencing token`。`fencing token` 是一个单调递增的数字，当客户端成功获取锁的时候它随同锁一起返回给客户端。而客户端访问共享资源的时候带着这个 `fencing token`，这样提供共享资源的服务就能根据它进行检查，拒绝掉延迟到来的访问请求（避免了冲突）。

![](https://r2.129870.xyz/img/20220521192853.png)

在上图中，客户端 1 先获取到的锁，因此有一个较小的 fencing token，等于 33，而客户端 2 后获取到的锁，有一个较大的 fencing token，等于 34。客户端 1 从 GC pause 中恢复过来之后，依然是向存储服务发送访问请求，但是带了 fencing token = 33。存储服务发现它之前已经处理过 34 的请求，所以会拒绝掉这次 33 的请求。这样就避免了冲突。

然而由于大多数资源本身不具备这种拒绝的能力，所以在实现上一种简化的设计思路是在 DB 层通过版本号的方式来更新数据，避免并发冲突。

##### 2.4.3.4. 基于 Zookeeper 的锁安全吗？

很多人（也包括 Martin 在内）都认为，如果你想构建一个更安全的分布式锁，那么应该使用 ZooKeeper，而不是 Redis。基于 ZooKeeper 的分布式锁能提供绝对的安全吗？它需要 `fencing token` 机制的保护吗？我们不得不提一下分布式专家 [Flavio Junqueira](https://fpj.me/) 所写的一篇 blog，题目叫 [“Note on fencing and distributed locks”](https://fpj.me/2016/02/10/note-on-fencing-and-distributed-locks/)。

Flavio Junqueira 是 ZooKeeper 的作者之一，他的这篇 blog 就写在 Martin 和 antirez 发生争论的那几天。他在文中给出了一个基于 ZooKeeper 构建分布式锁的描述（当然这不是唯一的方式），需要注意下面的例子并非 Zookeeper 实现分布式锁的最佳示例，这会出现[羊群效应](https://developer.aliyun.com/article/427024) ：

1. 客户端尝试创建一个 `znode` 节点，比如 `/lock`。那么第一个客户端就创建成功了，相当于拿到了锁；而其它的客户端会创建失败（`znode` 已存在），获取锁失败。
2. 持有锁的客户端访问共享资源完成后，将 `znode` 删掉，这样其它客户端接下来就能来获取锁了。
3. `znode` 应该被创建成 `ephemeral` 的。这是 `znode` 的一个特性，它保证如果创建 `znode` 的那个客户端崩溃了，那么相应的 `znode` 会被自动删除。这保证了锁一定会被释放。

看起来这个锁相当完美，没有 Redlock 过期时间的问题，而且能在需要的时候让锁自动释放。但仔细考察的话，并不尽然。

ZooKeeper 是怎么检测出某个客户端已经崩溃了呢？实际上，每个客户端都与 ZooKeeper 的某台服务器维护着一个 Session，这个 Session 依赖定期的心跳 (heartbeat) 来维持。如果 ZooKeeper 长时间收不到客户端的心跳（这个时间称为 Sesion 的过期时间），那么它就认为 Session 过期了，通过这个 Session 所创建的所有的 ephemeral 类型的 znode 节点都会被自动删除。

设想如下的执行序列：

1. 客户端 1 创建了 znode 节点 `/lock`，获得了锁。
2. 客户端 1 进入了长时间的 GC pause。
3. 客户端 1 连接到 ZooKeeper 的 Session 过期了。znode 节点 `/lock` 被自动删除。
4. 客户端 2 创建了 znode 节点 `/lock`，从而获得了锁。
5. 客户端 1 从 GC pause 中恢复过来，它仍然认为自己持有锁。

最后，客户端 1 和客户端 2 都认为自己持有了锁，冲突了。

这与之前 Martin 在文章中描述的由于 GC pause 导致的分布式锁失效的情况类似。虽然用 ZooKeeper 实现的分布式锁也不一定就是安全的。该有的问题它还是有。但是，ZooKeeper 作为一个专门为分布式应用提供方案的框架，它提供了一些非常好的特性，是 Redis 之类的方案所没有的。像前面提到的 ephemeral 类型的 znode 自动删除的功能与 watch 机制等。

所以我们再次强调一遍，**即使我们拥有一个完美实现的分布式锁（带自动过期功能），在没有共享资源参与进来提供某种 fencing 机制的前提下，我们仍然不可能获得足够的安全性。**