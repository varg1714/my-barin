![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220427132142.png)

# 1. 自动内存管理

## 1.1. 内存区域

### 1.1.1. 运行时内存区域

Java 虚拟机在执行 Java 程序的过程中会把它所管理的内存划分为若干个不同的数据区域：

![image.png](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202308130130790.png)

#### 1.1.1.1. 程序计数器

程序计数器（Program Counter Register）是一块较小的内存空间，它可以看作是当前线程所执行的字节码的行号指示器。在 Java 虚拟机的概念模型里，字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令，它是程序控制流的指示器，分支、循环、跳转、异常处理、线程恢复等基础功能都需要依赖这个计数器来完成。

由于 Java 虚拟机的多线程是通过线程轮流切换、分配处理器执行时间的方式来实现的，在任何一个确定的时刻，一个处理器（对于多核处理器来说是一个内核）都只会执行一条线程中的指令。因此，**为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个独立的程序计数器**，各条线程之间计数器互不影响，独立存储，我们称这类内存区域为“线程私有”的内存。

如果线程正在执行的是一个 Java 方法，这个计数器记录的是正在执行的虚拟机字节码指令的地址；如果正在执行的是本地（Native）方法，这个计数器值则应为空（Undefined）。此内存区域是唯一一个在《Java 虚拟机规范》中没有规定任何 OutOfMemoryError 情况的区域。

#### 1.1.1.2. Java 虚拟机栈

Java 虚拟机栈（Java Virtual Machine Stack）也是线程私有的，它的生命周期与线程相同。虚拟机栈描述的是 Java 方法执行的线程内存模型：**每个方法被执行的时候，Java 虚拟机都会同步创建一个栈帧 （Stack Frame）用于存储局部变量表、操作数栈、动态连接、方法出口等信息**。每一个方法被调用直至执行完毕的过程，就对应着一个栈帧在虚拟机栈中从入栈到出栈的过程。

局部变量表存放了编译期可知的各种 Java 虚拟机基本数据类型（boolean、byte、char、short、int、 float、long、double）、对象引用（reference 类型，它并不等同于对象本身，可能是一个指向对象起始地址的引用指针，也可能是指向一个代表对象的句柄或者其他与此对象相关的位置）和 returnAddress 类型（指向了一条字节码指令的地址）。

这些数据类型在局部变量表中的存储空间以局部变量槽（Slot）来表示，其中 64 位长度的 long 和 double 类型的数据会占用两个变量槽，其余的数据类型只占用一个。**局部变量表所需的内存空间在编译期间完成分配，当进入一个方法时，这个方法需要在栈帧中分配多大的局部变量空间是完全确定的**，在方法运行期间不会改变局部变量表的大小。这里说的**“大小”是指变量槽的数量**，虚拟机真正使用多大的内存空间（譬如按照 1 个变量槽占用 32 个比特、64 个比特，或者更多）来实现一个变量槽，这是完全由具体的虚拟机实现自行决定的事情。

在《Java 虚拟机规范》中，对这个内存区域规定了两类异常状况：
- 如果线程请求的栈深度大于虚拟机所允许的深度，将抛出 StackOverflowError 异常。
- 如果 Java 虚拟机栈容量可以动态扩展展时无法申请到足够的内存会抛出 OutOfMemoryError 异常。

#### 1.1.1.3. 本地方法栈

本地方法栈（Native Method Stacks）与虚拟机栈所发挥的作用是非常相似的，其区别只是虚拟机栈为虚拟机执行 Java 方法（也就是字节码）服务，而本地方法栈则是为虚拟机使用到的本地（Native） 方法服务。

《Java 虚拟机规范》对本地方法栈中方法使用的语言、使用方式与数据结构并没有任何强制规定，因此具体的虚拟机可以根据需要自由实现它，甚至有的 Java 虚拟机（譬如 Hot-Spot 虚拟机）直接就把本地方法栈和虚拟机栈合二为一。与虚拟机栈一样，本地方法栈也会在栈深度溢出或者栈扩展失败时分别抛出 StackOverflowError 和 OutOfMemoryError 异常。

#### 1.1.1.4. Java 堆

Java 堆是被所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例，Java 世界里**“几乎”所有的对象实例都在堆分配内存**。在《Java 虚拟机规范》中对 Java 堆的描述是：“所有的对象实例以及数组都应当在堆上分配 ”，而这里“几乎”是指从实现角度来看，随着 Java 语言的发展，现在已经能看到些许迹象表明日后可能出现值类型的支持，即使只考虑现在，**由于即时编译技术的进步，尤其是逃逸分析技术的日渐强大，栈上分配、标量替换优化手段已经导致一些微妙的变化悄然发生**，所以说 Java 对象实例都分配在堆上也渐渐变得不是那么绝对了。

如果从分配内存的角度看，所有线程共享的 Java 堆中可以划分出多个线程私有的分配缓冲区 （Thread Local Allocation Buffer，TLAB），以提升对象分配时的效率。不过无论从什么角度，无论如何划分，都不会改变 Java 堆中存储内容的共性，无论是哪个区域，存储的都只能是对象的实例，**将 Java 堆细分的目的只是为了更好地回收内存，或者更快地分配内存**。

根据《Java 虚拟机规范》的规定，Java 堆可以处于物理上不连续的内存空间中，但在逻辑上它应该被视为连续的，这点就像我们用磁盘空间去存储文件一样，并不要求每个文件都连续存放。但对于大对象（典型的如数组对象），多数虚拟机实现出于实现简单、存储高效的考虑，很可能会要求连续的内存空间。

Java 堆既可以被实现成固定大小的，也可以是可扩展的，不过当前主流的 Java 虚拟机都是按照可扩展来实现的（通过参数-Xmx 和-Xms 设定）。如果在 Java 堆中没有内存完成实例分配，并且堆也无法再扩展时，Java 虚拟机将会抛出 OutOfMemoryError 异常。

#### 1.1.1.5. 方法区

方法区（Method Area）与 Java 堆一样，是各个线程共享的内存区域，它用于**存储已被虚拟机加载的类型信息、常量、静态变量、即时编译器编译后的代码缓存等数据**。

《Java 虚拟机规范》对方法区的约束是非常宽松的，除了和 Java 堆一样不需要连续的内存和可以选择固定大小或者可扩展外，甚至还可以选择不实现垃圾收集。相对而言，垃圾收集行为在这个区域的确是比较少出现的，但并非数据进入了方法区就如永久代的名字一样“永久”存在了。**方法区的内存回收目标主要是针对常量池的回收和对类型的卸载，一般来说这个区域的回收效果比较难令人满意，尤其是类型的卸载，条件相当苛刻，但是这部分区域的回收有时又确实是必要的**。以前 Sun 公司的 Bug 列表中，曾出现过的若干个严重的 Bug 就是由于低版本的 HotSpot 虚拟机对此区域未完全回收而导致内存泄漏。

根据《Java 虚拟机规范》的规定，如果方法区无法满足新的内存分配需求时，将抛出 OutOfMemoryError 异常。

#### 1.1.1.6. 运行时常量池

运行时常量池（Runtime Constant Pool）是方法区的一部分。Class 文件中除了有类的版本、字段、方法、接口等描述信息外，还有一项信息是常量池表（Constant Pool Table），用于**存放编译期生成的各种字面量与符号引用，这部分内容将在类加载后存放到方法区的运行时常量池中**。

Java 虚拟机对于 Class 文件每一部分（自然也包括常量池）的格式都有严格规定，如每一个字节用于存储哪种数据都必须符合规范上的要求才会被虚拟机认可、加载和执行，但对于运行时常量池，《Java 虚拟机规范》并没有做任何细节的要求，不同提供商实现的虚拟机可以按照自己的需要来实现这个内存区域，不过一般来说，除了保存 Class 文件中描述的符号引用外，还会把由符号引用翻译出来的直接引用也存储在运行时常量池中。

运行时常量池相对于 Class 文件常量池的另外一个重要特征是具备动态性，Java 语言并不要求常量一定只有编译期才能产生，也就是说，并非预置入 Class 文件中常量池的内容才能进入方法区运行时常量池，**运行期间也可以将新的常量放入池中**，这种特性被开发人员利用得比较多的便是 String 类的 intern ()方法或者直接赋值如 `String a = "hehe";`。

既然运行时常量池是方法区的一部分，自然受到方法区内存的限制，当常量池无法再申请到内存时会抛出 OutOfMemoryError 异常。

#### 1.1.1.7. 直接内存

在 JDK 1.4 中新加入了 NIO（New Input/Output）类，引入了一种基于通道（Channel）与缓冲区 （Buffer）的 I/O 方式，它可以使用 Native 函数库直接分配堆外内存，然后通过一个存储在 Java 堆里面的 DirectByteBuffer 对象作为这块内存的引用进行操作。这样能在一些场景中显著提高性能，因为避免了在 Java 堆和 Native 堆中来回复制数据。

显然，**本机直接内存的分配不会受到 Java 堆大小的限制**，但是，既然是内存，则肯定还是会受到本机总内存（包括物理内存、SWAP 分区或者分页文件）大小以及处理器寻址空间的限制。一般服务器管理员配置虚拟机参数时，会根据实际内存去设置-Xmx 等参数信息，但经常忽略掉直接内存，使得各个内存区域总和大于物理内存限制（包括物理的和操作系统级的限制），从而导致动态扩展时出现 OutOfMemoryError 异常。

### 1.1.2. 内存分配与布局

#### 1.1.2.1. 内存对象的创建

##### 1.1.2.1.1. 分配内存

当 Java 虚拟机遇到一条字节码 new 指令时，首先将去检查这个指令的参数**是否能在常量池中定位到一个类的符号引用，并且检查这个符号引用代表的类是否已被加载、解析和初始化**过。如果没有，那必须先执行相应的类加载过程。

在类加载检查通过后，接下来虚拟机将为新生对象分配内存。对象所需内存的大小在类加载完成后便可完全确定，为对象分配空间的任务实际上便等同于把一块确定大小的内存块从 Java 堆中划分出来。

对于对象的内存分配有两种方式：
- 指针碰撞
    假设 Java 堆中内存是绝对规整的，所有被使用过的内存都被放在一边，空闲的内存被放在另一边，中间放着一个指针作为分界点的指示器，那所分配内存就仅仅是把那个指针向空闲空间方向挪动一段与对象大小相等的距离，这种分配方式称为“指针碰撞”（Bump The Pointer）。
- 空闲列表
    但如果 Java 堆中的内存并不是规整的，已被使用的内存和空闲的内存相互交错在一起，那就没有办法简单地进行指针碰撞了，虚拟机就必须维护一个列表，记录上哪些内存块是可用的，在分配的时候从列表中找到一块足够大的空间划分给对象实例，并更新列表上的记录，这种分配方式称为“空闲列表”（Free List）。

**选择哪种分配方式由 Java 堆是否规整决定**，而 Java 堆是否规整又由所采用的垃圾收集器是否带有空间压缩整理（Compact）的能力决定。因此，当使用 Serial、ParNew 等带压缩整理过程的收集器时，系统采用的分配算法是指针碰撞，既简单又高效；而当使用 CMS 这种基于清除（Sweep）算法的收集器时，理论上就只能采用较为复杂的空闲列表来分配内存。

> [!tip] CMS 的内存分布
> 
> 为在 CMS 的实现里面，为了能在多数情况下分配得更快，设计了一个叫作 Linear Allocation Buffer 的分配缓冲区，通过空闲列表拿到一大块分配缓冲区之后，在它里面仍然可以使用指针碰撞方式来分配。

除如何划分可用空间之外，还有另外一个需要考虑的问题：对象创建在虚拟机中是非常频繁的行为，即使仅仅**修改一个指针所指向的位置，在并发情况下也并不是线程安全的**，可能出现正在给对象 A 分配内存，指针还没来得及修改，对象 B 又同时使用了原来的指针来分配内存的情况。

解决这个问题有两种可选方案：
- 对分配内存空间的动作进行同步处理
    实际上虚拟机是采用 CAS 配上失败重试的方式保证更新操作的原子性。
- 把内存分配的动作按照线程划分在不同的空间之中进行
    每个线程在 Java 堆中预先分配一小块内存，称为本地线程分配缓冲（Thread Local Allocation Buffer，TLAB），哪个线程要分配内存，就在哪个线程的本地缓冲区中分配，只有本地缓冲区用完了，分配新的缓存区时才需要同步锁定。虚拟机是否使用 TLAB，可以通过 `-XX:+/-UseTLAB` 参数来设定。

##### 1.1.2.1.2. 初始化对象

对象的初始化可以分为以下三步：
1. 内存空间初始化为零值
    内存分配完成之后，虚拟机必须将分配到的内存空间（但不包括对象头）都初始化为零值，如果使用了 TLAB 的话，这一项工作也可以提前至 TLAB 分配时顺便进行。这步操作**保证了对象的实例字段在 Java 代码中可以不赋初始值就直接使用，使程序能访问到这些字段的数据类型所对应的零值**。
2. 初始化对象头信息
    接下来，Java 虚拟机还要对对象进行必要的设置，例如这个对象是哪个类的实例、如何才能找到类的元数据信息、对象的哈希码（实际上**对象的哈希码会延后到真正调用 `Object::hashCode()` 方法时才计算**）、对象的 GC 分代年龄等信息。这些信息存放在对象的对象头（Object Header）之中。根据虚拟机当前运行状态的不同，如是否启用偏向锁等，对象头会有不同的设置方式。
3. 调用构造方法
    在上面工作都完成之后，从虚拟机的视角来看，一个新的对象已经产生了。但是从 Java 程序的视角看来，对象创建才刚刚开始——构造函数，即 Class 文件中的 `<init>()` 方法还没有执行，所有的字段都为默认的零值，对象需要的其他资源和状态信息也还没有按照预定的意图构造好。
    
    一般来说（由字节码流中 new 指令后面是否跟随 invokespecial 指令所决定，Java 编译器会在遇到 new 关键字的地方同时生成这两条字节码指令，但如果直接通过其他方式产生的则不一定如此），new 指令之后会接着执行 `<init> ()` 方法，按照程序员的意愿对对象进行初始化，这样一个真正可用的对象才算完全被构造出来。

#### 1.1.2.2. 对象的内存布局

在 HotSpot 虚拟机里，对象在堆内存中的存储布局可以划分为三个部分：对象头（Header）、实例数据（Instance Data）和对齐填充（Padding）。

![image.png](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202308132241568.png)

##### 1.1.2.2.1. 对象头

HotSpot 虚拟机对象的对象头部分包括两类信息：
- 用于存储对象自身的运行时数据
    如哈希码（HashCode）、GC 分代年龄、锁状态标志、线程持有的锁、偏向线程 ID、偏向时间戳等，这部分数据的长度在 32 位和 64 位的虚拟机（未开启压缩指针）中分别为 32 个比特和 64 个比特，官方称它为 `Mark Word`。

    ![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220422223212.png)

    ![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220422223237.png)
    
    对象需要存储的运行时数据很多，其实已经超出了 32、64 位 Bitmap 结构所能记录的最大限度，但对象头里的信息是与对象自身定义的数据无关的额外存储成本，考虑到虚拟机的空间效率，Mark Word 被设计成一个有着动态定义的数据结构，以便在极小的空间内存储尽量多的数据，根据对象的状态复用自己的存储空间。
- 类型指针
    即对象指向它的类型元数据的指针，Java 虚拟机通过这个指针来确定该对象是哪个类的实例。**并不是所有的虚拟机实现都必须在对象数据上保留类型指针**，换句话说，查找对象的元数据信息并不一定要经过对象本身。
    
    此外，如果对象是一个 Java 数组，那在对象头中还必须有一块用于记录数组长度的数据，因为虚拟机可以通过普通 Java 对象的元数据信息确定 Java 对象的大小，但是如果数组的长度是不确定的，将无法通过元数据中的信息推断出数组的大小。

##### 1.1.2.2.2. 实例数据

实例数据部分是对象真正存储的有效信息，即我们在程序代码里面所定义的各种类型的字段内容，无论是从父类继承下来的，还是在子类中定义的字段都必须记录起来。

这部分的存储顺序会受到虚拟机分配策略参数（-XX：FieldsAllocationStyle 参数）和字段在 Java 源码中定义顺序的影响。 HotSpot 虚拟机默认的分配顺序为 longs/doubles、ints、shorts/chars、bytes/booleans、oops（Ordinary Object Pointers，OOPs）。

从以上默认的分配策略中可以看到，**相同宽度的字段总是被分配到一起存放，在满足这个前提条件的情况下，在父类中定义的变量会出现在子类之前**。如果 HotSpot 虚拟机的 +XX：CompactFields 参数值为 true（默认就为 true），那子类之中较窄的变量也允许插入父类变量的空隙之中，以节省出一点点空间。

##### 1.1.2.2.3. 对齐填充

对象的第三部分是对齐填充，这并不是必然存在的，也没有特别的含义，它仅仅起着占位符的作用。由于 HotSpot 虚拟机的自动内存管理系统要求对象起始地址必须是 8 字节的整数倍，换句话说就是任何对象的大小都必须是 8 字节的整数倍。对象头部分已经被精心设计成正好是 8 字节的倍数（1 倍或者 2 倍），因此，如果对象实例数据部分没有对齐的话，就需要通过对齐填充来补全。

#### 1.1.2.3. 对象的访问布局

创建对象自然是为了后续使用该对象，Java 程序会通过栈上的 reference 数据来操作堆上的具体对象。由于 reference 类型在《Java 虚拟机规范》里面只规定了它是一个指向对象的引用，并没有定义这个引用应该通过什么方式去定位、访问到堆中对象的具体位置，所以对象访问方式也是由虚拟机实现而定的，主流的访问方式主要有使用句柄和直接指针两种：
- 句柄访问
    使用句柄访问，Java 堆中将可能会划分出一块内存来作为句柄池，reference 中存储的就是对象的句柄地址，而**句柄中包含了对象实例数据与类型数据各自具体的地址信息**。

    ![image.png](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202308130204542.png)
    
    使用句柄来访问的最大好处就是 reference 中存储的是稳定句柄地址，在对象被移动（垃圾收集时移动对象是非常普遍的行为）时只会改变句柄中的实例数据指针，而 reference 本身不需要被修改。
- 直接指针
    使用直接指针访问的话，Java 堆中对象的内存布局就必须考虑如何放置访问类型数据的相关信息，reference 中存储的直接就是对象地址，如果只是访问对象本身的话，就不需要多一次间接访问的开销。

    ![image.png](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202308130205252.png)
    
    使用直接指针来访问最大的好处就是速度更快，它节省了一次指针定位的时间开销，由于对象访问在 Java 中非常频繁，因此这类开销积少成多也是一项极为可观的执行成本，就本书讨论的主要虚拟机 HotSpot 而言，它主要使用第二种方式进行对象访问。

## 1.2. 垃圾收集器与内存分配策略

垃圾收集需要完成的三件事情：哪些内存需要回收？什么时候回收？如何回收？

### 1.2.1. 对象存活性判断

#### 1.2.1.1. 引用计数法

引用计数法判断规则如下：在对象中添加一个引用计数器，每当有一个地方引用它时，计数器值就加一；当引用失效时，计数器值就减一；任何时刻计数器为零的对象就是不可能再被使用的。

客观地说，引用计数算法（Reference Counting）虽然占用了一些额外的内存空间来进行计数，但它的原理简单，判定效率也很高，在大多数情况下它都是一个不错的算法。但是，在 Java 领域，至少主流的 Java 虚拟机里面都没有选用引用计数算法来管理内存，主要原因是，这个看似简单的算法有很多例外情况要考虑，必须要配合大量额外处理才能保证正确地工作，譬如**单纯的引用计数就很难解决对象之间相互循环引用的问题**。

#### 1.2.1.2. 可达性分析法

可达性分析算法的基本思路就是通过一系列称为“GC Roots”的根对象作为起始节点集，从这些节点开始，根据引用关系向下搜索，搜索过程所走过的路径称为“引用链”（Reference Chain），如果某个对象到 GC Roots 间没有任何引用链相连，或者用图论的话来说就是从 **GC Roots 到这个对象不可达时，则证明此对象是不可能再被使用的**。

![image.png](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202308130214566.png)

在 Java 技术体系里面，固定可作为 GC Roots 的对象包括以下几种：
- 在虚拟机栈（栈帧中的本地变量表）中引用的对象，譬如各个线程被调用的方法堆栈中使用到的参数、局部变量、临时变量等。
- 在方法区中类静态属性引用的对象，譬如 Java 类的引用类型静态变量。
- 在方法区中常量引用的对象，譬如字符串常量池（String Table）里的引用。
- 在本地方法栈中 JNI（即通常所说的 Native 方法）引用的对象。
- Java 虚拟机内部的引用，如基本数据类型对应的 Class 对象，一些常驻的异常对象（比如 NullPointExcepiton、OutOfMemoryError）等，还有系统类加载器。
- 所有被同步锁（synchronized 关键字）持有的对象。
- 反映 Java 虚拟机内部情况的 JMXBean、JVMTI 中注册的回调、本地代码缓存等。

除了这些固定的 GC Roots 集合以外，根据用户所选用的垃圾收集器以及当前回收的内存区域不同，还可以有其他对象“临时性”地加入，共同构成完整 GC Roots 集合。譬如后文将会提到的分代收集和局部回收（Partial GC），如果只针对 Java 堆中某一块区域发起垃圾收集时（如最典型的只针对新生代的垃圾收集），必须考虑到内存区域是虚拟机自己的实现细节（在用户视角里任何内存区域都是不可见的），更不是孤立封闭的，所以某个区域里的对象完全有可能被位于堆中其他区域的对象所引用，这时候就需要将这些关联区域的对象也一并加入 GC Roots 集合中去，才能保证可达性分析的正确性。

#### 1.2.1.3. 引用的类型

Java 对引用的概念进行了扩充，将引用分为强引用（Strongly Re-ference）、软引用（Soft Reference）、弱引用（Weak Reference）和虚引用（Phantom Reference）4 种，这 4 种引用强度依次逐渐减弱：
- 强引用
    最传统的“引用”的定义，是指在程序代码之中普遍存在的引用赋值，即类似 `Object obj=new Object ();` 这种引用关系。无论任何情况下，只要强引用关系还存在，垃圾收集器就永远不会回收掉被引用的对象。
- 软引用
    用来描述一些还有用，但非必须的对象。只被软引用关联着的对象，在**系统将要发生内存溢出异常前，会把这些对象列进回收范围之中进行第二次回收**，如果这次回收还没有足够的内存，才会抛出内存溢出异常。在 JDK 1.2 版之后提供了 SoftReference 类来实现软引用。
- 弱引用
    也是用来描述那些非必须对象，但是它的强度比软引用更弱一些，被**弱引用关联的对象只能生存到下一次垃圾收集发生为止**。当垃圾收集器开始工作，无论当前内存是否足够，都会回收掉只被弱引用关联的对象。在 JDK 1.2 版之后提供了 WeakReference 类来实现弱引用。
- 虚引用
    也称为“幽灵引用”或者“幻影引用”，它是最弱的一种引用关系。一个对象是否有虚引用的存在，完全不会对其生存时间构成影响，也无法通过虚引用来取得一个对象实例。为一个对象**设置虚引用关联的唯一目的只是为了能在这个对象被收集器回收时收到一个系统通知**。在 JDK 1.2 版之后提供了 PhantomReference 类来实现虚引用。

#### 1.2.1.4. 真正的死亡

即使在可达性分析算法中判定为不可达的对象，也不是“非死不可”的，这时候它们暂时还处于“缓刑”阶段，要真正宣告一个对象死亡，至少要经历两次标记过程：
1. 如果对象在进行可达性分析后发现没有与 GC Roots 相连接的引用链，那它将会被第一次标记。
2. 进行一次筛选，筛选的条件是此对象是否有必要执行 `finalize()` 方法。假如对象没有覆盖 `finalize ()` 方法，或者 `finalize ()` 方法已经被虚拟机调用过，那么虚拟机将这两种情况都视为“没有必要执行”。

如果这个对象被判定为确有必要执行 `finalize ()` 方法，那么该对象将会被放置在一个名为 `F-Queue` 的队列之中，并在稍后由一条由虚拟机自动建立的、低调度优先级的 `Finalizer` 线程去执行它们的 `finalize ()` 方法。

这里所说的“执行”是指虚拟机会触发这个方法开始运行，但并不承诺一定会等待它运行结束。这样做的原因是，如果某个对象的 `finalize ()` 方法执行缓慢，或者更极端地发生了死循环，将很可能导致 `F-Queue` 队列中的其他对象永久处于等待，甚至导致整个内存回收子系统的崩溃。

`finalize ()` 方法是对象逃脱死亡命运的最后一次机会，稍后收集器将对 `F-Queue` 中的对象进行第二次小规模的标记，如果对象要在 `finalize ()` 中成功拯救自己——只要重新与引用链上的任何一个对象建立关联即可，譬如把自己 （this 关键字）赋值给某个类变量或者对象的成员变量，那在第二次标记时它将被移出“即将回收”的集合；如果对象这时候还没有逃脱，那基本上它就真的要被回收了。

任何一个对象的 `finalize ()` 方法都只会被系统自动调用一次，如果对象面临下一次回收，它的 `finalize ()` 方法不会被再次执行。

#### 1.2.1.5. 方法区的回收

方法区垃圾收集的“性价比”通常也是比较低的：在 Java 堆中，尤其是在新生代中，对常规应用进行一次垃圾收集通常可以回收 70%至 99%的内存空间，相比之下，方法区回收囿于苛刻的判定条件，其区域垃圾收集的回收成果往往远低于此。

方法区的垃圾收集主要回收两部分内容：
- 废弃的常量
    回收废弃常量与回收 Java 堆中的对象非常类似。

    举个常量池中字面量回收的例子，假如一个字符串“java”曾经进入常量池中，但是当前系统又没有任何一个字符串对象的值是“java”，换句话说，已经没有任何字符串对象引用常量池中的“java”常量，且虚拟机中也没有其他地方引用这个字面量。如果在这时发生内存回收，而且垃圾收集器判断确有必要的话，这个“java”常量就将会被系统清理出常量池。常量池中其他类（接口）、方法、字段的符号引用也与此类似。
- 不再使用的类
    要判定一个类是否属于“不再被使用的类”的条件就比较苛刻了。需要同时满足下面三个条件：
    1. 该类所有的实例都已经被回收，也就是 Java 堆中不存在该类及其任何派生子类的实例。
    2. 加载该类的类加载器已经被回收，这个条件除非是经过精心设计的可替换类加载器的场景，如 OSGi、JSP 的重加载等，否则通常是很难达成的。
    3. 该类对应的 `java.lang.Class` 对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法。
       
       Java 虚拟机被允许对满足上述三个条件的无用类进行回收，这里说的仅仅是“被允许”，而并不是和对象一样，没有引用了就必然会回收。关于是否要对类型进行回收，HotSpot 虚拟机提供了 Xnoclassgc 参数进行控制。
       
       在大量使用反射、动态代理、CGLib 等字节码框架，动态生成 JSP 以及 OSGi 这类频繁自定义类加载器的场景中，通常都需要 Java 虚拟机具备类型卸载的能力，以保证不会对方法区造成过大的内存压力。

### 1.2.2. 垃圾收集算法

#### 1.2.2.1. 分代收集理论

当前商业虚拟机的垃圾收集器，大多数都遵循了“分代收集”（Generational Collection） 的理论进行设计，分代收集名为理论，实质是一套符合大多数程序运行实际情况的经验法则，它建立在两个分代假说之上：
1. 弱分代假说（Weak Generational Hypothesis）：绝大多数对象都是朝生夕灭的。
    如果一个区域中大多数对象都是朝生夕灭，难以熬过垃圾收集过程的话，那么把它们集中放在一起，每次回收时只关注如何保留少量存活而不是去标记那些大量将要被回收的对象，就能以较低代价回收到大量的空间；
2. 强分代假说（Strong Generational Hypothesis）：熬过越多次垃圾收集过程的对象就越难以消亡。
    如果剩下的都是难以消亡的对象，那把它们集中放在一块，虚拟机便可以使用较低的频率来回收这个区域，这就同时兼顾了垃圾收集的时间开销和内存的空间有效利用。
3. 跨代引用假说（Intergenerational Reference Hypothesis）：跨代引用相对于同代引用来说仅占极少数。
    存在互相引用关系的两个对象，是应该倾向于同时生存或者同时消亡的。
    
    如果某个新生代对象存在跨代引用，由于老年代对象难以消亡，该引用会使得新生代对象在收集时同样得以存活，进而在年龄增长之后晋升到老年代中，这时跨代引用也随即被消除了。
    
    依据这条假说，我们就**不应再为了少量的跨代引用去扫描整个老年代，也不必浪费空间专门记录每一个对象是否存在及存在哪些跨代引用**，只需在新生代上建立一个全局的数据结构（该结构被称为“记忆集”，Remembered Set），这个结构**把老年代划分成若干小块，标识出老年代的哪一块内存会存在跨代引用**。此后当发生 Minor GC 时，只有包含了跨代引用的小块内存里的对象才会被加入到 GC Roots 进行扫描。虽然这种方法需要在对象改变引用关系（如将自己或者某个属性赋值）时维护记录数据的正确性，会增加一些运行时的开销，但比起收集时扫描整个老年代来说仍然是划算的。

分代假说共同奠定了多款常用的垃圾收集器的一致的设计原则：收集器应该将 Java 堆划分出不同的区域，然后将回收对象依据其年龄（年龄即对象熬过垃圾收集过程的次数）分配到不同的区域之中存储。显而易见，如果一个区域中大多数对象都是朝生夕灭，难以熬过垃圾收集过程的话，那么把它们集中放在一起，**每次回收时只关注如何保留少量存活而不是去标记那些大量将要被回收的对象，就能以较低代价回收到大量的空间**；如果剩下的都是难以消亡的对象，那把它们集中放在一块，虚拟机便可以使用较低的频率来回收这个区域，这就同时兼顾了垃圾收集的时间开销和内存的空间有效利用。

在 Java 堆划分出不同的区域之后，垃圾收集器才可以每次只回收其中某一个或者某些部分的区域 ——因而才有了“Minor GC”“Major GC”“Full GC”这样的回收类型的划分；也才能够针对不同的区域安排与里面存储对象存亡特征相匹配的垃圾收集算法——因而发展出了“标记-复制算法”“标记-清除算法”“标记-整理算法”等针对性的垃圾收集算法。

因此在基于不同的区域进行的收集下产生了多种收集算法：
- 部分收集（Partial GC）：指目标不是完整收集整个 Java 堆的垃圾收集
    - 新生代收集（Minor GC/Young GC）：指目标只是新生代的垃圾收集。
    - 老年代收集（Major GC/Old GC）：指目标只是老年代的垃圾收集。
        目前只有 CMS 收集器会有单独收集老年代的行为。另外请注意“Major GC”这个说法现在有点混淆，在不同资料上常有不同所指，读者需按上下文区分到底是指老年代的收集还是整堆收集。
    - 混合收集（Mixed GC）：指目标是收集整个新生代以及部分老年代的垃圾收集。目前只有 G1 收集器会有这种行为。
- 整堆收集（Full GC）：收集整个 Java 堆和方法区的垃圾收集
    分代收集理论也有其缺陷，最新出现（或在实验中）的几款垃圾收集器都展现出了面向全区域收集设计的思想，或者可以支持全区域不分代的收集的工作模式。

#### 1.2.2.2. 标记-清除算法

算法分为“标记”和“清除”两个阶段：首先标记出所有需要回收的对象，在标记完成后，统一回收掉所有被标记的对象，也可以反过来，标记存活的对象，统一回收所有未被标记的对象。标记过程就是对象是否属于垃圾的判定过程。

标记清除算法主要缺点有两个：
1. 第一个是执行效率不稳定
    如果 Java 堆中包含大量对象，而且其中大部分是需要被回收的，这时必须进行大量标记和清除的动作，导致**标记和清除两个过程的执行效率都随对象数量增长而降低**。
2. 第二个是内存空间的碎片化问题
    标记、清除之后会**产生大量不连续的内存碎片**，空间碎片太多可能会导致当以后在程序运行过程中需要分配较大对象时无法找到足够的连续内存而不得不提前触发另一次垃圾收集动作。

![image.png](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202308131601911.png)

#### 1.2.2.3. 标记-复制算法

标记-复制算法常被简称为复制算法。为了解决标记-清除算法面对大量可回收对象时执行效率低的问题，1969 年 Fenichel 提出了一种称为“半区复制”（Semispace Copying）的垃圾收集算法，它将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后再把已使用过的内存空间一次清理掉。

**如果内存中多数对象都是存活的，这种算法将会产生大量的内存间复制的开销，但对于多数对象都是可回收的情况，算法需要复制的就是占少数的存活对象**，而且每次都是针对整个半区进行内存回收，分配内存时也就不用考虑有空间碎片的复杂情况，只要移动堆顶指针，按顺序分配即可。这样实现简单，运行高效，不过其缺陷也显而易见，这种复制回收算法的代价是将可用内存缩小为了原来的一半，空间浪费未免太多了一点。

![image.png](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202308131602628.png)

有一种更优化的 Appel 式半区复制分代策略：**把新生代分为一块较大的 Eden 空间和两块较小的 Survivor 空间，每次分配内存只使用 Eden 和其中一块 Survivor**。发生垃圾搜集时，将 Eden 和 Survivor 中仍然存活的对象一次性复制到另外一块 Survivor 空间上，然后直接清理掉 Eden 和已用过的那块 Survivor 空间。

HotSpot 虚拟机默认 Eden 和 Survivor 的大小比例是 8∶1，也即每次新生代中可用内存空间为整个新生代容量的 90%（Eden 的 80%加上一个 Survivor 的 10%），只有一个 Survivor 空间，即 10%的新生代是会被“浪费”的。Appel 式回收还有一个充当罕见情况的“逃生门”的安全设计，**当 Survivor 空间不足以容纳一次 Minor GC 之后存活的对象时，就需要依赖其他内存区域（实际上大多就是老年代）进行分配担保（Handle Promotion）**，如果另外一块 Survivor 空间没有足够空间存放上一次新生代收集下来的存活对象，这些对象便将通过分配担保机制直接进入老年代，这对虚拟机来说就是安全的。

#### 1.2.2.4. 标记-整理算法

标记-复制算法在对象存活率较高时就要进行较多的复制操作，效率将会降低。更关键的是，如果不想浪费 50%的空间，就需要有额外的空间进行分配担保，以应对被使用的内存中所有对象都 100%存活的极端情况，所以在老年代一般不能直接选用这种算法。

标记-整理算法针对老年代对象的存亡特征，其中的标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向内存空间一端移动，然后直接清理掉边界以外的内存。

![image.png](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202308131608013.png)

标记-清除算法与标记-整理算法的本质差异在于前者是一种非移动式的回收算法，而后者是移动式的。**是否移动回收后的存活对象是一项优缺点并存的风险决策**：
- 移动存活对象并更新所有引用这些对象的地方将会是一种极为负重的操作。
    尤其是在老年代这种每次回收都有大量**对象存活区域涉及到大量的引用更新，而且这种对象移动操作必须全程暂停用户应用程序才能进行**这就更加让使用者不得不小心翼翼地权衡其弊端了，像这样的停顿被最初的虚拟机设计者形象地描述为“Stop The World”。
- 完全不考虑移动和整理存活对象的话，弥散于堆中的存活对象导致的空间碎片化问题就只能依赖更为复杂的内存分配器和内存访问器来解决。
    譬如通过“分区空闲分配链表”来解决内存分配问题。内存的访问是用户程序最频繁的操作，甚至都没有之一，假如在这个环节上增加了额外的负担，势必会直接影响应用程序的吞吐量。

基于以上两点，是否移动对象都存在弊端：**移动则内存回收时会更复杂，不移动则内存分配时会更复杂**。**从垃圾收集的停顿时间来看，不移动对象停顿时间会更短，甚至可以不需要停顿，但是从整个程序的吞吐量来看，移动对象会更划算**。即使不移动对象会使得收集器的效率提升一些，但因内存分配和访问相比垃圾收集频率要高得多，这部分的耗时增加，总吞吐量仍然是下降的。HotSpot 虚拟机里面关注吞吐量的 Parallel Scavenge 收集器是基于标记-整理算法的，而关注延迟的 CMS 收集器则是基于标记-清除算法的，这也从侧面印证这点。

另外，还有一种“和稀泥式”解决方案可以不在内存分配和访问上增加太大额外负担，做法是让**虚拟机平时多数时间都采用标记-清除算法，暂时容忍内存碎片的存在，直到内存空间的碎片化程度已经大到影响对象分配时，再采用标记-整理算法收集一次，以获得规整的内存空间**。前面提到的基于标记-清除算法的 CMS 收集器面临空间碎片过多时采用的就是这种处理办法。

> [!note] 延迟处理的思想
> 
> 容忍内存碎片的标记清除算法是一种延迟处理问题的方式，在问题可控之前无需处理，而是将其延迟到某个时候再统一处理。[[Mysql的一致性保证#4.2.2. DELETE 操作对应的 undo 日志|Mysql 的数据存储中也有类似的思想]]，当数据的变动导致页内碎片产生的时候，Mysql 并不会第一时间去进行碎片的整理，而是会延迟到新数据需要写入该页但是由于碎片无法连续写入时才会开始整理合并。


### 1.2.3. 垃圾收集算法实现

#### 1.2.3.1. 根结点枚举

固定可作为 GC Roots 的节点主要在全局性的引用（例如常量或类静态属性）与执行上下文（例如栈帧中的本地变量表）中，尽管目标明确，但查找过程要做到高效并非一件容易的事情。

迄今为止，所有收集器在根节点枚举这一步骤时都是必须暂停用户线程的，因此毫无疑问根节点枚举与之前提及的整理内存碎片一样会面临相似的“Stop The World”的困扰。现在可达性分析算法耗时最长的查找引用链的过程已经可以做到与用户线程一起并发，但根节点枚举始终还是必须在一个能保障一致性的快照中才得以进行，若这点不能满足的话，分析结果准确性也就无法保证。这是导致垃圾收集过程必须停顿所有用户线程的其中一个重要原因，即使是号称停顿时间可控，或者（几乎）不会发生停顿的 CMS、G1、 ZGC 等收集器，**枚举根节点时也是必须要停顿的**。

由于目前主流 Java 虚拟机使用的都是准确式垃圾收集，所以当用户线程停顿下来之后，其实并不需要一个不漏地检查完所有执行上下文和全局的引用位置，虚拟机应当是有办法直接得到哪些地方存放着对象引用的。在 HotSpot 的解决方案里，是使用一组称为 `OopMap` 的数据结构来达到这个目的。一旦**类加载动作完成的时候， HotSpot 就会把对象内什么偏移量上是什么类型的数据计算出来**，在即时编译过程中，也会在特定的位置记录下栈里和寄存器里哪些位置是引用。这样收集器在扫描时就可以直接得知这些信息了，并不需要真正一个不漏地从方法区等 GC Roots 开始查找。

> [!info] 准确式垃圾收集
> 
> 准确式内存管理是指虚拟机可以知道内存中某个位置的数据具体是什么类型。譬如内存中有一个 32bit 的整数 123456，虚拟机将有能力分辨出它到底是一个指向了 123456 的内存地址的引用类型还是一个数值为 123456 的整数，准确分辨出哪些内存是引用类型，这也是在垃圾收集时准确判断堆上的数据是否还可能被使用的前提。

> [!info] OopMap
> 
> HotSpot 的 OopMap（Object-oriented Pointer Map）在垃圾回收过程中起到关键作用，帮助垃圾回收器准确地标记和扫描对象。下面是 OopMap 如何实现这个目标的几个方面：
> 1. 定位引用类型变量：OopMap 记录了方法中栈帧、堆对象和寄存器中的引用类型变量的位置信息。垃圾回收器可以通过 OopMap 快速定位这些引用类型变量，以便进行标记和扫描。
> 2. 标记存活对象：垃圾回收器在标记阶段需要确定哪些对象是可达的（即存活对象），哪些对象是不可达的（即垃圾对象）。通过 OopMap 提供的引用位置信息，垃圾回收器可以遍历栈帧和堆中的对象，并标记那些在 OopMap 中被标记为引用的对象。
> 3. 过滤非引用类型变量：OopMap 中只记录引用类型变量的位置信息，而不记录非引用类型变量（如基本数据类型）。这样可以减少垃圾回收器在标记和扫描过程中的工作量，只关注需要处理的引用类型变量。
> 4. 动态更新：OopMap 的生成和更新是在编译时和运行时动态进行的。在方法编译期间，JIT 会生成初始的 OopMap，然后在运行时根据实际对象的分布和引用情况进行动态更新。这样可以保证垃圾回收器对对象的标记和扫描的准确性。
>    
> 通过以上方式，OopMap 帮助垃圾回收器准确地标记和扫描对象，确保只有存活对象被保留下来，垃圾对象可以被回收释放。这样可以提高垃圾回收的效率和内存利用率，减少不必要的内存占用。

#### 1.2.3.2. 安全点

在 OopMap 的协助下，HotSpot 可以快速准确地完成 GC Roots 枚举，但一个很现实的问题随之而来：可能导致引用关系变化，或者说导致 OopMap 内容变化的指令非常多，如果为每一条指令都生成对应的 OopMap，那将会需要大量的额外存储空间，这样垃圾收集伴随而来的空间成本就会变得无法忍受的高昂。

实际上 HotSpot 也的确没有为每条指令都生成 OopMap，前面已经提到，**只是在“特定的位置”记录了 OopMap，这些位置被称为安全点（Safepoint）**。有了安全点的设定，也就决定了用户程序执行时并非在代码指令流的任意位置都能够停顿下来开始垃圾收集，而是**强制要求必须执行到达安全点后才能够暂停**。

因此，安全点的选定既不能太少以至于让收集器等待时间过长，也不能太过频繁以至于过分增大运行时的内存负荷。**安全点位置的选取基本上是以“是否具有让程序长时间执行的特征”为标准进行选定的**，因为每条指令执行的时间都非常短暂，程序不太可能因为指令流长度太长这样的原因而长时间执行，“长时间执行”的最明显特征就是指令序列的复用，例如方法调用、循环跳转、异常跳转等都属于指令序列复用，所以只有具有这些功能的指令才会产生安全点。

对于安全点，另外一个需要考虑的问题是，**如何在垃圾收集发生时让所有线程（这里其实不包括执行 JNI 调用的线程）都跑到最近的安全点，然后停顿下来**。这里有两种方案可供选择：
- 抢先式中断 （Preemptive Suspension）
    抢先式中断不需要线程的执行代码主动去配合，在垃圾收集发生时，系统首先把所有用户线程全部中断，如果发现有用户线程中断的地方不在安全点上，就恢复这条线程执行，让它一会再重新中断，直到跑到安全点上。现在几乎没有虚拟机实现采用抢先式中断来暂停线程响应 GC 事件。
- 主动式中断（Voluntary Suspension）
    主动式中断的思想是当垃圾收集需要中断线程的时候，不直接对线程操作，仅仅简单地设置一个标志位，**各个线程执行过程时会不停地主动去轮询这个标志**，一旦发现中断标志为真时就自己在最近的安全点上主动中断挂起。轮询标志的地方和安全点是重合的，另外还要加上所有创建对象和其他需要在 Java 堆上分配内存的地方，这是为了检查是否即将要发生垃圾收集，避免没有足够内存分配新对象。

    由于轮询操作在代码中会频繁出现，这要求它必须足够高效。HotSpot 使用内存保护陷阱的方式，把轮询操作精简至只有一条汇编指令的程度。

#### 1.2.3.3. 安全区域

使用安全点的设计似乎已经完美解决如何停顿用户线程，让虚拟机进入垃圾回收状态的问题了，但实际情况却并不一定。安全点机制保证了程序执行时，在不太长的时间内就会遇到可进入垃圾收集过程的安全点。但是，程序“不执行”的时候呢？所谓的程序不执行就是没有分配处理器时间，典型的场景便是用户线程处于 Sleep 状态或者 Blocked 状态，这时候线程无法响应虚拟机的中断请求，不能再走到安全的地方去中断挂起自己，虚拟机也显然不可能持续等待线程重新被激活分配处理器时间。对于这种情况，就必须引入安全区域（Safe Region）来解决。

**安全区域是指能够确保在某一段代码片段之中，引用关系不会发生变化，因此，在这个区域中任意地方开始垃圾收集都是安全的**。我们也可以把安全区域看作被扩展拉伸了的安全点。

当用户线程执行到安全区域里面的代码时，首先会标识自己已经进入了安全区域，那样当这段时间里虚拟机要发起垃圾收集时就不必去管这些已声明自己在安全区域内的线程了。当线程要离开安全区域时，它要检查虚拟机是否已经完成了根节点枚举（或者垃圾收集过程中其他需要暂停用户线程的阶段），**如果完成了，那线程就当作没事发生过，继续执行；否则它就必须一直等待，直到收到可以离开安全区域的信号为止**。

#### 1.2.3.4. 记忆集与卡表

分代收集理论为解决对象跨代引用所带来的问题，垃圾收集器在新生代中建立了名为记忆集（Remembered Set）的数据结构，用以避免把整个老年代加进 GC Roots 扫描范围。事实上并不只是新生代、老年代之间才有跨代引用的问题，**所有涉及部分区域收集（Partial GC）行为的垃圾收集器，典型的如 G1、ZGC 和 Shenandoah 收集器，都会面临相同的问题**。

在垃圾收集的场景中，收集器**只需要通过记忆集判断出某一块非收集区域是否存在有指向了收集区域的指针就可以了**，并不需要了解这些跨代指针的全部细节。那设计者在实现记忆集的时候，便可以**选择更为粗犷的记录粒度来节省记忆集的存储和维护成本**，下面列举了一些可供选择：
- 字长精度：每个记录精确到一个机器字长（就是处理器的寻址位数，如常见的 32 位或 64 位，这个精度决定了机器访问物理内存地址的指针长度），该字包含跨代指针。
- 对象精度：每个记录精确到一个对象，该对象里有字段含有跨代指针。
- 卡精度：每个记录精确到一块内存区域，该区域内有对象含有跨代指针。
    卡精度是用一种称为“卡表”（Card Table）的方式去实现记忆集，一个卡页的内存中通常包含不止一个对象，只要卡页内有一个（或更多）对象的字段存在着跨代指针，那就将对应卡表的数组元素的值标识为 1，称为这个元素变脏（Dirty），没有则标识为 0。在垃圾收集发生时，只要筛选出卡表中变脏的元素，就能轻易得出哪些卡页内存块中包含跨代指针，把它们加入 GC Roots 中一并扫描。

    ![image.png](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202308131633133.png)

#### 1.2.3.5. 写屏障

记忆集可以缩减 GC Roots 扫描范围的问题，但还没有解决卡表元素如何维护的问题，例如**它们何时变脏、谁来把它们变脏**等。

卡表元素何时变脏的答案是很明确的——有其他分代区域中对象引用了本区域对象时，其对应的卡表元素就应该变脏，变脏时间点原则上应该发生在引用类型字段赋值的那一刻。但问题是如何变脏，即如何在对象赋值的那一刻去更新维护卡表呢？

假如是解释执行的字节码，那相对好处理，虚拟机负责每条字节码指令的执行，有充分的介入空间；但在编译执行的场景中呢？经过即时编译后的代码已经是纯粹的机器指令流了，这就必须找到一个**在机器码层面的手段，把维护卡表的动作放到每一个赋值操作之中**。

在 HotSpot 虚拟机里是通过**写屏障（Write Barrier）技术维护卡表状态的**。**写屏障可以看作在虚拟机层面对“引用类型字段赋值”这个动作的 AOP 切面，在引用对象赋值时会产生一个环形（Around）通知，供程序执行额外的动作**，也就是说赋值的前后都在写屏障的覆盖范畴内。在赋值前的部分的写屏障叫作写前屏障（Pre-Write Barrier），在赋值后的则叫作写后屏障（Post-Write Barrier）。HotSpot 虚拟机的许多收集器中都有使用到写屏障，但直至 G1 收集器出现之前，其他收集器都只用到了写后屏障。下面这段代码是一段更新卡表状态的简化逻辑：

```c
void oop_field_store(oop* field, oop new_value) { 
// 引用字段赋值操作
*field = new_value;
// 写后屏障，在这里完成卡表状态更新
post_write_barrier(field, new_value);
}
```

应用写屏障后，虚拟机就会为所有赋值操作生成相应的指令，一旦收集器在写屏障中增加了更新卡表操作，无论更新的是不是老年代对新生代对象的引用，每次只要对引用进行更新，就会产生额外的开销，不过这个开销与 Minor GC 时扫描整个老年代的代价相比还是低得多的。

除了写屏障的开销外，卡表在高并发场景下还面临着“伪共享”（False Sharing）问题。伪共享是处理并发底层细节时一种经常需要考虑的问题，现代中央处理器的缓存系统中是以缓存行（Cache Line） 为单位存储的，当多线程修改互相独立的变量时，如果这些变量恰好共享同一个缓存行，就会彼此影响（写回、无效化或者同步）而导致性能降低，这就是伪共享问题。

假设处理器的缓存行大小为 64 字节，由于一个卡表元素占 1 个字节，64 个卡表元素将共享同一个缓存行。这 64 个卡表元素对应的卡页总的内存为 32KB（64×512 字节），也就是说如果不同线程更新的对象正好处于这 32KB 的内存区域内，就会导致更新卡表时正好写入同一个缓存行而影响性能。为了避免伪共享问题，**一种简单的解决方案是不采用无条件的写屏障，而是先检查卡表标记，只有当该卡表元素未被标记过时才将其标记为变脏**。

#### 1.2.3.6. 并发的可达性分析

当前主流编程语言的垃圾收集器基本上都是依靠可达性分析算法来判定对象是否存活的，**可达性分析算法理论上要求全过程都基于一个能保障一致性的快照中才能够进行分析**，这意味着必须全程冻结用户线程的运行。

想解决或者降低用户线程的停顿，就要先搞清楚为什么必须在一个能保障一致性的快照上才能进行对象图的遍历？为了能解释清楚这个问题，我们引入三色标记（Tri-color Marking）作为工具来辅助推导，把遍历对象图过程中遇到的对象，按照“是否访问过”这个条件标记成以下三种颜色：
- 白色：表示对象尚未被垃圾收集器访问过。显然在可达性分析刚刚开始的阶段，所有的对象都是白色的，若在分析结束的阶段，仍然是白色的对象，即代表不可达。
- 黑色：表示对象已经被垃圾收集器访问过，且这个对象的所有引用都已经扫描过。黑色的对象代表已经扫描过，它是安全存活的，如果有其他对象引用指向了黑色对象，无须重新扫描一遍。黑色对象不可能直接（不经过灰色对象）指向某个白色对象。
- 灰色：表示对象已经被垃圾收集器访问过，但这个对象上至少存在一个引用还没有被扫描过。

如果用户线程与收集器是并发工作呢？收集器在对象图上标记颜色，同时用户线程在修改引用关系——即修改对象图的结构，这样可能出现两种后果。一种是把原本消亡的对象错误标记为存活，这不是好事，但其实是可以容忍的，只不过产生了一点逃过本次收集的浮动垃圾而已，下次收集清理掉就好。另一种是把原本存活的对象错误标记为已消亡，这就是非常致命的后果了，程序肯定会因此发生错误，下面演示了这样的致命错误具体是如何产生的：

![image.png](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202308131647250.png)

当且仅当以下两个条件同时满足时，会产生“对象消失”的问题，即原本应该是黑色的对象被误标为白色：
- 赋值器插入了一条或多条从黑色对象到白色对象的新引用。
- 赋值器删除了全部从灰色对象到该白色对象的直接或间接引用。

因此，我们要解决并发扫描时的对象消失问题，只需破坏这两个条件的任意一个即可。由此分别产生了两种解决方案：增量更新（Incremental Update）和原始快照（Snapshot At The Beginning， SATB）。
- 增量更新
    破坏的是第一个条件，当黑色对象插入新的指向白色对象的引用关系时，就将这个新插入的引用记录下来，等并发扫描结束之后，再将这些记录过的引用关系中的黑色对象为根，重新扫描一次。这可以简化理解为，黑色对象一旦新插入了指向白色对象的引用之后，它就变回灰色对象了。
    
    变成灰色对象以后要重新走一遍扫描的流程，再对象数量多的情况下这个扫描的成本会更大，因为这意味着该根对应需要扫描的链会很长。
- 原始快照
    破坏的是第二个条件，当灰色对象要删除指向白色对象的引用关系时，就将这个要删除的引用记录下来，在并发扫描结束之后，再将这些记录过的引用关系中的灰色对象为根，重新扫描一次。这也可以简化理解为，无论引用关系删除与否，都会按照刚刚开始扫描那一刻的对象图快照来进行搜索。

以上**无论是对引用关系记录的插入还是删除，虚拟机的记录操作都是通过写屏障实现的**。在 HotSpot 虚拟机中，增量更新和原始快照这两种解决方案都有实际应用，譬如，CMS 是基于增量更新来做并发标记的，G1、Shenandoah 则是用原始快照来实现。

### 1.2.4. 经典垃圾收集器

![image.png](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202308131711441.png)

#### 1.2.4.1. 新生代收集器

##### 1.2.4.1.1. Serial 收集器

Serial 收集器是最基础、历史最悠久的收集器，曾经（在 JDK 1.3.1 之前）是 HotSpot 虚拟机新生代收集器的唯一选择。这个收集器是一个单线程工作的收集器，但它的“单线程”的意义并不仅仅是说明它只会使用一个处理器或一条收集线程去完成垃圾收集工作，更重要的是强调在它进行垃圾收集时，必须暂停其他所有工作线程，直到它收集结束。

![image.png](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202308131719513.png)

迄今为止，它依然是 HotSpot 虚拟机运行在客户端模式下的默认新生代收集器，有着优于其他收集器的地方，那就是简单而高效（与其他收集器的单线程相比），对于内存资源受限的环境，它是所有收集器里额外内存消耗最小的；对于单核处理器或处理器核心数较少的环境来说，Serial 收集器由于没有线程交互的开销，专心做垃圾收集自然可以获得最高的单线程收集效率。

在用户桌面的应用场景以及近年来流行的部分微服务应用中，分配给虚拟机管理的内存一般来说并不会特别大，收集几十兆甚至一两百兆的新生代（仅仅是指新生代使用的内存，桌面应用甚少超过这个容量），垃圾收集的停顿时间完全可以控制在十几、几十毫秒，最多一百多毫秒以内，只要不是频繁发生收集，这点停顿时间对许多用户来说是完全可以接受的。所以， Serial 收集器对于运行在客户端模式下的虚拟机来说是一个很好的选择。

##### 1.2.4.1.2. ParNew 收集器

ParNew 收集器实质上是 Serial 收集器的多线程并行版本，ParNew 收集器除了支持多线程并行收集之外，其他与 Serial 收集器相比并没有太多创新之处，但它却是不少运行在服务端模式下的 HotSpot 虚拟机，尤其是 JDK 7 之前的遗留系统中首选的新生代收集器，其中有一个与功能、性能无关但其实很重要的原因是：除了 Serial 收集器外，目前只有它能与 CMS 收集器配合工作。

![image.png](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202308131719074.png)

ParNew 收集器在单核心处理器的环境中绝对不会有比 Serial 收集器更好的效果，甚至由于存在线程交互的开销，该收集器在通过超线程（Hyper-Threading）技术实现的伪双核处理器环境中都不能百分之百保证超越 Serial 收集器。当然，随着可以被使用的处理器核心数量的增加，ParNew 对于垃圾收集时系统资源的高效利用还是很有好处的。它默认开启的收集线程数与处理器核心数量相同。

##### 1.2.4.1.3. Parallel Scavenge 收集器

Parallel Scavenge 收集器也是一款新生代收集器，它同样是基于标记-复制算法实现的收集器，也是能够并行收集的多线程收集器。Parallel Scavenge 收集器的特点是它的关注点与其他收集器不同，**CMS 等收集器的关注点是尽可能地缩短垃圾收集时用户线程的停顿时间，而 Parallel Scavenge 收集器的目标则是达到一个可控制的吞吐量（Throughput）**。所谓吞吐量就是处理器用于运行用户代码的时间与处理器总消耗时间的比值，即：

$$
吞吐量 = \frac{运行用户代码时间}{运行用户代码时间+运行垃圾收集时间}
$$

停顿时间越短就越适合需要与用户交互或需要保证服务响应质量的程序，良好的响应速度能提升用户体验；而高吞吐量则可以最高效率地利用处理器资源，尽快完成程序的运算任务，主要适合在后台运算而不需要太多交互的分析任务。

Parallel Scavenge 收集器提供了两个参数用于精确控制吞吐量，分别是控制最大垃圾收集停顿时间的 `-XX:MaxGCPauseMillis` 参数以及直接设置吞吐量大小的 `-XX:GCTimeRatio` 参数。不过大家不要异想天开地认为如果把这个参数的值设置得更小一点就能使得系统的垃圾收集速度变得更快，垃圾收集停顿时间缩短是以牺牲吞吐量和新生代空间为代价换取的： 系统把新生代调得小一些，收集 300MB 新生代肯定比收集 500MB 快，但这也直接导致垃圾收集发生得更频繁，原来 10 秒收集一次、每次停顿 100 毫秒，现在变成 5 秒收集一次、每次停顿 70 毫秒。停顿时间的确在下降，但吞吐量也降下来了。

由于与吞吐量关系密切，Parallel Scavenge 收集器也经常被称作“吞吐量优先收集器”。除上述两个参数之外，Parallel Scavenge 收集器还有一个参数 `-XX:+UseAdaptiveSizePolicy` 值得我们关注。这是一个开关参数，当这个参数被激活之后，就不需要人工指定新生代的大小（-Xmn）、Eden 与 Survivor 区的比例（`-XX:SurvivorRatio`）、晋升老年代对象大小（`-XX:PretenureSizeThreshold`）等细节参数了，虚拟机会根据当前系统的运行情况收集性能监控信息，动态调整这些参数以提供最合适的停顿时间或者最大的吞吐量。这种调节方式称为垃圾收集的自适应的调节策略。

#### 1.2.4.2. 老年代收集器

##### 1.2.4.2.1. Serial Old 收集器

Serial Old 是 Serial 收集器的老年代版本，它同样是一个单线程收集器，使用标记-整理算法。这个收集器的主要意义也是供客户端模式下的 HotSpot 虚拟机使用。

![image.png](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202308131734252.png)

##### 1.2.4.2.2. Parallel Old 收集器

Parallel Old 是 Parallel Scavenge 收集器的老年代版本，支持多线程并发收集，基于标记-整理算法实现。

![image.png](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202308131735458.png)

##### 1.2.4.2.3. CMS 收集器

CMS（Concurrent Mark Sweep）收集器是一种以**获取最短回收停顿时间为目标的收集器**。目前很大一部分的 Java 应用集中在互联网网站或者基于浏览器的 B/S 系统的服务端上，这类应用通常都会较为关注服务的响应速度，希望系统停顿时间尽可能短，以给用户带来良好的交互体验。CMS 收集器就非常符合这类应用的需求。

CMS 收集器是基于标记-清除算法实现的，它的运作过程相对于前面几种收集器来说要更复杂一些，整个过程分为四个步骤，包括：
1. 初始标记（CMS initial mark）
    初始标记仅仅只是标记一下 GC Roots 能直接关联到的对象，速度很快。
2. 并发标记（CMS concurrent mark）
    并发标记阶段就是从 GC Roots 的直接关联对象开始遍历整个对象图的过程，这个过程耗时较长但是不需要停顿用户线程，可以与垃圾收集线程一起并发运行。
3. 重新标记（CMS remark）
    重新标记阶段则是为了修正并发标记期间，因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录（CMS 基于增量更新来重新标记变动的记录），这个阶段的停顿时间通常会比初始标记阶段稍长一些，但也远比并发标记阶段的时间短。
4. 并发清除（CMS concurrent sweep）
    清理删除掉标记阶段判断的已经死亡的对象，由于不需要移动存活对象，所以这个阶段也是可以与用户线程同时并发的。

整个过程中**耗时最长的并发标记和并发清除阶段中，垃圾收集器线程都可以与用户线程一起工作**，所以从总体上来说，CMS 收集器的内存回收过程是与用户线程一起并发执行的。

![image.png](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202308131743018.png)

CMS 是一款优秀的收集器，它最主要的优点在名字上已经体现出来：并发收集、低停顿，一些官方公开文档里面也称之为“并发低停顿收集器”（Concurrent Low Pause Collector）。CMS 收集器是 HotSpot 虚拟机追求低停顿的第一次成功尝试，但是它还远达不到完美的程度，至少有以下三个明显的缺点：
1. CMS 收集器对处理器资源非常敏感
    事实上，面向并发设计的程序都对处理器资源比较敏感。在并发阶段，它虽然不会导致用户线程停顿，但却会因为占用了一部分线程（或者说处理器的计算能力）而导致应用程序变慢，降低总吞吐量。
    
    CMS 默认启动的**回收线程数是（处理器核心数量 +3）/ 4**，也就是说，如果处理器核心数在四个或以上，并发回收时垃圾收集线程只占用不超过 25%的处理器运算资源，并且会随着处理器核心数量的增加而下降。但是**当处理器核心数量不足四个时， CMS 对用户程序的影响就可能变得很大**。如果应用本来的处理器负载就很高，还要分出一半的运算能力去执行收集器线程，就可能导致用户程序的执行速度忽然大幅降低。
2. CMS 收集器无法处理“浮动垃圾”（Floating Garbage）
    由于 CMS 收集器无法处理“浮动垃圾”（Floating Garbage），有可能出现 `Con-current Mode Failure` 失败进而导致另一次完全“Stop The World”的 Full GC 的产生。
    
    在 CMS 的并发标记和并发清理阶段，用户线程是还在继续运行并伴随有新的垃圾对象不断产生，但这一部分垃圾对象是出现在标记过程结束以后，CMS 无法在当次收集中处理掉它们，只好留待下一次垃圾收集时再清理掉。这一部分垃圾就称为“浮动垃圾”。同样也是由于在垃圾收集阶段用户线程还需要持续运行，那就还需要预留足够内存空间提供给用户线程使用，因此 CMS 收集器不能像其他收集器那样等待到老年代几乎完全被填满了再进行收集，必须预留一部分空间供并发收集时的程序运作使用。
    
    **要是 CMS 运行期间预留的内存无法满足程序分配新对象的需要，就会出现一次“并发失败”（Concurrent Mode Failure），这时候虚拟机将不得不启动后备预案：冻结用户线程的执行，临时启用 Serial Old 收集器来重新进行老年代的垃圾收集**，但这样停顿时间就很长了。所以参数 `-XX:CMSInitiatingOccupancyFraction` 设置得太高将会很容易导致大量的并发失败产生，性能反而降低，用户应在生产环境中根据实际应用情况来权衡设置。
3. CMS 是一款基于“标记-清除”算法实现的收集器，这意味着收集结束时会有大量空间碎片产生
    空间碎片过多时，将会给大对象分配带来很大麻烦，往往会出现老年代还有很多剩余空间，但就是无法找到足够大的连续空间来分配当前对象，而不得不提前触发一次 Full GC 的情况。
    
    在 CMS 收集器不得不进行 Full GC 时开启内存碎片的合并整理过程时，由于这个内存整理必须移动存活对象，是无法并发的。这样空间碎片问题是解决了，但停顿时间又会变长。

#### 1.2.4.3. 全堆收集器

作为 CMS 收集器的替代者和继承人，设计者们希望做出一款能够建立起“停顿时间模型”（Pause Prediction Model）的收集器：**支持指定在一个长度为 M 毫秒的时间片段内，消耗在垃圾收集上的时间大概率不超过 N 毫秒这样的目标**，这几乎已经是实时 Java（RTSJ）的中软实时垃圾收集器特征了。

那具体要怎么做才能实现这个目标呢？首先要有一个思想上的改变，在 G1 收集器出现之前的所有其他收集器，包括 CMS 在内，垃圾收集的目标范围要么是整个新生代（Minor GC），要么就是整个老年代（Major GC），再要么就是整个 Java 堆（Full GC）。而 G1 跳出了这个樊笼，它可以面向堆内存任何部分来组成回收集（Collection Set，一般简称 CSet）进行回收，**衡量标准不再是它属于哪个分代，而是哪块内存中存放的垃圾数量最多，回收收益最大**，这就是 G1 收集器的 Mixed GC 模式。

##### 1.2.4.3.1. GarbageFirst 收集器

G1 开创的基于 Region 的堆内存布局是它能够实现这个目标的关键。虽然 G1 也仍是遵循分代收集理论设计的，但其堆内存的布局与其他收集器有非常明显的差异：**G1 不再坚持固定大小以及固定数量的分代区域划分，而是把连续的 Java 堆划分为多个大小相等的独立区域（Region）**，每一个 Region 都可以根据需要，扮演新生代的 Eden 空间、Survivor 空间，或者老年代空间。收集器能够对扮演不同角色的 Region 采用不同的策略去处理，这样无论是新创建的对象还是已经存活了一段时间、熬过多次收集的旧对象都能获取很好的收集效果。

Region 中还有一类特殊的 Humongous 区域，专门用来存储大对象。G1 认为只要大小超过了一个 Region 容量一半的对象即可判定为大对象。每个 Region 的大小可以通过参数- `XX:G1HeapRegionSize` 设定，取值范围为 1MB～32MB，且应为 2 的 N 次幂。而对于那些超过了整个 Region 容量的超级大对象，将会被存放在 N 个连续的 Humongous Region 之中，G1 的大多数行为都把 Humongous Region 作为老年代的一部分来进行看待，如图 3-12 所示。

虽然 G1 仍然保留新生代和老年代的概念，但新生代和老年代不再是固定的了，它们都是一系列区域（不需要连续）的动态集合。G1 收集器之所以能建立可预测的停顿时间模型，是因为它将 Region 作为单次回收的最小单元，即每次收集到的内存空间都是 Region 大小的整数倍，这样可以有计划地避免在整个 Java 堆中进行全区域的垃圾收集。

更具体的处理思路是**让 G1 收集器去跟踪各个 Region 里面的垃圾堆积的“价值”大小**，价值即回收所获得的空间大小以及回收所需时间的经验值，然后在后台维护一个优先级列表，每次根据用户设定允许的收集停顿时间（使用参数 `-XX:MaxGCPauseMillis` 指定，默认值是 200 毫秒），优先处理回收价值收益最大的那些 Region，这也就是“Garbage First”名字的由来。这种使用 Region 划分内存空间，以及具有优先级的区域回收方式，保证了 G1 收集器在有限的时间内获取尽可能高的收集效率。

![image.png](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202308131804694.png)

G1 收集器至少有（不限于）以下这些关键的细节问题需要妥善解决：
- 跨 Region 引用对象如何解决
    可以使用记忆集避免全堆作为 GC Roots 扫描，但在 G1 收集器上记忆集的应用其实要复杂很多，它的每个 Region 都维护有自己的记忆集，这些记忆集会记录下别的 Region 指向自己的指针，并标记这些指针分别在哪些卡页的范围之内。

    G1 的记忆集在存储结构的本质上是一种哈希表，Key 是别的 Region 的起始地址，Value 是一个集合，里面存储的元素是卡表的索引号。这种“双向”的卡表结构（卡表是“我指向谁”，这种结构还记录了“谁指向我”）比原来的卡表实现起来更复杂，同时由于 Region 数量比传统收集器的分代数量明显要多得多，因此 **G1 收集器要比其他的传统垃圾收集器有着更高的内存占用负担**。根据经验，G1 至少要耗费大约相当于 Java 堆容量 10%至 20%的额外内存来维持收集器工作。
- 并发标记阶段如何保证收集线程与用户线程互不干扰地运行
    CMS 收集器采用增量更新算法实现，而 G1 收集器则是通过原始快照（SATB）算法来实现的。

    此外，垃圾收集对用户线程的影响还体现在回收过程中新创建对象的内存分配上，程序要继续运行就肯定会持续有新对象被创建，G1 为每一个 Region 设计了两个名为 TAMS（Top at Mark Start）的指针，把 Region 中的一部分空间划分出来用于并发回收过程中的新对象分配，并发回收时新分配的对象地址都必须要在这两个指针位置以上。G1 收集器默认在这个地址以上的对象是被隐式标记过的，即默认它们是存活的，不纳入回收范围。与 CMS 中的“Concurrent Mode Failure”失败会导致 Full GC 类似，**如果内存回收的速度赶不上内存分配的速度， G1 收集器也要被迫冻结用户线程执行，导致 Full GC 而产生长时间“Stop The World”**。
- 怎样建立起可靠的停顿预测模型
    用户通过 `-XX:MaxGCPauseMillis` 参数指定的停顿时间只意味着垃圾收集发生之前的期望值，但 G1 收集器要怎么做才能满足用户的期望呢？

    G1 收集器的停顿预测模型是以衰减均值（Decaying Average）为理论基础来实现的，在垃圾收集过程中，G1 收集器会记录每个 Region 的回收耗时、每个 Region 记忆集里的脏卡数量等各个可测量的步骤花费的成本，并分析得出平均值、标准偏差、置信度等统计信息。这里强调的“衰减平均值”是指它会比普通的平均值更容易受到新数据的影响，平均值代表整体平均状态，但衰减平均值更准确地代表“最近的”平均状态。换句话说，**Region 的统计状态越新越能决定其回收的价值**。然后通过这些信息预测现在开始回收的话，由哪些 Region 组成回收集才可以在不超过期望停顿时间的约束下获得最高的收益。

如果我们不去计算用户线程运行过程中的动作（如使用写屏障维护记忆集的操作），G1 收集器的运作过程大致可划分为以下四个步骤：
1. 初始标记（Initial Marking）
    仅仅只是标记一下 GC Roots 能直接关联到的对象，并且修改 TAMS 指针的值，让下一阶段用户线程并发运行时，能正确地在可用的 Region 中分配新对象。这个阶段需要停顿线程，但耗时很短，而且是借用进行 Minor GC 的时候同步完成的，所以 G1 收集器在这个阶段实际并没有额外的停顿。
2. 并发标记（Concurrent Marking）
    从 GC Root 开始对堆中对象进行可达性分析，递归扫描整个堆里的对象图，找出要回收的对象，这阶段耗时较长，但可与用户程序并发执行。当对象图扫描完成以后，还要重新处理 SATB 记录下的在并发时有引用变动的对象。
3. 最终标记（Final Marking）
    对用户线程做另一个短暂的暂停，用于处理并发阶段结束后仍遗留下来的最后那少量的 SATB 记录。
4. 筛选回收（Live Data Counting and Evacuation）
    负责更新 Region 的统计数据，对各个 Region 的回收价值和成本进行排序，根据用户所期望的停顿时间来制定回收计划，可以自由选择任意多个 Region 构成回收集，然后把决定回收的那一部分 Region 的存活对象复制到空的 Region 中，再清理掉整个旧 Region 的全部空间。**这里的操作涉及存活对象的移动，是必须暂停用户线程，由多条收集器线程并行完成的**。

![image.png](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202308132019552.png)

从上述阶段的描述可以看出，G1 收集器除了并发标记外，其余阶段也是要完全暂停用户线程的，换言之，**它并非纯粹地追求低延迟，官方给它设定的目标是在延迟可控的情况下获得尽可能高的吞吐量**，所以才能担当起“全功能收集器”的重任与期望。

相比 CMS，G1 的优点有很多，暂且不论可以指定最大停顿时间、分 Region 的内存布局、按收益动态确定回收集这些创新性设计带来的红利，单从最传统的算法理论上看，G1 也更有发展潜力。与 CMS 的“标记-清除”算法不同，G1 从整体来看是基于“标记-整理”算法实现的收集器，但从局部（两个 Region 之间）上看又是基于“标记-复制”算法实现，无论如何，这两种算法都意味着 **G1 运作期间不会产生内存空间碎片**，垃圾收集完成之后能提供规整的可用内存。这种特性有利于程序长时间运行，在程序为大对象分配内存时不容易因无法找到连续内存空间而提前触发下一次收集。

不过，G1 相对于 CMS 仍然不是占全方位、压倒性优势的：
- G1 的记忆集占用更多的内存空间
    虽然 G1 和 CMS 都使用卡表来处理跨代指针，但 G1 的卡表实现更为复杂，而且堆中每个 Region，无论扮演的是新生代还是老年代角色，都必须有一份卡表，这导致 G1 的记忆集（和其他内存消耗）可能会占整个堆容量的 20%乃至更多的内存空间。

    相比起来 CMS 的卡表就相当简单，只有唯一一份，而且只需要处理老年代到新生代的引用，反过来则不需要，由于新生代的对象具有朝生夕灭的不稳定性，引用变化频繁，能省下这个区域的维护开销是很划算的。代价就是当 CMS 发生 Old GC 时（所有收集器中只有 CMS 有针对老年代的 Old GC），要把整个新生代作为 GC Roots 来进行扫描。
- G1 的执行负载会更高
    CMS 用写后屏障来更新维护卡表，而 **G1 除了使用写后屏障来进行同样的（由于 G1 的卡表结构复杂，其实是更烦琐的）卡表维护操作外，为了实现原始快照搜索 （SATB）算法，还需要使用写前屏障来跟踪并发时的指针变化情况**。相比起增量更新算法，**原始快照搜索能够减少并发标记和重新标记阶段的消耗**，避免 CMS 那样在最终标记阶段停顿时间过长的缺点，但是在用户程序运行过程中确实会产生由跟踪引用变化带来的额外负担。

    由于 G1 对写屏障的复杂操作要比 CMS 消耗更多的运算资源，所以 CMS 的写屏障实现是直接的同步操作，而 **G1 就不得不将其实现为类似于消息队列的结构，把写前屏障和写后屏障中要做的事情都放到队列里，然后再异步处理**。

##### 1.2.4.3.2. 分代收集与全堆收集的抉择

从 G1 开始，最先进的垃圾收集器的设计导向都不约而同地变为**追求能够应付应用的内存分配速率 （Allocation Rate），而不追求一次把整个 Java 堆全部清理干净**。这样，应用在分配，同时收集器在收集，只要收集的速度能跟得上对象分配的速度，那一切就能运作得很完美。这种新的收集器设计思路从工程实现上看是从 G1 开始兴起的，所以说 G1 是收集器技术发展的一个里程碑。

### 1.2.5. 低延迟垃圾收集器

衡量垃圾收集器的三项最重要的指标是：**内存占用（Footprint）、吞吐量（Throughput）和延迟（Latency）**，三者共同构成了一个“不可能三角”。三者总体的表现会随技术进步而越来越好，但是要在这三个方面同时具有卓越表现的“完美”收集器是极其困难甚至是不可能的，一款优秀的收集器通常最多可以同时达成其中的两项。

在内存占用、吞吐量和延迟这三项指标里，延迟的重要性日益凸显，越发备受关注。其原因是随着计算机硬件的发展、性能的提升，我们越来越能容忍收集器多占用一点点内存；硬件性能增长，对软件系统的处理能力是有直接助益的，硬件的规格和性能越高，也有助于降低收集器运行时对应用程序的影响，换句话说，吞吐量会更高。但对延迟则不是这样，硬件规格提升，准确地说是内存的扩大，对延迟反而会带来负面的效果，这点也是很符合直观思维的：虚拟机要回收完整的 1TB 的堆内存，毫无疑问要比回收 1GB 的堆内存耗费更多时间。由此，我们就不难理解为何延迟会成为垃圾收集器最被重视的性能指标了。

#### 1.2.5.1. Shenandoah 收集器

##### 1.2.5.1.1. 整体链路

Shenandoah 也是使用基于 Region 的堆内存布局，同样有着用于存放大对象的 Humongous Region，默认的回收策略也同样是优先处理回收价值最大的 Region，但在管理堆内存方面，它与 G1 至少有三个明显的不同之处：
1. 支持并发的整理算法
    G1 的回收阶段是可以多线程并行的，但却不能与用户线程并发，支持并发的整理算法是 Shenandoah 最核心的功能。
2. Shenandoah（目前）是默认不使用分代收集的
    换言之，不会有专门的新生代 Region 或者老年代 Region 的存在，没有实现分代，并不是说分代对 Shenandoah 没有价值，这更多是出于性价比的权衡，基于工作量上的考虑而将其放到优先级较低的位置上。
3.  Shenandoah 摒弃了在 G1 中耗费大量内存和计算资源去维护的记忆集，改用名为“连接矩阵”（Connection Matrix）的全局数据结构来记录跨 Region 的引用关系
    降低了处理跨代指针时的记忆集维护消耗，也降低了伪共享问题的发生概率。连接矩阵可以简单理解为一张二维表格，如果 Region N 有对象指向 Region M，就在表格的 N 行 M 列中打上一个标记。在回收时通过连接矩阵就可以得出哪些 Region 之间产生了跨代引用。

    ![image.png](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202308132034701.png)

Shenandoah 收集器的工作过程大致可以划分为以下九个阶段：
1. 初始标记（Initial Marking）
    与 G1 一样，首先标记与 GC Roots 直接关联的对象，这个阶段仍是“Stop The World”的，但停顿时间与堆大小无关，只与 GC Roots 的数量相关。
2. 并发标记（Concurrent Marking）
    与 G1 一样，遍历对象图，标记出全部可达的对象，这个阶段是与用户线程一起并发的，时间长短取决于堆中存活对象的数量以及对象图的结构复杂程度。
3. 最终标记（Final Marking）
    与 G1 一样，处理剩余的 SATB 扫描，并在这个阶段统计出回收价值最高的 Region，将这些 Region 构成一组回收集（Collection Set）。最终标记阶段也会有一小段短暂的停顿。
4. 并发清理（Concurrent Cleanup）
    这个阶段用于清理那些整个区域内连一个存活对象都没有找到的 Region（这类 Region 被称为 Immediate Garbage Region）。
5. 并发回收（Concurrent Evacuation）
    并发回收阶段是 Shenandoah 与之前 HotSpot 中其他收集器的核心差异。在这个阶段，Shenandoah 要**把回收集里面的存活对象先复制一份到其他未被使用的 Region 之中**。复制对象这件事情如果将用户线程冻结起来再做那是相当简单的，但如果两者必须要同时并发进行的话，就变得复杂起来了。
    
    其困难点是在移动对象的同时，用户线程仍然可能不停对被移动的对象进行读写访问，移动对象是一次性的行为，但移动之后整个内存中所有指向该对象的引用都还是旧对象的地址，这是很难一瞬间全部改变过来的。对于并发回收阶段遇到的这些困难，Shenandoah 将会**通过读屏障和被称为“Brooks Pointers”的转发指针来解决**。并发回收阶段运行的时间长短取决于回收集的大小。
6. 初始引用更新（Initial Update Reference）
    并发回收阶段复制对象结束后，还需要把堆中所有指向旧对象的引用修正到复制后的新地址，这个操作称为引用更新。
    
    引用更新的初始化阶段实际上并未做什么具体的处理，设立这个阶段只是为了建立一个线程集合点，确保所有并发回收阶段中进行的收集器线程都已完成分配给它们的对象移动任务而已。初始引用更新时间很短，会产生一个非常短暂的停顿。
7. 并发引用更新（Concurrent Update Reference）
    真正开始进行引用更新操作，这个阶段是与用户线程一起并发的，时间长短取决于内存中涉及的引用数量的多少。并发引用更新与并发标记不同，它不再需要沿着对象图来搜索，只需要按照内存物理地址的顺序，线性地搜索出引用类型，把旧值改为新值即可。
8. 最终引用更新（Final Update Reference）
    解决了堆中的引用更新后，还要修正存在于 GC Roots 中的引用。这个阶段是 Shenandoah 的最后一次停顿，停顿时间只与 GC Roots 的数量相关。
9. 并发清理（Concurrent Cleanup）
    经过并发回收和引用更新之后，整个回收集中所有的 Region 已再无存活对象，这些 Region 都变成 Immediate Garbage Regions 了，最后再调用一次并发清理过程来回收这些 Region 的内存空间，供以后新对象分配使用。

其中三个核心的并发阶段是（并发标记、并发回收、并发引用更新），图中黄色的区域代表的是被选入回收集的 Region，绿色部分代表还存活的对象，蓝色是用户线程可以用来分配对象的内存 Region：

![image.png](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202308132043415.png)

图中展示了 Shenandoah 三个并发阶段的工作过程，表示了并发标记阶段如何找出回收对象确定回收集，并发回收阶段如何移动回收集中的存活对象，并发引用更新阶段如何将指向回收集中存活对象的所有引用全部修正，此后回收集便不存在任何引用可达的存活对象了。

##### 1.2.5.1.2. 转发指针

1984 年，Rodney A.Brooks 在论文《Trading Data Space for Reduced Time and Code Space in Real-Time Garbage Collection on Stock Hardware》中提出了使用转发指针（Forwarding Pointer，也常被称为 Indirection Pointer）来实现对象移动与用户程序并发的一种解决方案。此前，要做类似的并发操作，通常是在被移动对象原有的内存上设置保护陷阱（Memory Protection Trap），一旦用户程序访问到归属于旧对象的内存空间就会产生自陷中段，进入预设好的异常处理器中，再由其中的代码逻辑把访问转发到复制后的新对象上。虽然确实能够实现对象移动与用户线程并发，但是如果没有操作系统层面的直接支持，这种方案将导致用户态频繁切换到核心态代价是非常大的，不能频繁使用。

Shenandoah 收集器不需要用到内存保护陷阱，而是**在原有对象布局结构的最前面统一增加一个新的引用字段**，在正常不处于并发移动的情况下，该引用指向对象自己：

![image.png](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202308132050274.png)

从结构上来看，Brooks 提出的转发指针与某些早期 Java 虚拟机使用过的[[JVM#1.1.2.3. 对象的访问布局|句柄定位]]有一些相似之处，两者都是**一种间接性的对象访问方式**，差别是句柄通常会统一存储在专门的句柄池中，而转发指针是分散存放在每一个对象头前面。

转发指针加入后带来的收益是当对象拥有了一份新的副本时，只需要修改一处指针的值，即旧对象上转发指针的引用位置，使其指向新对象，便可将所有对该对象的访问转发到新的副本上。这样只要旧对象的内存仍然存在，未被清理掉，虚拟机内存中所有通过旧引用地址访问的代码便仍然可用，都会被自动转发到新对象上继续工作。

但是既然是间接访问对象，那么其存在以下缺点：
- 每次对象访问会带来一次额外的转向开销
- 多线程竞争问题
    Brooks 形式的转发指针在设计上决定了它是必然会出现多线程竞争问题的，如果收集器线程与用户线程发生的只是并发读取，那无论读到旧对象还是新对象上的字段，返回的结果都应该是一样的，这个场景还可以有一些“偷懒”的处理余地；但如果发生的是并发写入，就一定必须保证写操作只能发生在新复制的对象上，而不是写入旧对象的内存中。

    实际上 Shenandoah 收集器是**通过比较并交换（Compare And Swap，CAS）操作确性的，来保证并发时对象的访问正性的**。
- 执行频率问题
    转发指针另一点必须注意的是执行频率的问题，尽管通过对象头上的 Brooks Pointer 来保证并发时原对象与复制对象的访问一致性，但对象的读取、写入，对象的比较，为对象哈希值计算，用对象加锁等，这些操作都属于对象访问的范畴，它们在代码中比比皆是，要覆盖全部对象访问操作，Shenandoah 不得不同时设置读、写屏障去拦截。之前介绍其他收集器时，或者是用于维护卡表，或者是用于实现并发标记，写屏障已被使用多次，累积了不少的处理任务了，这些写屏障有相当一部分在 Shenandoah 收集器中依然要被使用到。
    
    除此以外，为了实现 Brooks Pointer，Shenandoah 在读、写屏障中都加入了额外的转发处理，尤其是**使用读屏障的代价**，这是比写屏障更大的。代码里对象读取的出现频率要比对象写入的频率高出很多，读屏障数量自然也要比写屏障多得多，所以读屏障的使用必须更加谨慎，不允许任何的重量级操作。
    
    Shenandoah 是本书中第一款使用到读屏障的收集器，它的开发者也意识到数量庞大的读屏障带来的性能开销会是 Shenandoah 被诟病的关键点之一，所以计划在 JDK 13 中将 Shenandoah 的内存屏障模型改进为基于引用访问屏障（Load Reference Barrier）的实现，所谓“引用访问屏障”是指内存屏障只拦截对象中数据类型为引用类型的读写操作，而不去管原生数据类型等其他非引用字段的读写，这能够省去大量对原生类型、对象比较、对象加锁等场景中设置内存屏障所带来的消耗。

#### 1.2.5.2. ZGC 收集器

与 Shenandoah 和 G1 一样，ZGC 也采用基于 Region 的堆内存布局，但与它们不同的是，ZGC 的 Region 具有动态性——动态创建和销毁，以及动态的区域容量大小。在 x64 硬件平台下，ZGC 的 Region 可以具有大、中、小三类容量：
- 小型 Region（Small Region）：容量固定为 2MB，用于放置小于 256KB 的小对象。
- 中型 Region（Medium Region）：容量固定为 32MB，用于放置大于等于 256KB 但小于 4MB 的对象。
- 大型 Region（Large Region）：容量不固定，可以动态变化，但必须为 2MB 的整数倍，用于放置 4MB 或以上的大对象。
    每个大型 Region 中只会存放一个大对象，这也预示着虽然名字叫作“大型 Region”，但它的实际容量完全有可能小于中型 Region，最小容量可低至 4MB。大型 Region 在 ZGC 的实现中是不会被重分配的，因为复制一个大对象的代价非常高昂。

![image.png](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202308132121810.png)

##### 1.2.5.2.1. 染色指针

ZGC 的核心问题——并发整理算法的实现，**ZGC 收集器有一个标志性的设计是它采用的染色指针技术**（Colored Pointer，其他类似的技术中可能将它称为 Tag Pointer 或者 Version Pointer）。

追踪式收集算法的标记阶段就可能存在只跟指针打交道而不必涉及指针所引用的对象本身的场景。例如对象标记的过程中需要给对象打上三色标记，这些标记本质上就只和对象的引用有关，而与对象本身无关——某个对象只有它的引用关系能决定它存活与否，对象上其他所有的属性都不能够影响它的存活判定结果。

HotSpot 虚拟机的几种收集器有不同的标记实现方案，有的把标记直接记录在对象头上（如 Serial 收集器），有的把标记记录在与对象相互独立的数据结构上（如 G1、Shenandoah 使用了一种相当于堆内存的 1/64 大小的，称为 BitMap 的结构来记录标记信息），而 ZGC 的染色指针是最直接的、最纯粹的，它直接把标记信息记在引用对象的指针上。这时，与其说可达性分析是遍历对象图来标记对象，还不如说是遍历“引用图”来标记“引用”了。

**染色指针是一种直接将少量额外的信息存储在指针上的技术**，ZGC 的染色指针技术盯上了指针的高 4 位，将其提取出来存储四个标志信息。通过这些标志位，虚拟机可以直接从指针中看到其引用对象的三色标记状态、是否进入了重分配集（即被移动过）、是否只能通过 `finalize ()` 方法才能被访问到。当然，由于这些标志位进一步压缩了原本就只有 46 位的地址空间，也直接导致 ZGC 能够管理的内存不可以超过 4TB：

![image.png](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202308132127305.png)

使用染色指针可以带来以下好处：
- 一旦某个 Region 的存活对象被移走之后，这个 Region 立即就能够被释放和重用
    这点相比起 Shenandoah 是一个颇大的优势，使得**理论上只要还有一个空闲 Region，ZGC 就能完成收集，而 Shenandoah 需要等到引用更新阶段结束以后才能释放回收集中的 Region**。这意味着堆中几乎所有对象都存活的极端情况，需要 1∶1 复制对象到新 Region 的话，就必须要有一半的空闲 Region 来完成收集。
- 染色指针可以大幅减少在垃圾收集过程中内存屏障的使用数量
    设置内存屏障，尤其是写屏障的目的通常是为了记录对象引用的变动情况，如果将这些信息直接维护在指针中，显然就可以省去一些专门的记录操作。实际上，到目前为止 ZGC 都并未使用任何写屏障，只使用了读屏障（一部分是染色指针的功劳，一部分是 **ZGC 现在还不支持分代收集**，天然就没有跨代引用的问题）。
- 染色指针可以作为一种可扩展的存储结构用来记录更多与对象标记、重定位过程相关的数据，以便日后进一步提高性能
    现在 Linux 下的 64 位指针还有前 18 位并未使用，它们虽然不能用来寻址，却可以通过其他手段用于信息记录。如果开发了这 18 位，既可以腾出已用的 4 个标志位，将 ZGC 可支持的最大堆内存从 4TB 拓展到 64TB，也可以利用其余位置再存储更多的标志，譬如存储一些追踪信息来让垃圾收集器在移动对象时能将低频次使用的对象移动到不常访问的内存区域。

要顺利应用染色指针有一个必须解决的前置问题：Java 虚拟机作为一个普普通通的进程，这样随意重新定义内存中某些指针的其中几位，操作系统是否支持？处理器是否支持？

得益于虚拟内存的存在，不同层次的虚拟内存到物理内存的转换关系可以在硬件层面、操作系统层面或者软件进程层面实现，如何完成地址转换，是一对一、多对一还是一对多的映射，也可以根据实际需要来设计。

Linux/x86-64 平台上的 ZGC 使用了多重映射（Multi-Mapping）将多个不同的虚拟内存地址映射到同一个物理内存地址上，这是一种多对一映射，意味着 ZGC 在虚拟内存中看到的地址空间要比实际的堆内存容量来得更大。把染色指针中的标志位看作是地址的分段符，那只要将这些不同的地址段都映射到同一个物理内存空间，经过多重映射转换后，就可以使用染色指针正常进行寻址了

![image.png](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202308132141779.png)

##### 1.2.5.2.2. 整体链路

ZGC 的运作过程大致可划分为以下四个大的阶段。全部四个阶段都是可以并发执行的，仅是两个阶段中间会存在短暂的停顿小阶段，这些小阶段，譬如初始化 GC Root 直接关联对象的 Mark Start，与之前 G1 和 Shenandoah 的 Initial Mark 阶段并没有什么差异：
1. 并发标记（Concurrent Mark）
    与 G1、Shenandoah 一样，并发标记是遍历对象图做可达性分析的阶段，**前后也要经过类似于 G1、Shenandoah 的初始标记、最终标记（尽管 ZGC 中的名字不叫这些）的短暂停顿，而且这些停顿阶段所做的事情在目标上也是相类似的**。
    
    与 G1、Shenandoah 不同的是，ZGC 的标记是在指针上而不是在对象上进行的，标记阶段会更新染色指针中的 Marked 0、Marked 1 标志位。
2. 并发预备重分配（Concurrent Prepare for Relocate）
    这个阶段需要根据特定的查询条件统计得出本次收集过程要清理哪些 Region，将这些 Region 组成重分配集（Relocation Set）。重分配集与 G1 收集器的回收集（Collection Set）还是有区别的，ZGC 划分 Region 的目的并非为了像 G1 那样做收益优先的增量回收。相反，ZGC 每次回收都会扫描所有的 Region，用范围更大的扫描成本换取省去 G1 中记忆集的维护成本。
    
    因此，**ZGC 的重分配集只是决定了里面的存活对象会被重新复制到其他的 Region 中，里面的 Region 会被释放**，而并不能说回收行为就只是针对这个集合里面的 Region 进行，因为标记过程是针对全堆的。此外，在 JDK 12 的 ZGC 中开始支持的类卸载以及弱引用的处理，也是在这个阶段中完成的。
3. 并发重分配（Concurrent Relocate）
    重分配是 ZGC 执行过程中的核心阶段，这个过程要把重分配集中的存活对象复制到新的 Region 上，并**为重分配集中的每个 Region 维护一个转发表（Forward Table），记录从旧对象到新对象的转向关系**。
    
    使用染色指针有以下好处：
    - ZGC 收集器能仅从引用上就明确得知一个对象是否处于重分配集之中
        如果用户线程此时并发访问了位于重分配集中的对象，这次访问将会被预置的内存屏障所截获，然后立即根据 Region 上的转发表记录将访问转发到新复制的对象上，并同时修正更新该引用的值，使其直接指向新对象，ZGC 将这种行为称为指针的“自愈”（SelfHealing）能力。
    - 仅做一次转发
        是只有第一次访问旧对象会陷入转发，也就是只慢一次，对比 Shenandoah 的 Brooks 转发指针，那是每次对象访问都必须付出的固定开销，简单地说就是每次都慢，因此 ZGC 对用户程序的运行时负载要比 Shenandoah 来得更低一些。
     - 一旦重分配集中某个 Region 的存活对象都复制完毕后，这个 Region 就可以立即释放用于新对象的分配
        Region 释放后转发表还得留着不能释放掉、哪怕堆中还有很多指向这个对象的未更新指针也没有关系，这些旧指针一旦被使用，它们都是可以自愈的。
4. 并发重映射（Concurrent Remap）
    重映射所做的就是**修正整个堆中指向重分配集中旧对象的所有引用**，这一点从目标角度看是与 Shenandoah 并发引用更新阶段一样的，但是 ZGC 的并发重映射并**不是一个必须要“迫切”去完成的任务**。
    
    **即使是旧引用，它也是可以自愈的**，最多只是第一次使用时多一次转发和修正操作。**重映射清理这些旧引用的主要目的是为了不变慢**（还有清理结束后可以释放转发表这样的附带收益），所以说这并不是很“迫切”。
    
    因此，**ZGC 很巧妙地把并发重映射阶段要做的工作，合并到了下一次垃圾收集循环中的并发标记阶段里去完成**，反正它们都是要遍历所有对象的，这样合并就节省了一次遍历对象图对象关系的开销。一旦所有指针都被修正之后，原来记录新旧对象关系的转发表就可以释放掉了。

ZGC 就完全没有使用记忆集，它甚至连分代都没有，连像 CMS 中那样只记录新生代和老年代间引用的卡表也不需要，因而完全没有用到写屏障，所以给用户线程带来的运行负担也要小得多。可是，必定要有优有劣才会称作权衡，**ZGC 的这种选择也限制了它能承受的对象分配速率不会太高**。

可以想象以下场景来理解 ZGC 的这个劣势：ZGC 准备要对一个很大的堆做一次完整的并发收集，假设其全过程要持续十分钟以上，在这段时间里面，由于应用的对象分配速率很高，将创造大量的新对象，这些新对象很难进入当次收集的标记范围，通常就只能全部当作存活对象来看待——尽管其中绝大部分对象都是朝生夕灭的，这就产生了大量的浮动垃圾。

如果这种高速分配持续维持的话，每一次完整的并发收集周期都会很长，回收到的内存空间持续小于期间并发产生的浮动垃圾所占的空间，堆中剩余可腾挪的空间就越来越小了。目前唯一的办法就是尽可能地增加堆容量大小，获得更多喘息的时间。但是若要从根本上提升 ZGC 能够应对的对象分配速率，还是需要引入分代收集，让新生对象都在一个专门的区域中创建，然后专门针对这个区域进行更频繁、更快的收集。Azul 的 C4 收集器实现了分代收集后，能够应对的对象分配速率就比不分代的 PGC 收集器提升了十倍之多。

ZGC 还有一个常在技术资料上被提及的优点是支持“NUMA-Aware”的内存分配，在 NUMA 架构下，ZGC 收集器会优先尝试在请求线程当前所处的处理器的本地内存上分配对象，以保证高效内存访问。

### 1.2.6. 内存分配与回收策略

Java 技术体系的自动内存管理，最根本的目标是自动化地解决两个问题：自动给对象分配内存以及自动回收分配给对象的内存。

对象的内存分配，从概念上讲，应该都是在堆上分配（而实际上也有可能经过即时编译后被拆散为标量类型并间接地在栈上分配）。在经典分代的设计下，新生对象通常会分配在新生代中，少数情况下（例如对象大小超过一定阈值）也可能会直接分配在老年代。对象分配的规则并不是固定的，《Java 虚拟机规范》并未规定新对象的创建和存储细节，这取决于虚拟机当前使用的是哪一种垃圾收集器，以及虚拟机中与内存相关的参数的设定。

在 HotSpot 虚拟机下，内存分配有以下策略：
- 对象优先在 Eden 分配
    大多数情况下，对象在新生代 Eden 区中分配。当 Eden 区没有足够空间进行分配时，虚拟机将发起一次 Minor GC。
- 大对象直接进入老年代
    大对象就是指需要大量连续内存空间的 Java 对象，最典型的大对象便是那种很长的字符串，或者元素数量很庞大的数组。
    
    在 Java 虚拟机中要避免大对象的原因是，在分配空间时，它容易导致内存明明还有不少空间时就提前触发垃圾收集，以获取足够的连续空间才能安置好它们。而当复制对象时，大对象就意味着高额的内存复制开销。HotSpot 虚拟机提供了 `-XX:PretenureSizeThreshold` 参数，指定大于该设置值的对象直接在老年代分配，这样做的目的就是避免在 Eden 区及两个 Survivor 区之间来回复制，产生大量的内存复制操作。
- 长期存活的对象将进入老年代
    HotSpot 虚拟机中多数收集器都采用了分代收集来管理堆内存，那内存回收时就必须能决策哪些存活对象应当放在新生代，哪些存活对象放在老年代中。
    
    为做到这点，虚拟机给每个对象定义了一个对象年龄（Age）计数器，存储在对象头中。对象通常在 Eden 区里诞生，如果经过第一次 Minor GC 后仍然存活，并且能被 Survivor 容纳的话，该对象会被移动到 Survivor 空间中，并且将其对象年龄设为 1 岁。对象在 Survivor 区中每熬过一次 Minor GC，年龄就增加 1 岁，当它的年龄增加到一定程度（默认为 15），就会被晋升到老年代中。对象晋升老年代的年龄阈值，可以通过参数 `-XX:MaxTenuringThreshold` 设置。
- 动态对象年龄判定
    为了能更好地适应不同程序的内存状况，HotSpot 虚拟机并不是永远要求对象的年龄必须达到 `XX:MaxTenuringThreshold` 才能晋升老年代，如果**在 Survivor 空间中相同年龄所有对象大小的总和大于 Survivor 空间的一半，年龄大于或等于该年龄的对象就可以直接进入老年代**，无须等到 `-XX:MaxTenuringThreshold` 中要求的年龄。
- 空间分配担保
    在发生 Minor GC 之前，虚拟机必须先检查老年代最大可用的连续空间是否大于新生代所有对象总空间，如果这个条件成立，那这一次 Minor GC 可以确保是安全的。如果不成立，则虚拟机会继续检查老年代最大可用的连续空间是否大于历次晋升到老年代对象的平均大小，如果大于，将尝试进行一次 Minor GC，尽管这次 Minor GC 是有风险的；如果小于，那这时就要改为进行一次 Full GC。

    在 JDK 6 Update 24 之前，`-XX:HandlePromotionFailure` 参数会控制虚拟机的空间分配担保策略是否生效。

# 2. 虚拟机执行子系统

## 2.1. 类加载机制

Java 虚拟机把描述类的数据从 Class 文件加载到内存，并对数据进行校验、转换解析和初始化，最终形成可以被虚拟机直接使用的 Java 类型，这个过程被称作虚拟机的类加载机制。

与那些在编译时需要进行连接的语言不同，在 Java 语言里面，类型的加载、连接和初始化过程都是在程序运行期间完成的，这种策略让 Java 语言进行提前编译会面临额外的困难，也会让类加载时稍微增加一些性能开销，但是却为 Java 应用提供了极高的扩展性和灵活性，Java 天生可以动态扩展的语言特性就是依赖运行期动态加载和动态连接这个特点实现的。

### 2.1.1. 类加载时机

一个类型从被加载到虚拟机内存中开始，到卸载出内存为止，它的整个生命周期将会经历加载 （Loading）、验证（Verification）、准备（Preparation）、解析（Resolution）、初始化 （Initialization）、使用（Using）和卸载（Unloading）七个阶段，其中验证、准备、解析三个部分统称为连接（Linking）：

![image.png](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202308160105723.png)

加载、验证、准备、初始化和卸载这五个阶段的顺序是确定的，类型的加载过程必须**按照这种顺序按部就班地开始（开始并不意味着完成）**，**而解析阶段则不一定：它在某些情况下可以在初始化阶段之后再开始，这是为了支持 Java 语言的运行时绑定特性**。

关于在什么情况下需要开始类加载过程的第一个阶段“加载”，《Java 虚拟机规范》中并没有进行强制约束，这点可以交给虚拟机的具体实现来自由把握。但是对于初始化阶段，《Java 虚拟机规范》则是严格规定了有且只有六种情况必须立即对类进行“初始化”（而加载、验证、准备自然需要在此之前开始）：
1. 遇到 new、getstatic、putstatic 或 invokestatic 这四条字节码指令时，如果类型没有进行过初始化，则需要先触发其初始化阶段。
    能够生成这四条指令的典型 Java 代码场景有：
    - 使用 new 关键字实例化对象的时候
    - 读取或设置一个类型的静态字段（被 final 修饰、已在编译期把结果放入常量池的静态字段除外） 的时候
    - 调用一个类型的静态方法的时候
2. 使用 `java.lang.reflect` 包的方法对类型进行反射调用的时候，如果类型没有进行过初始化，则需要先触发其初始化
3. 当初始化类的时候，如果发现其父类还没有进行过初始化，则需要先触发其父类的初始化
4. 当虚拟机启动时，用户需要指定一个要执行的主类（包含 `main()` 方法的那个类），虚拟机会先初始化这个主类
5. ）当使用 JDK 7 新加入的动态语言支持时，如果一个 `java.lang.invoke.MethodHandle` 实例最后的解析结果为 REF_getStatic、REF_putStatic、REF_invokeStatic、REF_newInvokeSpecial 四种类型的方法句柄，并且这个方法句柄对应的类没有进行过初始化，则需要先触发其初始化
6. 当一个接口中定义了 JDK 8 新加入的默认方法（被 default 关键字修饰的接口方法）时，如果有这个接口的实现类发生了初始化，那该接口要在其之前被初始化

对于这六种会触发类型进行初始化的场景，《Java 虚拟机规范》中使用了一个非常强烈的限定语 ——“有且只有”，这六种场景中的行为称为对一个类型进行主动引用。**除此之外，所有引用类型的方式都不会触发初始化，称为被动引用**。

被动引用的示例场景如下：
1. 通过子类引用父类的静态字段，不会导致子类初始化
    
    ```java
    # value 属性在父类中定义，通过子类引用并不会导致子类被初始化
    System.out.println(SubClass.value);
    ```
2. 通过数组定义来引用类，不会触发此类的初始化
    ```java
    # 并不会初始化SuperClass类
    SuperClass[] sca = new SuperClass[10];
    ```
    
    这段代码里面触发了另一个名为 `Lorg.fenixsoft.classloading.SuperClass` 的类的初始化阶段，对于用户代码来说，这并不是一个合法的类型名称，它是一个由虚拟机自动生成的、直接继承于 Object 的子类，创建动作由字节码指令 newarray 触发。
    
    这个类代表了一个元素类型为 SuperClass 的一维数组，数组中应有的属性和方法（用户可直接使用的只有被修饰为 public 的 length 属性和 clone ()方法）都实现在这个类里。Java 语言中对数组的访问要比 C/C++相对安全，很大程度上就是因为这个类包装了数组元素的访问  C/C++中则是直接翻译为对数组指针的移动。在 Java 语言里，当检查到发生数组越界时会抛出 `ArrayIndexOutOfBoundsException` 异常，避免了直接造成非法内存访问。
3. 常量在编译阶段会存入调用类的常量池中，本质上没有直接引用到定义常量的类，因此不会触发定义常量的类的初始化
    ```java
    public static final String HELLOWORLD = "hello world";
    System.out.println(ConstClass.HELLOWORLD);
    ```

接口的加载过程与类加载过程稍有不同，针对接口需要做一些特殊说明：接口也有初始化过程，这点与类是一致的，而接口中不能使用“static{}”语句块，但编译器仍然会为接口生成 `<clinit>()` 类构造器，用于**初始化接口中所定义的成员变量**。

接口与类真正有所区别的是前面讲述的六种“有且仅有”需要触发初始化场景中的第三种： 当一个类在初始化时，要求其父类全部都已经初始化过了，但是**一个接口在初始化时，并不要求其父接口全部都完成了初始化**，只有在真正使用到父接口的时候（如引用接口中定义的常量）才会初始化。

### 2.1.2. 类加载过程

#### 2.1.2.1. 加载

在加载阶段，Java 虚拟机需要完成以下三件事情：
1. 通过一个类的全限定名来获取定义此类的二进制字节流
    二进制流可以从各种地方获取：ZIP 压缩包、网络、磁盘、数据库、加密文件等等。
2. 将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构
3. 在内存中生成一个代表这个类的 `java.lang.Class` 对象，作为方法区这个类的各种数据的访问入口

对于待加载类的不同类型，也需要采取不同的加载方式：
- 非数组类型的加载
    加载阶段既可以使用 Java 虚拟机里内置的引导类加载器来完成，也可以由用户自定义的类加载器去完成，开发人员通过定义自己的类加载器去控制字节流的获取方式（重写一个类加载器的 `findClass ()` 或 `loadClass ()` 方法），实现根据自己的想法来赋予应用程序获取运行代码的动态性。
- 数组类型的加载
    数组类本身不通过类加载器创建，它是由 Java 虚拟机直接在内存中动态构造出来的。但数组类与类加载器仍然有很密切的关系，因为数组类的元素类型（Element Type，指的是数组去掉所有维度的类型）最终还是要靠类加载器来完成加载，一个数组类（下面简称为 C）创建过程遵循以下规则：
    1. 如果数组的元素类型是引用类型，那就递归采用非数组类型的加载过程去加载这个类型，数组 C 将被标识在加载该组件类型的类加载器的类名称空间上。
    2. 如果数组的组件类型不是引用类型（例如 int\[]数组的组件类型为 int），Java 虚拟机将会把数组 C 标记为与引导类加载器关联。
    3. 数组类的可访问性与它的组件类型的可访问性一致，如果组件类型不是引用类型，它的数组类的可访问性将默认为 public，可被所有的类和接口访问到。

加载阶段结束后，Java 虚拟机外部的二进制字节流就按照虚拟机所设定的格式存储在方法区之中了，方法区中的数据存储格式完全由虚拟机实现自行定义。类型数据妥善安置在方法区之后，会在 Java 堆内存中实例化一个 `java.lang.Class` 类的对象，这个对象将作为程序访问方法区中的类型数据的外部接口。

**加载阶段与连接阶段的部分动作（如一部分字节码文件格式验证动作）是交叉进行的，加载阶段尚未完成，连接阶段可能已经开始**，但这些夹在加载阶段之中进行的动作，仍然属于连接阶段的一部分，这两个阶段的开始时间仍然保持着固定的先后顺序。

#### 2.1.2.2. 验证

验证是连接阶段的第一步，这一阶段的目的是确保 Class 文件的字节流中包含的信息符合《Java 虚拟机规范》的全部约束要求，**保证这些信息被当作代码运行后不会危害虚拟机自身的安全**。

从整体上看，验证阶段大致上会完成下面四个阶段的检验动作：
1. 文件格式验证
    包括但不限于以下类型的验证：
    - 是否以魔数 0xCAFEBABE 开头
    - 主、次版本号是否在当前 Java 虚拟机接受范围之内
    - 常量池的常量中是否有不被支持的常量类型
    - 指向常量的各种索引值中是否有指向不存在的常量或不符合类型的常量
    - Class 文件中各个部分及文件本身是否有被删除的或附加的其他信息
2. 元数据验证：保证不存在与《Java 语言规范》定义相悖的元数据信息
    包括但不限于以下类型的验证：
    - 这个类是否有父类（除了 `java.lang.Object` 之外，所有的类都应当有父类）
    - 这个类的父类是否继承了不允许被继承的类（被 final 修饰的类）
    - 如果这个类不是抽象类，是否实现了其父类或接口之中要求实现的所有方法
3. 字节码验证
    该阶段是整个验证过程中最复杂的一个阶段，主要目的是通过数据流分析和控制流分析，确定程序语义是合法的、符合逻辑的。
    
    包括但不限于以下类型的验证：
    - 保证任意时刻操作数栈的数据类型与指令代码序列都能配合工作
    - 保证任何跳转指令都不会跳转到方法体以外的字节码指令上
    - 保证方法体中的类型转换总是有效的
    
    即使字节码验证阶段中进行了再大量、再严密的检查，也依然不能保证这一点。这里涉及了离散数学中一个很著名的问题——“停机问题”（Halting Problem） ，即**不能通过程序准确地检查出程序是否能在有限的时间之内结束运行**。
4. 符号引用验证
    **最后一个阶段的校验行为发生在虚拟机将符号引用转化为直接引用的时候，这个转化动作将在连接的第三阶段——解析阶段中发生**。
    
    符号引用验证可以看作是对类自身以外（常量池中的各种符号引用）的各类信息进行匹配性校验，通俗来说就是，该类是否缺少或者被禁止访问它依赖的某些外部类、方法、字段等资源。
    包括但不限于以下类型的验证：
    - 符号引用中通过字符串描述的全限定名是否能找到对应的类
    - 在指定类中是否存在符合方法的字段描述符及简单名称所描述的方法和字段
    - 符号引用中的类、字段、方法的可访问性（private、protected、public、\<package>）是否可被当前类访问

**验证阶段对于虚拟机的类加载机制来说，是一个非常重要的、但却不是必须要执行的阶段**，因为验证阶段只有通过或者不通过的差别，只要通过了验证，其后就对程序运行期没有任何影响了。如果程序运行的全部代码（包括自己编写的、第三方包中的、从外部加载的、动态生成的等所有代码）都已经被反复使用和验证过，在生产环境的实施阶段就可以考虑使用 `-Xverify:none` 参数来关闭大部分的类验证措施，以缩短虚拟机类加载的时间。

#### 2.1.2.3. 准备

准备阶段是**正式为类中定义的变量（即静态变量，被 static 修饰的变量）分配内存并设置类变量初始值的阶段**。这时候进行内存分配的仅包括类变量，而不包括实例变量，实例变量将会在对象实例化时随着对象一起分配在 Java 堆中。其次是这里所说的初始值通常情况下是数据类型的零值，如果对于 final 修饰的常量就直接初始化为指定值。

#### 2.1.2.4. 解析

解析阶段是 Java 虚拟机将常量池内的符号引用替换为直接引用的过程：
- 符号引用
    **符号引用以一组符号来描述所引用的目标，符号可以是任何形式的字面量，只要使用时能无歧义地定位到目标即可**。符号引用与虚拟机实现的内存布局无关，引用的目标并不一定是已经加载到虚拟机内存当中的内容。各种虚拟机实现的内存布局可以各不相同，但是它们能接受的符号引用必须都是一致的，因为符号引用的字面量形式明确定义在《Java 虚拟机规范》的 Class 文件格式中。
- 直接引用
    **直接引用是可以直接指向目标的指针、相对偏移量或者是一个能间接定位到目标的句柄**。直接引用是和虚拟机实现的内存布局直接相关的，同一个符号引用在不同虚拟机实例上翻译出来的直接引用一般不会相同。如果有了直接引用，那引用的目标必定已经在虚拟机的内存中存在。

解析动作主要针对类或接口、字段、类方法、接口方法、方法类型、方法句柄和调用点限定符这 7 类符号引用进行。其中方法类型、方法句柄和调用点限定符为动态语言调用时的相关特性。

以下进行一些示例说明：
- 类或接口的解析
    假设当前代码所处的类为 D，如果要把一个从未解析过的符号引用 N 解析为一个类或接口 C 的直接引用，那虚拟机完成整个解析的过程需要包括以下 3 个步骤：
    1. 如果 C 不是一个数组类型
        那虚拟机将会把代表 N 的全限定名传递给 D 的类加载器去加载这个类 C。在加载过程中，由于元数据验证、字节码验证的需要，又可能触发其他相关类的加载动作，例如加载这个类的父类或实现的接口。一旦这个加载过程出现了任何异常，解析过程就将宣告失败。
    2. 如果 C 是一个数组类型，并且数组的元素类型为对象
        此时 N 的描述符会是类似“\[Ljava/lang/Integer”的形式，那将会按照第一点的规则加载数组元素类型。如果 N 的描述符如前面所假设的形式，需要加载的元素类型就是“`java.lang.Integer`”，接着由虚拟机生成一个代表该数组维度和元素的数组对象。
    3. 检查访问权限
        如果上面两步没有出现任何异常，那么 C 在虚拟机中实际上已经成为一个有效的类或接口了，但在解析完成前还要进行符号引用验证，确认 D 是否具备对 C 的访问权限。如果发现不具备访问权限，将抛出 `java.lang.IllegalAccessError` 异常。
- 字段解析
    要解析一个未被解析过的字段符号引用，首先将会对字段所属的类或接口的符号引用进行解析。如果在解析这个类或接口符号引用的过程中出现了任何异常，都会导致字段符号引用解析的失败。如果解析成功完成，那把这个字段所属的类或接口用 C 表示。
    
    字段解析的规则如下：
    1. 查找类本身字段
        如果 C 本身就包含了简单名称和字段描述符都与目标相匹配的字段，则返回这个字段的直接引用，查找结束。
    2. 查找父接口字段
        如果在 C 中实现了接口，将会按照继承关系从下往上递归搜索各个接口和它的父接口，如果接口中包含了简单名称和字段描述符都与目标相匹配的字段，则返回这个字段的直接引用，查找结束。
    3. 查找父类字段
        否则，如果 C 不是 `java.lang.Object` 的话，将会按照继承关系从下往上递归搜索其父类，如果在父类中包含了简单名称和字段描述符都与目标相匹配的字段，则返回这个字段的直接引用，查找结束。
    4. 查找失败，抛出 `java.lang.NoSuchFieldError` 异常。
    
    如果查找成功，还会检查堆该字段的可访问性。
- 方法解析
    要解析一个未被解析过的方法引用，首先将会对方法所属的类或接口的符号引用进行解析。
    
    以目标类为 C 解析的规则如下：
    1. 检查方法归属合法性
        由于 Class 文件格式中类的方法和接口的方法符号引用的常量类型定义是分开的，如果在类的方法表中发现索引的 C 是个接口的话，那就直接抛出 `java.lang.IncompatibleClassChangeError` 异常。
    2. 查找类 C 本身的方法
        在类 C 中查找是否有简单名称和描述符都与目标相匹配的方法，如果有则返回这个方法的直接引用，查找结束。
    3. 查找父类方法
        在类 C 的父类中递归查找是否有简单名称和描述符都与目标相匹配的方法，如果有则返回这个方法的直接引用，查找结束。
    4. 查找接口方法
        在类 C 实现的接口列表及它们的父接口之中递归查找是否有简单名称和描述符都与目标相匹配的方法，如果存在匹配的方法，说明类 C 是一个抽象类，这时候查找结束，抛出 `java.lang.AbstractMethodError` 异常。
    5. 查找失败，抛出 `java.lang.NoSuchMethodError`。
    
    如果查找成功，还会检查堆该方法的可访问性。
- 接口方法解析
    同样首先将会对方法所属的类或接口的符号引用进行解析。解析成功后按以下规则进行接口方法解析：
    1. 检查方法归属合法性
        与类的方法解析相反，如果在接口方法表中发现索引 C 是个类而不是接口，那么就直接抛出 `java.lang.IncompatibleClassChangeError` 异常。
    2. 查找接口本身的方法
        在接口 C 中查找是否有简单名称和描述符都与目标相匹配的方法，如果有则返回这个方法的直接引用，查找结束。
    3. 查找父接口的方法
        在接口 C 的父接口中递归查找，直到 `java.lang.Object` 类（接口方法的查找范围也会包括 Object 类中的方法）为止，看是否有简单名称和描述符都与目标相匹配的方法，如果有则返回这个方法的直接引用，查找结束。
        
        由于 Java 的接口允许多重继承，如果 C 的不同父接口中存有多个简单名称和描述符都与目标相匹配的方法，那将会从这多个方法中返回其中一个并结束查找。
        
        《Java 虚拟机规范》中并没有进一步规则约束应该返回哪一个接口方法。与字段查找类似地，不同发行商实现的 Javac 编译器有可能会按照更严格的约束拒绝编译这种代码来避免不确定性。
    4. 查找失败，抛出出 `java.lang.NoSuchMethodError` 异常。

#### 2.1.2.5. 初始化

类的初始化阶段是类加载过程的最后一个步骤，之前介绍的几个类加载的动作里，除了在加载阶段用户应用程序可以通过自定义类加载器的方式局部参与外，其余动作都完全由 Java 虚拟机来主导控制。直到初始化阶段，Java 虚拟机才真正开始执行类中编写的 Java 程序代码，将主导权移交给应用程序。

准备阶段时，变量已经赋过一次系统要求的初始零值，而在**初始化阶段，则会根据程序编码制定的主观计划去初始化类变量和其他资源**。初始化阶段就是执行类构造器 `<clinit>()` 方法的过程。`<clinit>()` 并不是程序员在 Java 代码中直接编写的方法，`<clinit>()` 方法是由**编译器自动收集类中的所有类变量的赋值动作和静态语句块（`static{}` 块）中的语句合并产生的**。编译器收集的顺序是由语句在源文件中出现的顺序决定的，静态语句块中只能访问到定义在静态语句块之前的变量，定义在它之后的变量，在前面的静态语句块可以赋值，但是不能访问。

对于初始化，有以下规则：
- 初始化当前类时无需主动调用父类 `<clinit>()` 方法
    `<clinit>()` 方法与类的构造函数（即在虚拟机视角中的实例构造器 `<init>()` 方法）不同，它不需要显式地调用父类构造器，Java 虚拟机会保证在子类的 `<clinit>()` 方法执行前，父类的 `<clinit>()` 方法已经执行完毕。
    
    因此在 Java 虚拟机中第一个被执行的 `<clinit>()` 方法的类型肯定是 `java.lang.Object`。由于父类的 `<clinit>()` 方法先执行，也就意味着父类中定义的静态语句块要优先于子类的变量赋值操作。
-  `<clinit>()` 方法对于类或接口来说并不是必需的
    如果一个类中没有静态语句块，也没有对变量的赋值操作，那么编译器可以不为这个类生成 `<clinit>()` 方法。
- 接口初始化时父接口 `<clinit>()` 方法不一定被执行
    接口中不能使用静态语句块，但仍然有变量初始化的赋值操作，因此接口与类一样都会生成 `<clinit>()` 方法。但接口与类不同的是，执行接口的 `<clinit>()` 方法不需要先执行父接口的 `<clinit>()` 方法，因为只有当父接口中定义的变量被使用时，父接口才会被初始化。
    
    此外，接口的实现类在初始化时也一样不会执行接口的 `<clinit>()` 方法。
- `<clinit>()` 方法调用需要保证多线程安全性
    Java 虚拟机必须保证一个类的 `<clinit>()` 方法在多线程环境中被正确地加锁同步，如果多个线程同时去初始化一个类，那么只会有其中一个线程去执行这个类的 `<clinit>()` 方法，其他线程都需要阻塞等待，直到活动线程执行完毕 `<clinit>()` 方法。如果在一个类的 `<clinit>()` 方法中有耗时很长的操作，那就可能造成多个进程阻塞，在实际应用中这种阻塞往往是很隐蔽的。

    以下示例了多个线程的阻塞情况：
    ```java
    static class DeadLoopClass {
    
    static { 
        // 如果不加上这个 if 语句，编译器将提示“Initializer does not complete normally” 并拒绝编译 
        if (true) { 
            System.out.println (Thread.currentThread () + "init DeadLoopClass");
            while (true) {
            } 
        }
    
    }
    
    public static void main (String[] args) {
    
        Runnable script = new Runnable () {
            public void run () {
                System.out.println (Thread.currentThread () + "start");
                DeadLoopClass dlc = new DeadLoopClass ();
                System.out.println (Thread.currentThread () + " run over");
            }
        };
    
        Thread thread1 = new Thread (script);
        Thread thread2 = new Thread (script);
        thread1.start ();
        thread2.start ();
    
    }
    ```

### 2.1.3. 类加载器

Java 虚拟机设计团队有意把类加载阶段中的“通过一个类的全限定名来获取描述该类的二进制字节流”这个动作放到 Java 虚拟机外部去实现，以便让应用程序自己决定如何去获取所需的类。实现这个动作的代码被称为“类加载器”（Class Loader）。

#### 2.1.3.1. 类与类加载器

类加载器虽然只用于实现类的加载动作，但它在 Java 程序中起到的作用却远超类加载阶段。**对于任意一个类，都必须由加载它的类加载器和这个类本身一起共同确立其在 Java 虚拟机中的唯一性，每一个类加载器，都拥有一个独立的类名称空间**。这句话可以表达得更通俗一些：比较两个类是否“相等”，只有在这两个类是由同一个类加载器加载的前提下才有意义，否则，即使这两个类来源于同一个 Class 文件，被同一个 Java 虚拟机加载，只要加载它们的类加载器不同，那这两个类就必定不相等。

这里所指的“相等”，包括代表类的 Class 对象的 `equals()` 方法、`isAssignableFrom()` 方法、`isInstance()` 方法的返回结果，也包括了使用 instanceof 关键字做对象所属关系判定等各种情况。如果没有注意到类加载器的影响，在某些情况下可能会产生具有迷惑性的结果。

#### 2.1.3.2. 双亲委派模型

![image.png](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202308170003106.png)

Java 一直保持着三层类加载器、双亲委派的类加载架构：
1. 启动类加载器
    负责加载存放在 `<JAVA_HOME>\lib` 目录，或者被 `-Xbootclasspath` 参数所指定的路径中存放的，而且是 Java 虚拟机能够识别的（按照文件名识别，如 rt. jar、tools. jar，名字不符合的类库即使放在 lib 目录中也不会被加载）类库加载到虚拟机的内存中。
    
    启动类加载器无法被 Java 程序直接引用，用户在编写自定义类加载器时，如果需要把加载请求委派给引导类加载器去处理，那直接使用 null 代替即可。
2. 扩展类加载器
    这个类加载器是在类 `sun.misc.Launcher$ExtClassLoader` 中以 Java 代码的形式实现的。它负责加载 `<JAVA_HOME>\lib\ext` 目录中，或者被 `java.ext.dirs` 系统变量所指定的路径中所有的类库。
    
    根据“扩展类加载器”这个名称，就可以推断出这是一种 Java 系统类库的扩展机制，JDK 的开发团队允许用户将具有通用性的类库放置在 ext 目录里以扩展 Java SE 的功能，在 JDK 9 之后，这种扩展机制被模块化带来的天然的扩展能力所取代。由于扩展类加载器是由 Java 代码实现的，开发者可以直接在程序中使用扩展类加载器来加载 Class 文件。
3. 应用程序类加载器
    这个类加载器由 `sun.misc.Launcher$AppClassLoader` 来实现。由于应用程序类加载器是 ClassLoader 类中的 `getSystemClassLoader()` 方法的返回值，所以有些场合中也称它为“系统类加载器”。
    
    它负责加载用户类路径 （ClassPath）上所有的类库，开发者同样可以直接在代码中使用这个类加载器。如果应用程序中没有自定义过自己的类加载器，一般情况下这个就是程序中默认的类加载器。
4. 自定义类加载器
    需要增加除了磁盘位置之外的 Class 文件来源，或者通过类加载器实现类的隔离、重载等功能可以通过自定义类加载器实现。

各种类加载器之间的层次关系被称为类加载器的“双亲委派模型（Parents Delegation Model）”。双亲委派模型要求除了顶层的启动类加载器外，其余的类加载器**都应有自己的父类加载器**。不过这里**类加载器之间的父子关系一般不是以继承（Inheritance）的关系来实现的，而是通常使用组合（Composition）关系来复用父加载器的代码**。

双亲委派模型的工作过程是：如果一个类加载器收到了类加载的请求，它首先**不会自己去尝试加载这个类，而是把这个请求委派给父类加载器去完成**，每一个层次的类加载器都是如此，因此所有的加载请求最终都应该传送到最顶层的启动类加载器中，只有**当父加载器反馈自己无法完成这个加载请求（它的搜索范围中没有找到所需的类）时，子加载器才会尝试自己去完成加载**。

使用双亲委派模型来组织类加载器之间的关系，一个显而易见的好处就是 **Java 中的类随着它的类加载器一起具备了一种带有优先级的层次关系**。例如类 `java.lang.Object`，它存放在 `rt.jar` 之中，无论哪一个类加载器要加载这个类，最终都是委派给处于模型最顶端的启动类加载器进行加载，因此 Object 类在程序的各种类加载器环境中都能够保证是同一个类。反之，如果没有使用双亲委派模型，都由各个类加载器自行去加载的话，如果用户自己也编写了一个名为 `java.lang.Object` 的类，并放在程序的 ClassPath 中，那系统中就会出现多个不同的 Object 类，Java 类型体系中最基础的行为也就无从保证，应用程序将会变得一片混乱。如果读者有兴趣的话，可以尝试去写一个与 rt. jar 类库中已有类重名的 Java 类，将会发现它可以正常编译，但永远无法被加载运行。

#### 2.1.3.3. 破坏双亲委派模型

双亲委派模型并不是一个具有强制性约束的模型，而是 Java 设计者推荐给开发者们的类加载器实现方式。在 Java 的世界中大部分的类加载器都遵循这个模型，但也有例外的情况，直到 Java 模块化出现为止，双亲委派模型主要出现过 3 次较大规模“被破坏”的情况。

1. 双亲委派模式第一次出现时
    双亲委派模型的第一次“被破坏”其实发生在双亲委派模型出现之前——即 JDK 1.2 面世以前的“远古”时代。由于双亲委派模型在 JDK 1.2 之后才被引入，但是类加载器的概念和抽象类 `java.lang.ClassLoader` 则在 Java 的第一个版本中就已经存在，面对已经存在的用户自定义类加载器的代码，Java 设计者们引入双亲委派模型时不得不做出一些妥协。
    
    为了兼容这些已有代码，无法再以技术手段避免 loadClass ()被子类覆盖的可能性，只能在 JDK 1.2 之后的 `java.lang.ClassLoader` 中添加一个新的 `protected` 方法 `findClass ()`，并引导用户编写的类加载逻辑时尽可能去重写这个方法，而不是在 `loadClass ()` 中编写代码，双亲委派的具体逻辑就实现在这里面，按照 `loadClass ()` 方法的逻辑，如果父类加载失败，会自动调用自己的 `findClass ()` 方法来完成加载，这样既不影响用户按照自己的意愿去加载类，又可以保证新写出来的类加载器是符合双亲委派规则的。
2. 扩展性需求
    双亲委派模型的第二次“被破坏”是由这个模型自身的缺陷导致的，双亲委派很好地解决了各个类加载器协作时基础类型的一致性问题（越基础的类由越上层的加载器进行加载），基础类型之所以被称为“基础”，是因为它们总是作为被用户代码继承、调用的 API 存在，但程序设计往往没有绝对不变的完美规则，如果有基础类型又要调用回用户的代码，那该怎么办呢？
    
    Java 提供了很多服务提供者接口（SPI，`Service Provider Interface`），它可以允许第三方为这些接口提供实现，比如数据库中的 SPI 服务 - JDBC。这些 SPI 的接口由 Java 核心类提供，实现者确是第三方。如果继续沿用双亲委派，就会存在问题，提供者由 Bootstrap ClassLoader 加载，而实现者是由第三方自定义类加载器加载。这个时候，顶层类加载就无法使用子类加载器加载过的类。
    
    这并非是不可能出现的事情，一个典型的例子便是 JNDI 服务，JNDI 现在已经是 Java 的标准服务，它的代码由启动类加载器来完成加载（在 JDK 1.3 时加入到 `rt.jar` 的），肯定属于 Java 中很基础的类型了。但 JNDI 存在的目的就是对资源进行查找和集中管理，它需要调用由其他厂商实现并部署在应用程序的 ClassPath 下的 JNDI 服务提供者接口（Service Provider Interface，SPI）的代码，现在问题来了，启动类加载器是绝不可能认识、加载这些代码的，那该怎么办？
    
    为了解决这个困境，Java 的设计团队只好引入了一个不太优雅的设计：线程上下文类加载器 （Thread Context ClassLoader）。这个类加载器可以通过 `java.lang.Thread` 类的 `setContext-ClassLoader()` 方法进行设置，如果创建线程时还未设置，它将会从父线程中继承一个，如果在应用程序的全局范围内都没有设置过的话，那这个类加载器默认就是应用程序类加载器。
    
    有了线程上下文类加载器，程序就可以做一些“舞弊”的事情了。JNDI 服务使用这个线程上下文类加载器去加载所需的 SPI 服务代码，这是一种父类加载器去请求子类加载器完成类加载的行为，这种行为实际上是打通了双亲委派模型的层次结构来逆向使用类加载器，已经违背了双亲委派模型的一般性原则，但也是无可奈何的事情。
    
    Java 中涉及 SPI 的加载基本上都采用这种方式来完成，例如 JNDI、 JDBC、JCE、JAXB 和 JBI 等。不过，当 SPI 的服务提供者多于一个的时候，代码就只能根据具体提供者的类型来硬编码判断，为了消除这种极不优雅的实现方式，在 JDK 6 时，JDK 提供了 `java.util.ServiceLoader` 类，以 `META-INF/services` 中的配置信息，辅以责任链模式，这才算是给 SPI 的加载提供了一种相对合理的解决方案。
3. 程序动态性追求
    代码热替换（Hot Swap）、模块热部署（Hot Deployment）等实现需要使用自定义类加载器。
    
    OSGi 实现模块化热部署的关键是它自定义的类加载器机制的实现，每一个程序模块（OSGi 中称为 Bundle）都有一个自己的类加载器，当需要更换一个 Bundle 时，就把 Bundle 连同类加载器一起换掉以实现代码的热替换。在 OSGi 环境下，类加载器不再双亲委派模型推荐的树状结构，而是进一步发展为更加复杂的网状结构。

要达到破坏双亲委派机制的目的，可以使用以下方式：
1. 重写 loadClass 方法
    在双亲委派的过程，都是在 `loadClass()` 方法中实现的，因此要想要破坏这种机制，可以自定义一个类加载器，继承 `ClassLoader` 并重写 `loadClass()` 方法即可，使其不进行双亲委派。
2. 利用线程上下文加载器
    Java 应用上下文加载器默认是使用 `AppClassLoader`。若想要在父类加载器使用到子类加载器加载的类，可以使用 `Thread.currentThread().getContextClassLoader()`。

# 3. 高效并发

## 3.1. Java 内存模型

由于计算机的存储设备与处理器的运算速度有着几个数量级的差距，所以现代计算机系统都不得不加入一层或多层读写速度尽可能接近处理器运算速度的高速缓存（Cache）来作为内存与处理器之间的缓冲：将运算需要使用的数据复制到缓存中，让运算能快速进行，当运算结束后再从缓存同步回内存之中，这样处理器就无须等待缓慢的内存读写了。

基于高速缓存的存储交互很好地解决了处理器与内存速度之间的矛盾，但是也为计算机系统带来更高的复杂度，它引入了一个新的问题：**缓存一致性（Cache Coherence）**。在多路处理器系统中，每个处理器都有自己的高速缓存，而它们又共享同一主内存（Main Memory），这种系统称为共享内存多核系统（Shared Memory Multiprocessors System）。

当多个处理器的运算任务都涉及同一块主内存区域时，将可能导致各自的缓存数据不一致。如果真的发生这种情况，那同步回到主内存时该以谁的缓存数据为准呢？为了解决一致性的问题，需要各个处理器访问缓存时都遵循一些协议，在读写时要根据协议来进行操作，这类协议有 MSI、MESI（Illinois Protocol）、MOSI、 Synapse、Firefly 及 Dragon Protocol 等。

![image.png](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202308190052413.png)

**内存模型：在特定的操作协议下，对特定的内存或高速缓存进行读写访问的过程抽象**。不同架构的物理机器可以拥有不一样的内存模型，而 Java 虚拟机也有自己的内存模型，并且与内存访问操作及硬件的缓存访问操作具有高度的可类比性。

除了增加高速缓存之外，**为了使处理器内部的运算单元能尽量被充分利用，处理器可能会对输入代码进行乱序执行（Out-Of-Order Execution）优化**，处理器会在计算之后将乱序执行的结果重组，保证该结果与顺序执行的结果是一致的，但并不保证程序中各个语句计算的先后顺序与输入代码中的顺序一致，因此如果存在一个计算任务依赖另外一个计算任务的中间结果，那么其顺序性并不能靠代码的先后顺序来保证。与处理器的乱序执行优化类似，Java 虚拟机的即时编译器中也有指令重排序 （Instruction Reorder）优化。

> [!info] 指令重排序的原因
> 
> CPU 可能会以不同于程序代码顺序的方式重新排列指令的执行顺序。这是为了提高指令级并行性和性能而采取的一种优化技术。重排序可以在不改变程序的语义和最终结果的前提下重新安排指令的执行顺序。
> 
> 例如对于两个变量进行以下操作：`a = 1; b = 2; a = a + 1;`，对于 a 的操作其实可以连接到一起完成，避免对 a 的操作进行两次内存的交互（load，store）。


### 3.1.1. 主内存与工作内存

**Java 内存模型的主要目的是定义程序中各种变量的访问规则，即关注在虚拟机中把变量值存储到内存和从内存中取出变量值这样的底层细节**。此处的变量（Variables）与 Java 编程中所说的变量有所区别，它包括了实例字段、静态字段和构成数组对象的元素，但是不包括局部变量与方法参数，因为后者是线程私有的，不会被共享，自然就不会存在竞争问题。为了获得更好的执行效能，Java 内存模型并没有限制执行引擎使用处理器的特定寄存器或缓存来和主内存进行交互，也没有限制即时编译器是否要进行调整代码执行顺序这类优化措施。

Java 内存模型规定了所有的变量都存储在主内存（Main Memory）中（此处的主内存与介绍物理硬件时提到的主内存名字一样，两者也可以类比，但物理上它仅是虚拟机内存的一部分）。每条线程还有自己的工作内存（Working Memory，可与前面讲的处理器高速缓存类比），**线程的工作内存中保存了被该线程使用的变量的主内存副本，线程对变量的所有操作（读取、赋值等）都必须在工作内存中进行，而不能直接读写主内存中的数据。不同的线程之间也无法直接访问对方工作内存中的变量，线程间变量值的传递均需要通过主内存来完成**。

![image.png](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202308190056996.png)

### 3.1.2. 内存间交互动作

关于主内存与工作内存之间具体的交互协议，即一个变量如何从主内存拷贝到工作内存、如何从工作内存同步回主内存这一类的实现细节，Java 内存模型中定义了以下 8 种操作来完成。Java 虚拟机实现时必须保证下面提及的每一种操作都是原子的、不可再分的：
- lock（锁定）
    作用于主内存的变量，它把一个变量标识为一条线程独占的状态。
- unlock（解锁）
    作用于主内存的变量，它把一个处于锁定状态的变量释放出来，释放后的变量才可以被其他线程锁定。
- read（读取）
    作用于主内存的变量，它把一个变量的值从主内存传输到线程的工作内存中，以便随后的 load 动作使用。
- load（载入）
    作用于工作内存的变量，它把 read 操作从主内存中得到的变量值放入工作内存的变量副本中。
- use（使用）
    作用于工作内存的变量，它把工作内存中一个变量的值传递给执行引擎，每当虚拟机遇到一个需要使用变量的值的字节码指令时将会执行这个操作。
- assign（赋值）
    作用于工作内存的变量，它把一个从执行引擎接收的值赋给工作内存的变量，每当虚拟机遇到一个给变量赋值的字节码指令时执行这个操作。
- store（存储）
    作用于工作内存的变量，它把工作内存中一个变量的值传送到主内存中，以便随后的 write 操作使用。
- write（写入）
    作用于主内存的变量，它把 store 操作从工作内存中得到的变量的值放入主内存的变量中。

如果要把一个变量从主内存拷贝到工作内存，那就要按顺序执行 read 和 load 操作，如果要把变量从工作内存同步回主内存，就要按顺序执行 store 和 write 操作。注意，Java 内存模型只要求上述两个操作必须**按顺序执行，但不要求是连续执行**。也就是说 read 与 load 之间、store 与 write 之间是可插入其他指令的，如对主内存中的变量 a、b 进行访问时，一种可能出现的顺序是 read a、read b、load b、load a。除此之外，Java 内存模型还规定了在执行上述 8 种基本操作时必须满足如下规则：
1. 不允许 read 和 load、store 和 write 操作之一单独出现
    即不允许一个变量从主内存读取了但工作内存不接受，或者工作内存发起回写了但主内存不接受的情况出现。
2. 不允许一个线程丢弃它最近的 assign 操作
    即变量在工作内存中改变了之后必须把该变化同步回主内存。
3. 不允许一个线程无原因地（没有发生过任何 assign 操作）把数据从线程的工作内存同步回主内存中
4. 一个新的变量只能在主内存中“诞生”
    不允许在工作内存中直接使用一个未被初始化（load 或 assign）的变量，换句话说就是对一个变量实施 use、store 操作之前，必须先执行 assign 和 load 操作。
5. 一个变量在同一个时刻只允许一条线程对其进行 lock 操作
    lock 操作可以被同一条线程重复执行多次，多次执行 lock 后，只有执行相同次数的 unlock 操作，变量才会被解锁。
6. 如果对一个变量执行 lock 操作，那将会清空工作内存中此变量的值
    清空以后，在执行引擎使用这个变量前，需要重新执行 load 或 assign 操作以初始化变量的值。
7. 仅在 lock 状态下可以 unlock
    如果一个变量事先没有被 lock 操作锁定，那就不允许对它执行 unlock 操作，也不允许去 unlock 一个被其他线程锁定的变量
8. 对一个变量执行 unlock 操作之前，必须先把此变量同步回主内存中（执行 store、write 操作）

这 8 种内存访问操作以及上述规则限定，再加上稍后会介绍的专门针对 volatile 的一些特殊规定，就已经能准确地描述出 Java 程序中哪些内存访问操作在并发下才是安全的。

### 3.1.3. volatile 类型变量的特殊规则

当一个变量被定义成 volatile 之后，它将具备两项特性：
1. 保证此变量对所有线程的可见性
    这里的“可见性”是指**当一条线程修改了这个变量的值，新值对于其他线程来说是可以立即得知的**。而普通变量并不能做到这一点，普通变量的值在线程间传递时均需要通过主内存来完成。比如，线程 A 修改一个普通变量的值，然后向主内存进行回写，另外一条线程 B 在线程 A 回写完成了之后再对主内存进行读取操作，新变量值才会对线程 B 可见。
2. 禁止指令重排序优化
    普通的变量仅会保证**在该方法的执行过程中所有依赖赋值结果的地方都能获取到正确的结果，而不能保证变量赋值操作的顺序与程序代码中的执行顺序一致**。因为在同一个线程的方法执行过程中无法感知到这点，这就是 Java 内存模型中描述的所谓“**线程内表现为串行的语义**”（Within-Thread As-If-Serial Semantics）。
    
    有 volatile 修饰的变量，赋值后多执行了一个“lock add”操作，这个操作的作用相当于一个内存屏障 （Memory Barrier 或 Memory Fence），**指重排序时不能把后面的指令重排序到内存屏障之前的位置**。只有一个处理器访问内存时，并不需要内存屏障；但如果有两个或更多处理器访问同一块内存，且其中有一个在观测另一个，就需要内存屏障来保证一致性了。这里的关键在于 lock 前缀，它的作用是**将本处理器的缓存写入了内存，该写入动作也会引起别的处理器或者别的内核无效化（Invalidate）其缓存**。这种操作相当于对缓存中的变量做了一次前面介绍 Java 内存模式中所说的“store 和 write”操作，所以通过这样一个操作，可让前面 volatile 变量的修改对其他处理器立即可见。

由于 volatile 变量只能保证可见性，在不符合以下两条规则的运算场景中，我们仍然要通过加锁 （使用 synchronized、`java.util.concurrent` 中的锁或原子类）来保证原子性：
1. 修改变量并不依赖变量的当前值，或者能够确保只有单一的线程修改变量的值。
2. 变量不需要与其他的状态变量共同参与不变约束。

Java 内存模型中对 volatile 变量有特殊规则的定义。假定 T 表示一个线程，V 和 W 分别表示两个 volatile 型变量，那么在进行 read、load、use、assign、store 和 write 操作时需要满足如下规则：
1. 在工作内存中，每次使用 V 前都必须先从主内存刷新最新的值，用于保证能看见其他线程对变量 V 所做的修改
    只有当线程 T 对变量 V 执行的前一个动作是 load 的时候，线程 T 才能对变量 V 执行 use 动作；并且，只有当线程 T 对变量 V 执行的后一个动作是 use 的时候，线程 T 才能对变量 V 执行 load 动作。线程 T 对变量 V 的 use 动作可以认为是和线程 T 对变量 V 的 load、read 动作相关联的，必须连续且一起出现。
2. 在工作内存中，每次修改 V 后都必须立刻同步回主内存中，用于保证其他线程可以看到自己对变量 V 所做的修改
    只有当线程 T 对变量 V 执行的前一个动作是 assign 的时候，线程 T 才能对变量 V 执行 store 动作；并且，只有当线程 T 对变量 V 执行的后一个动作是 store 的时候，线程 T 才能对变量 V 执行 assign 动作。线程 T 对变量 V 的 assign 动作可以认为是和线程 T 对变量 V 的 store、write 动作相关联的，必须连续且一起出现。
3. volatile 修饰的变量不会被指令重排序优化，从而保证代码的执行顺序与程序的顺序相同
    假定动作 A 是线程 T 对变量 V 实施的 use 或 assign 动作，假定动作 F 是和动作 A 相关联的 load 或 store 动作，假定动作 P 是和动作 F 相应的对变量 V 的 read 或 write 动作；与此类似，假定动作 B 是线程 T 对变量 W 实施的 use 或 assign 动作，假定动作 G 是和动作 B 相关联的 load 或 store 动作，假定动作 Q 是和动作 G 相应的对变量 W 的 read 或 write 动作。如果 A 先于 B，那么 P 先于 Q。

### 3.1.4. long 和 double 的特殊规则

Java 内存模型要求 lock、unlock、read、load、assign、use、store、write 这八种操作都具有原子性，但是对于 64 位的数据类型（long 和 double），在模型中特别定义了一条宽松的规定：允许虚拟机将没有被 volatile 修饰的 64 位数据的读写操作划分为两次 32 位的操作来进行，即允许虚拟机实现自行选择是否要保证 64 位数据类型的 load、store、read 和 write 这四个操作的原子性，这就是所谓的“long 和 double 的非原子性协定”（Non-Atomic Treatment of double and long Variables）。

如果有多个线程共享一个并未声明为 volatile 的 long 或 double 类型的变量，并且同时对它们进行读取和修改操作，那么某些线程可能会读取到一个既不是原值，也不是其他线程修改值的代表了“半个变量”的数值。不过这种读取到“半个变量”的情况是非常罕见的。

### 3.1.5. 原子性、可见行与有序性

Java 内存模型是围绕着在并发过程中如何处理原子性、可见性和有序性这三个特征来建立的：
1. 原子性
    由 Java 内存模型来直接保证的原子性变量操作包括 read、load、assign、use、store 和 write 这六个，我们大致可以认为，基本数据类型的访问、读写都是具备原子性的。
    
    如果应用场景需要一个更大范围的原子性保证，Java 内存模型还提供了 lock 和 unlock 操作来满足这种需求，尽管虚拟机未把 lock 和 unlock 操作直接开放给用户使用，但是却提供了更高层次的字节码指令 monitorenter 和 monitorexit 来隐式地使用这两个操作。这两个字节码指令反映到 Java 代码中就是同步块——synchronized 关键字，因此在 synchronized 块之间的操作也具备原子性。
2. 可见性
    可见性就是指当一个线程修改了共享变量的值时，其他线程能够立即得知这个修改。
    
    Java 内存模型是通过**在变量修改后将新值同步回主内存，在变量读取前从主内存刷新变量值这种依赖主内存作为传递媒介的方式来实现可见性的**，无论是普通变量还是 volatile 变量都是如此。普通变量与 volatile 变量的区别是，**volatile 的特殊规则保证了新值能立即同步到主内存，以及每次使用前立即从主内存刷新**。因此我们可以说 volatile 保证了多线程操作时变量的可见性，而普通变量则不能保证这一点。
    
    除了 volatile 之外，Java 还有两个关键字能实现可见性，它们是 synchronized 和 final。同步块的可见性是由“对一个变量执行 unlock 操作之前，必须先把此变量同步回主内存中（执行 store、write 操作）”这条规则获得的。而 final 关键字的可见性是指：被 final 修饰的字段在构造器中一旦被初始化完成，并且构造器没有把“this”的引用传递出去（this 引用逃逸是一件很危险的事情，其他线程有可能通过这个引用访问到“初始化了一半”的对象），那么在其他线程中就能看见 final 字段的值。
3. 有序性
    Java 程序中天然的有序性可以总结为一句话：**如果在本线程内观察，所有的操作都是有序的；如果在一个线程中观察另一个线程，所有的操作都是无序的**。前半句是指“线程内似表现为串行的语义”（Within-Thread As-If-Serial Semantics），后半句是指“指令重排序”现象和“工作内存与主内存同步延迟”现象。
    
    Java 语言提供了 volatile 和 synchronized 两个关键字来保证线程之间操作的有序性，volatile 关键字本身就包含了禁止指令重排序的语义，而 synchronized 则是由“一个变量在同一个时刻只允许一条线程对其进行 lock 操作”这条规则获得的，这个规则决定了持有同一个锁的两个同步块只能串行地进入。

### 3.1.6. 先行发生原则

**先行发生是 Java 内存模型中定义的两项操作之间的偏序关系**，比如说操作 A 先行发生于操作 B，其实就是说在发生操作 B 之前，操作 A 产生的影响能被操作 B 观察到，“影响”包括修改了内存中共享变量的值、发送了消息、调用了方法等。

Java 内存模型下存在一些“天然的”先行发生关系，这些先行发生关系无须任何同步器协助就已经存在，可以在编码中直接使用。如果两个操作之间的关系不在此列，并且无法从下列规则推导出来，则它们就没有顺序性保障，虚拟机可以对它们随意地进行重排序。

1. 程序次序规则（Program Order Rule）
    **在一个线程内**，按照控制流顺序，书写在前面的操作先行发生于书写在后面的操作。注意，这里说的是控制流顺序而不是程序代码顺序，因为要考虑分支、循环等结构。
2. 管程锁定规则（Monitor Lock Rule）
    一个 unlock 操作先行发生于后面对同一个锁的 lock 操作。这里必须强调的是“同一个锁”，而“后面”是指时间上的先后。
3. volatile 变量规则（Volatile Variable Rule）
    对一个 volatile 变量的写操作先行发生于后面对这个变量的读操作，这里的“后面”同样是指时间上的先后。
4. 线程启动规则（Thread Start Rule）
    Thread 对象的 `start()` 方法先行发生于此线程的每一个动作。
5. 线程终止规则（Thread Termination Rule）
    线程中的所有操作都先行发生于对此线程的终止检测，我们可以通过 `Thread::join()` 方法是否结束、`Thread::isAlive()` 的返回值等手段检测线程是否已经终止执行。
6. 线程中断规则（Thread Interruption Rule）
    对线程 interrupt ()方法的调用先行发生于被中断线程的代码检测到中断事件的发生，可以通过 `Thread::interrupted()` 方法检测到是否有中断发生。
7. 对象终结规则（Finalizer Rule）
    一个对象的初始化完成（构造函数执行结束）先行发生于它的 `finalize()` 方法的开始。
8. 传递性（Transitivity）
    如果操作 A 先行发生于操作 B，操作 B 先行发生于操作 C，那就可以得出操作 A 先行发生于操作 C 的结论。

时间先后顺序与先行发生原则之间基本没有因果关系，所以我们衡量并发安全问题的时候不要受时间顺序的干扰，一切必须以先行发生原则为准。

## 3.2. Java 内存模型与缓存一致性

### 3.2.1. 操作系统缓存一致性

当多个处理器的运算任务都涉及同一块主内存区域时，将可能导致各自的缓存数据不一致。如果真的发生这种情况，那同步回到主内存时该以谁的缓存数据为准呢？为了解决一致性的问题，需要各个处理器访问缓存时都遵循一些协议，在读写时要根据协议来进行操作，这类协议有 MSI、MESI（Illinois Protocol）、MOSI、 Synapse、Firefly 及 Dragon Protocol 等。

![image.png](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202308190052413.png)

#### 3.2.1.1. 总线锁

缓存一致性的第一个问题在于，在多 CPU 缓存的情况下，一个 CPU 修改了主存的共享变量，其它 CPU 是不知道的，所以解决这个问题最直接的办法就是使用通知机制，当一个 CPU 修改了主存的数据时，其它 CPU 都会收到相应的数据变更通知，收到通知的 CPU 如果发现自己也缓存了对应的数据，那么就会将自己缓存的数据所在缓存行标记为失效，当下次读取该数据时发现自己的缓存行已过期，那么就会选择从主存加载最新的数据。而实现这个功能的机制就叫**“总线嗅探”**，总线嗅探是通过 CPU 侦听总线上发生的数据交换操作，当总线上发生了数据操作，那么总线就会广播对应的通知。

![image.png](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202308202234307.png)

导致缓存不一致的另外一个问题在于，CPU 操作共享数据的顺序性，想让并发的操作变得有序，那么常用的方式就是让操作的资源具备独占性，这也就是我们常用的的方式加锁，当一个 CPU 对操作的资源加了锁，那么其它 CPU 就只能等待，只有等前一个释放了锁（资源占用权），后面的才能获得执行权，从而保证整体操作的顺序性。

而实现这个机制的功能就叫“总线仲裁”，在多个 CPU 同时申请对总线的使用权时，为避免产生总线冲突，需由总线仲裁来合理地控制和管理系统中需要占用总线的申请者，在多个申请者同时提出总线请求时，以一定的优先算法仲裁哪个应获得对总线的使用权。

![image.png](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202308202235213.png)

所以针对总线锁，简单来说就是在多 cpu 下，当其中一个处理器要对共享内存进行操作的时候，在总线上发出一个 LOCK 信号，这个信号使得其他处理器无法通过总线来访问到共享内存中的数据，总线锁定把 CPU 和内存之间的通信锁住了，这使得锁定期间，其他处理器不能操作其他内存地址的数据。因此总线锁定的开销比较大，这种机制并不是最优的方式。

如何优化呢？最好的方法就是控制锁的保护粒度，我们只需要保证对于被多个 CPU 缓存的同一份数据是一致的就行。在 P6 架构的 CPU 后，引入了缓存锁，如果当前数据已经被 CPU 缓存了，并且是要写会到主内存中的，就可以采用缓存锁来解决问题。

所谓的缓存锁，就是指内存区域如果被缓存在处理器的缓存行中，并且在 Lock 期间被锁定，那么当它执行锁操作回写到内存时，不再总线上加锁，而是修改内部的内存地址，基于缓存一致性协议来保证操作的原子性。

总线锁和缓存锁怎么选择，取决于很多因素，比如 CPU 是否支持、以及存在无法缓存的数据时（比较大或者快约多个缓存行的数据），必然还是会使用总线锁。

#### 3.2.1.2. 缓存一致性协议

总线性能瓶颈在于基于在总线与主存打交道会造成阻塞，那么反过来想如果不通过总线与内存发生数据交互就可以避免总线加锁，所以优化这个问题的核心在于如何减少必须通过总线与主存交互的操作，换而言之就是**如非必要，就不要通过总线与主存打交道**。

至于如何减少 CPU 通过主线和主存打交道的次数，这里可以分为两个方向，**一方面尽量避免通过主线从主存读取的数据，另一个方面是减少修改数据后而把数据同步到主存的频率**。

- 减少从主存读取数据的频率
    减少从主存读取数据频率的核心思路在于：如果 CPU 读取一个自己缓存没有的数据时，不是直接通过总线向主存读取，而是优先从其它已经缓存了对应数据的 CPU 缓存获。
    
    当一个 CPU 读取数据时，首先从自己的缓存里面读取对应的缓存行，如果此时自己的缓存里面没有，那么它会向总线发起一个读取事务，此时其它 CPU 会收到一个来自总线读取的消息，如果其它 CPU 的缓存有持有对应缓存行的数据时，它会把缓存行的地址放到总线上，那么读取数据的 CPU 只需要通过地址拷贝对应的缓存行到自己的 CPU 缓存即可。
    
    ![image.png](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202308202239379.png)
- 减少修改数据同步主存的频率
    如果当前数据只有自己缓存了，那么就不存在多个 CPU 缓存的一致性问题，所以无论当前 CPU 修改多少次当前缓存行的值，也不会影响到其他人，所以这种情况下的数据变更可以不必马上同步到主存去，只有在其他 CPU 也需要读取对应的数据时候，那么此时数据就会由一个人独占变成共享了，可以这个时候再把数据同步到主存去。
    
    ![image.png](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202308202240938.png)

##### 3.2.1.2.1. MESI 协议

总线性能的优化思路已经有了，读取数据的时候如果要从其它 CPU 缓存获取数据的话，首先得知道其他 CPU 有没有这个数据，并且该 CPU 的数据是否是最新的；修改数据的时候，需要想办法通知其他 CPU 对应的改动。但是 CPU 之间没办法直接通信，所以它们之间就必须商定一套机制，**如何通过自己的数据状态就能知道其他 CPU 的缓存情况**，**从而做出对应的策略，而这套机制就是缓存一致性协议。**

为了达到数据访问的一致，需要各个处理器在访问缓存时遵循一些协议，在读写时根据协议来操作，常见的协议有 MSI，MESI，MOSI 等。最常见的就是 MESI 协议。

CPU 以缓存行为单位来读写数据，MESI 协议处理的对象也是缓存行，MESI 定义了 4 种不同的缓存行状态，如下
- M（Modified）: 缓存行中的数据被修改，但未同步到主内存中。
- E（Exclusive）：当前 CPU 有此缓存行中的数据，其他 CPU 没有。
- S（Shared）：当前 CPU 和其他 CPU 缓存行中都有此数据。
- I（Invalid）：当前 CPU 中的缓存行数据无效，这往往是由于其他 CPU 对缓存行中的数据进行修改导致的，当前 CPU 如果去缓存中读数据的话，由于数据已经无效，会重新从内存中加载缓存行。

接下来我们来举例看看 MESI 协议如何让 CPU 间的缓存保持一致，我们假设有两个 CPU ，CPU0 和 CPU 1，然后来看看当对这两个 CPU 执行一系列的读写操作时：

1. CPU0 执行读操作
	![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202210310011180.png)

	由于其它 CPU 无 a=0 的缓存行，所以其缓存行状态为 E。
2. CPU0 将数据 a=1 写入 cache 中
	此时缓存块被修改了，但由于其他 CPU 无此缓存块，所以修改后的数据无需同步至内存中，所以 CPU0 中缓存行的状态为 M。

	![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202210310012238.png)
3. CPU1 读取数据 a
	CPU1 读取 a 时，首先会通过总线向其他 CPU 广播一下读请求，然后 CPU0 发现自己的缓存块为 M，于是首先会将 a=1 刷新至内存，并将自己的缓存块标志位置为 S，然后才允许 CPU1 从内存中读数据，读取后由于 CPU0 也有此数据，所以会将其缓存行状态置为 S。

	![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202210310013228.png)

4. CPU1 写数据 a=2
	由于 CPU1 中的缓存行状态为 S，所以它首先会往总线上广播一条 invalidate 消息，其他 CPU 收到消息后，会将其缓存行置为 I，然后发一个 invalidate ack 消息给 CPU1，CPU1 收到此消息后会将 a=2 写入缓存行中，然后再同步到内存，最后会将缓存行状态置为 E（因为其它 CPU 缓存行都失效了，所以缓存行为此 CPU 独有）。
	![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202210310013670.png)

5. CPU0 读数据
	由于 CPU0 的缓存块为 I，为已失效，所以它会广播一个读请求，CPU1 的缓存行状态为 E，所以它会将缓存行状态置为 S，并且将缓存行同步到 CPU0 中，注意这里是 CPU 间的缓存行传输，这样比起从内存读显然传输更快。

MESI 协议很复杂，以上只是列表了 MESI 协议的一部分，实际上 MESI 协议的状态有几十种，转换状态也很复杂，完全列举是不可能的，大家知道其基本思想即可。

##### 3.2.1.2.2. Store Buffer 与 Store Forward

在共享状态下，因为一个缓存行的数据在多个 CPU 核心的 Cache 里都有。所以，当我们想要更新 Cache 里面的数据的时候，不能直接修改，而是要先**向所有的其他 CPU 广播一个请求，要求先把其他 CPU 里面的缓存行都变成无效的状态**，等其他 CPU 都响应对于 invalid 操作的 ACK 后，修改数据的 CPU 才能更新当前 Cache 里面的数据。这个广播操作，一般叫作 RFO（Request For Ownership），也就是获取当前对应 Cache Block 数据的所有权，也就是所谓的**缓存锁**。

从广播指令，到收到所有其他 CPU 的 ACK 之后才能继续后面的操作，整个过程都处于阻塞状态，而对于 CPU 来说这个时间是很漫长的，所以这里就从两个方向进行了优化。一方面是在 **CPU 等待其他 CPU 回复的过程中可以去干一些其它的事情，所以就有了 Store Buffere 。另一方面是尽量缩短其他 CPU 回复 Invalid ack 的时间，所以就有了 Invalidate Queue**。

在 Store Buffer 机制下，CPU 广播了通知之后，不再等待着其它 CPU 回复，而是把广播 invalid 指令发出去以后，然后直接把要修改的数据放到 Store Bufferes 里，然后就去干其它事情了，当等到其他 CPU 都响应了 ACK 之后，然后再回头从 Store Buffer 读取出来执行最后的数据修改操作。

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202210310016015.png)

这样的话 CPU 要修改数据，先写入 Store Buffer 中，然后马上返回，之后再由 Store Buffer 异步执行发送广播消息和写入 cache 的操作，于是 CPU 执行写的效率就得到了极大的提升。

这里需要注意一下 Cache 和 Store buffer 的区别，Cache 一般指数据的副本，如果 Cache 中的数据没了，还可以从 Memory 中加载，但 Store Buffer 则不是，它更像是蓄水池的作用，先存储一堆数据，然后再异步执行操作，我们可以把它想像成课代表，先把所有同学的作业收集好再一次性交给老师，如果 Store Buffer 中的数据丢了，那就彻底丢失了。

Store Bufferes 的确提高了 CPU 的资源利用率，不过优化了带来了新的问题，如果数据写入了 Store Buffer 但还没来得及更新，如果此时 CPU 接到了一个读取的指令，那么 CPU 这时候会从自己的缓存中去读取共享变量的数据，而当前缓存中的数据并不是最新的，那么这是一个很明显的问题。要解决这个问题就必须要求 **CPU 读取数据时得先看 Store Bufer 里面有没有，如果有则直接读取 Store Bufer 里的值，如果没有才能读取自己缓存里面的数据**，这也就是所谓的“Store Forward”。

##### 3.2.1.2.3. Invalidate Queue

Store Buffer 的存储容量是有限的，前面我们介绍了，由 Store buffer 来发送 invalidate 广播消息，然后其它 CPU 收到消息后，先将缓存行状态置为 I，然后再回复 invalidate ack 消息，但很有可能其他 CPU 在收到 invalidate 消息时正在忙其他事，还来不及将缓存行状态置为 I，这样就会造成 Store buffer 不断堆积，直至溢出。

为了提升失效处理的速度，在 Cache 和总线间又加入了一个 Invalidate Queue，这样的话一旦 CPU 收到 invalidate 广播消息，就将此消息存储在 Invalidate queue 中，然后立即回复 invalidate ack 消息给发出广播的那个 CPU，之后 Invalidate queue 再异步执行将缓存行失效（设置状态为 I）的操作。

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202210310017474.png)

这样的话 Store buffer 就能节省写操作的时间，可以及时清空。Store buffer 和 Invalidate Queue 的引入相当于对 MESI 缓存一致性协议进行了改造，这样的改造本质上是为了提高 CPU 的执行效率（尤其是写的效率），但天下没有免费的午餐，这样的方式又造成了数据的短暂不一致，举个例子，假设 CPU0 和 CPU1 的状态如下：

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202210310018553.png)

此时 CPU0 执行了对 a 的写操作 a = 2，由于此操作会先写入 Store buffer，然后再由 Store buffer 执行异步的操作，那么从 Store buffer 发送 invalidate 广播到 Invalidate queue 让缓存行失效期间，CPU 1 读取 a 的值都是老值（即 a = 1），这就造成了**短暂的不一致**，当然**最终还是会一致**的，我们把这种协议称为**弱一致性**或者说**最终一致性**协议。

#### 3.2.1.3. 内存屏障

既然经过了优化过后的缓存一致性协议无法达到数据的强一致，那么我们为什么还要去优化呢？因为大多数情况都不存在并发问题，只有少数场景才会导致这种问题，我们不能因为极少数场景的问题而放弃了大多数场景的性能提成。当然虽然是极少数场景的问题，但是也不能放任不管，所以针对这种少数场景就必须要有一套处理机制来保证我们程序不出问题，所以这就是内存屏障的职责了。

内存屏障可以简单的认为它就是用来禁用我们的 CPU 缓存优化的，**使用了内存屏障后，写入数据时候会保证所有的指令都执行完毕**，这样就能保证修改过的数据能即时的暴露给其他的 CPU。在**读取数据的时候保证所有的“无效队列”消息都已经被读取完毕**，这样就保证了其他 CPU 修改的数据消息都能被当前 CPU 知道，然后根据 Invalid 消息判断自己的缓存是否处于无效状态，这样就读取数据的时候就能正确的读取到最新的数据。

##### 3.2.1.3.1. Store Barrier (写屏障)

强制所有在 store 屏障指令之前的 store 指令，都在该 store 屏障指令执行之前被执行，并把 store 缓冲区的数据都刷到 CPU 缓存。这个指令其实就是告诉 CPU，执行这个指令的时候需要把 store buffer 的数据都同步到内存中去。

##### 3.2.1.3.2. Load Barrier (读屏障)

强制所有在 load 屏障指令之后的 load 指令，都在该 load 屏障指令执行之后被执行，并且一直等到 load 缓冲区被该 CPU 读完才能执行之后的 load 指令。这个指令的意思是，在读取共享变量的指令前，先处理所有在失效队列中的消息，这样就保证了在读取数据之前所有失效的消息都得到了执行，从而保证自己读取到的数据是最新的。

##### 3.2.1.3.3. Full Barrier（全能屏障）

包含了 Store Barrier 和 Load Barrier 的功能。

### 3.2.2. Java 内存模型与缓存一致性的关系

![image.png](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202308210027642.png)

既然操作系统已经提供了缓存一致性的保证，那么 Java 定义自己的内存模型是出于什么策略呢？其实这两者作用不同，操作系统的缓存一致性解决的是多核情况下高速缓存一致性问题，而 JMM 解决的是 Java 下线程之间工作内存的一致性问题。
1. 与 Java 自身相关
    Java 语言层次有自身的优化，比如指令重排与 JIT 编译等，因此 Java 需要有自己的内存模型来控制这些操作何时可做何时不可做。
2. 与 CPU 缓存有关
    操作系统可能出于优化原因，使用 Store Buffer 与 Invalid queue 来提升操作的速度，但是这会带来一定的一致性问题。
3. 多个变量的重排序现象
    MESI 协议最多只是保证了对于一个变量，在多个核上的读写顺序，对于多个变量而言是没有任何保证的。因此当多个变量都在进行缓存读写时，那么它们之间的顺序是不确定的。
    
    除此之外，对于 CPU 运行指令，无关操作的变量也可能会被重排序执行，因此在这种情况下也无法保证变量读写的顺序。

因此**即使操作系统有了缓存一致性的保证，在应用层也还是有可能出现不一致的现象**。所以在基于以上的情况下，Java 定义了自己的内存模型，来实现一致性与可见行。在 Volatile 关键字的作用下，才能达到以下效果：
1. 禁止编译器以及 CPU 优化向的指令重排序
    禁止重排序是在指令中插入内存屏障来实现的：
    - LoadLoad
        确保 Load1 的数据加载在 Load2 访问的数据之前加载。
    - StoreStore
        确保 Store1 数据对其他处理器可见行先于 Store2 及后续存储指令。
    - LoadStore
        确保 Load1 的数据加载先于 Store2 及后续的存储指令。
    - StoreLoad
        确保 Store1 的数据存储先于 Load2 及后续所有的数据加载指令。
2. 使写核心 Store Buffer 的值能立即写入缓存行，其他共享核心能立即处理失效请求（清空 Invalid queue）

## 3.3. Java 的线程与协程

### 3.3.1. Java 线程的实现

线程是比进程更轻量级的调度执行单位，线程的引入，可以**把一个进程的资源分配和执行调度分开，各个线程既可以共享进程资源（内存地址、文件 I/O 等），又可以独立调度**。

实现线程主要有三种方式：使用内核线程实现（1 : 1 实现），使用用户线程实现（1 : N 实现），使用用户线程加轻量级进程混合实现（N : M 实现）。

1. 使用内核线程实现
    内核线程（Kernel-Level Thread，KLT）就是直接由操作系统内核（Kernel，下称内核）支持的线程，这种线程由内核来完成线程切换，内核通过操纵调度器（Scheduler）对线程进行调度，并负责将线程的任务映射到各个处理器上。每个内核线程可以视为内核的一个分身，这样操作系统就有能力同时处理多件事情，支持多线程的内核就称为多线程内核 （Multi-Threads Kernel）。
    
    程序一般不会直接使用内核线程，而是使用内核线程的一种高级接口——轻量级进程（Light Weight Process，LWP），轻量级进程就是我们通常意义上所讲的线程，由于每个轻量级进程都由一个内核线程支持，因此只有先支持内核线程，才能有轻量级进程。这种轻量级进程与内核线程之间 1 :  1 的关系称为一对一的线程模型：
    
    ![image.png](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202308192210553.png)
    
    由于内核线程的支持，每个轻量级进程都成为一个独立的调度单元，即使其中某一个轻量级进程在系统调用中被阻塞了，也不会影响整个进程继续工作。但轻量级进程也具有它的局限性：
    - 系统调用成本高
        首先，由于是基于内核线程实现的，所以各种线程操作，如创建、析构及同步，都需要进行系统调用。而系统调用的代价相对较高，需要在用户态（User Mode）和内核态（Kernel Mode）中来回切换。
    - 系统内核线程数量有限
        其次，每个轻量级进程都需要有一个内核线程的支持，因此轻量级进程要消耗一定的内核资源（如内核线程的栈空间），因此**一个系统支持轻量级进程的数量是有限的**。
2. 使用用户线程实现
    狭义上的用户线程指的是完全建立在用户空间的线程库上，系统内核不能感知到用户线程的存在及如何实现的。用户线程的建立、同步、销毁和调度完全在用户态中完成，不需要内核的帮助。如果程序实现得当，这种线程不需要切换到内核态，因此操作可以是非常快速且低消耗的，也能够支持规模更大的线程数量，部分高性能数据库中的多线程就是由用户线程实现的。这种进程与用户线程之间 1 :  N 的关系称为一对多的线程模型：
    
    ![image.png](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202308192213058.png)
    
    **用户线程的优势在于不需要系统内核支援，劣势也在于没有系统内核的支援，所有的线程操作都需要由用户程序自己去处理**。线程的创建、销毁、切换和调度都是用户必须考虑的问题，而且由于操作系统只把处理器资源分配到进程，那诸如“阻塞如何处理”“多处理器系统中如何将线程映射到其他处理器上”这类问题解决起来将会异常困难。
3. 混合实现
    在混合实现下，既存在用户线程，也存在轻量级进程。用户线程还是完全建立在用户空间中，因此用户线程的创建、切换、析构等操作依然廉价，并且可以支持大规模的用户线程并发。而操作系统支持的轻量级进程则作为用户线程和内核线程之间的桥梁，这样可以使用内核提供的线程调度功能及处理器映射，并且用户线程的系统调用要通过轻量级进程来完成，这大大降低了整个进程被完全阻塞的风险。在这种混合模式中，用户线程与轻量级进程的数量比是不定的，是 N :  M 的关系：
    
    ![image.png](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202308192215969.png)

Java 线程如何实现并不受 Java 虚拟机规范的约束，这是一个与具体虚拟机相关的话题。目前“主流”平台上的“主流”商用 Java 虚拟机的线程模型普遍都被替换为基于操作系统原生线程模型来实现，即采用 1 : 1 的线程模型。

以 HotSpot 为例，它的每一个 Java 线程都是直接映射到一个操作系统原生线程来实现的，而且中间没有额外的间接结构，所以 HotSpot 自己是不会去干涉线程调度的（可以设置线程优先级给操作系统提供调度建议），全权交给底下的操作系统去处理，所以何时冻结或唤醒线程、该给线程分配多少处理器执行时间、该把线程安排给哪个处理器核心去执行等，都是由操作系统完成的，也都是由操作系统全权决定的。

### 3.3.2. 线程调度

线程调度是指系统为线程分配处理器使用权的过程，调度主要方式有两种，分别是协同式 （Cooperative Threads-Scheduling）线程调度和抢占式（Preemptive Threads-Scheduling）线程调度。

- 协同调度
    使用协同式调度的多线程系统，线程的执行时间由线程本身来控制，**线程把自己的工作执行完了之后，要主动通知系统切换到另外一个线程上去**。

    协同式多线程的最大好处是实现简单，而且由于线程要把自己的事情干完后才会进行线程切换，切换操作对线程自己是可知的，所以一般没有什么线程同步的问题。它的坏处也很明显：线程执行时间不可控制，甚至如果一个线程的代码编写有问题，一直不告知系统进行线程切换，那么程序就会一直阻塞在那里。
- 抢占式调度
    抢占式调度的多线程系统，每个线程将由系统来分配执行时间，线程的切换不由线程本身来决定。
    
    在这种实现线程调度的方式下，线程的执行时间是系统可控的，也不会有一个线程导致整个进程甚至整个系统阻塞的问题。Java 使用的线程调度方式就是抢占式调度。

虽然说 Java 线程调度是系统自动完成的，但是我们仍然可以“建议”操作系统给某些线程多分配一点执行时间，另外的一些线程则可以少分配一点——这项操作是通过设置线程优先级来完成的。Java 语言一共设置了 10 个级别的线程优先级（`Thread.MIN_PRIORITY` 至 `Thread.MAX_PRIORITY`）。在两个线程同时处于 Ready 状态时，优先级越高的线程越容易被系统选择执行。

不过，线程优先级并不是一项稳定的调节手段，很显然因为主流虚拟机上的 Java 线程是被映射到系统的原生线程上来实现的，所以线程调度最终还是由操作系统说了算。尽管现代的操作系统基本都提供线程优先级的概念，但是并不见得能与 Java 线程的优先级一一对应。

![image.png](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202308192235419.png)

线程优先级并不是一项稳定的调节手段，这不仅仅体现在某些操作系统上不同的优先级实际会变得相同这一点上，还有其他情况让我们不能过于依赖线程优先级：优先级可能会被系统自行改变，例如在 Windows 系统中存在一个叫“优先级推进器”的功能（Priority Boosting，当然它可以被关掉），大致作用是当系统发现一个线程被执行得特别频繁时，可能会越过线程优先级去为它分配执行时间，从而减少因为线程频繁切换而带来的性能损耗。因此，我们并不能在程序中通过优先级来完全准确判断一组状态都为 Ready 的线程将会先执行哪一个。

### 3.3.3. 状态切换

Java 语言定义了 6 种线程状态，在任意一个时间点中，一个线程只能有且只有其中的一种状态，并且可以通过特定的方法在不同状态之间转换。这 6 种状态分别是：

- 新建（New）
    创建后尚未启动的线程处于这种状态。
- 运行（Runnable）
    包括操作系统线程状态中的 Running 和 Ready，也就是处于此状态的线程有可能正在执行，也有可能正在等待着操作系统为它分配执行时间。
- 无限期等待（Waiting）
    处于这种状态的线程不会被分配处理器执行时间，它们要等待被其他线程显式唤醒。以下方法会让线程陷入无限期的等待状态：
    - 没有设置 Timeout 参数的 `Object::wait ()` 方法。
    - 没有设置 Timeout 参数的 `Thread::join()` 方法。
    - `LockSupport::park()` 方法。
- 限期等待（Timed Waiting）
    处于这种状态的线程也不会被分配处理器执行时间，不过无须等待被其他线程显式唤醒，在一定时间之后它们会由系统自动唤醒。以下方法会让线程进入限期等待状态：
    - `Thread::sleep()` 方法。
    - 设置了 Timeout 参数的 `Object::wait()` 方法。
    - 设置了 Timeout 参数的 `Thread::join()` 方法。
    - `LockSupport::parkNanos()` 方法。
    - `LockSupport::parkUntil()` 方法。
- 阻塞（Blocked）
    “阻塞状态”与“等待状态”的区别是“阻塞状态”在等待着获取到一个排它锁，这个事件将在另外一个线程放弃这个锁的时候发生；而“等待状态”则是在等待一段时间，或者唤醒动作的发生。在程序等待进入同步区域的时候，线程将进入这种状态。
- 结束（Terminated）
    已终止线程的线程状态，线程已经结束执行。

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220421224339.png)



### 3.3.4. 协程的复苏

现代应用系统大多要求每一个服务都必须在极短的时间内完成计算，这样组合多个服务的总耗时才不会太长；也要求每一个服务提供者都要能同时处理数量更庞大的请求，这样才不会出现请求由于某个服务被阻塞而出现等待。

Java 目前的并发编程机制就与上述架构趋势产生了一些矛盾，1 : 1 的内核线程模型是如今 Java 虚拟机线程实现的主流选择，但是这种映射到操作系统上的线程天然的缺陷是切换、调度成本高昂，系统能容纳的线程数量也很有限。以前处理一个请求可以允许花费很长时间在单体应用中，具有这种线程切换的成本也是无伤大雅的，但现在**在每个请求本身的执行时间变得很短、数量变得很多的前提下，用户线程切换的开销甚至可能会接近用于计算本身的开销**，这就会造成严重的浪费。

> [!info] 内核线程调度成本为什么会更高？
> 
> 内核线程的调度成本主要来自于用户态与核心态之间的状态转换，而这两种状态转换的开销主要来自于响应中断、保护和恢复执行现场的成本。
> 
> 以线程的角度来看，方法的调用栈中存储了各类信息；而以操作系统和硬件的角度来看，则是存储在内存、缓存和寄存器中的一个个具体数值。物理硬件的各种存储设备和寄存器是被操作系统内所有线程共享的资源，当中断发生，从线程 A 切换到线程 B 去执行之前，操作系统首先要把线程 A 的上下文数据妥善保管好，然后把寄存器、内存分页等恢复到线程 B 挂起时候的状态，这样线程 B 被重新激活后才能仿佛从来没有被挂起过。这种保护和恢复现场的工作，免不了涉及一系列数据在各种寄存器、缓存中的来回拷贝，当然不可能是一种轻量级的操作。

如果说内核线程的切换开销是来自于保护和恢复现场的成本，那如果改为采用用户线程，这部分开销就能够省略掉吗？答案是“不能”。但是，一旦把保护、恢复现场及调度的工作从操作系统交到程序员手上，那我们就可以打开脑洞，通过玩出很多新的花样来缩减这些开销。

协程的主要优势是轻量，无论是有栈协程还是无栈协程，都要比传统内核线程要轻量得多。如果进行量化的话，那么如果不显式设置-Xss 或 `-XX:ThreadStackSize`，则在 64 位 Linux 上 HotSpot 的线程栈容量默认是 1MB，此外内核数据结构（Kernel Data Structures）还会额外消耗 16KB 内存。与之相对的，一个协程的栈通常在几百个字节到几 KB 之间，所以 Java 虚拟机里线程池容量达到两百就已经不算小了，而很多支持协程的应用中，同时并存的协程数量可数以十万计。

协程当然也有它的局限，需要在应用层面实现的内容（调用栈、调度器这些）特别多，这个缺点就不赘述了。除此之外，协程在最初，甚至在今天很多语言和框架中会被设计成协同式调度，这样在语言运行平台或者框架上的调度器就可以做得非常简单。不过有不少资料上显示，既然取了“协程”这样的名字，它们之间就一定以协同调度的方式工作。

## 3.4. 线程安全性问题

当多个线程同时访问一个对象时，如果不用考虑这些线程在运行时环境下的调度和交替执行，也不需要进行额外的同步，或者在调用方进行任何其他的协调操作，调用这个对象的行为都可以获得正确的结果，那就称这个对象是线程安全的。

线程安全要求代码都必须具备一个共同特征：**代码本身封装了所有必要的正确性保障手段（如互斥同步等）**，令调用者无须关心多线程下的调用问题，更无须自己实现任何措施来保证多线程环境下的正确调用。

我们这里讨论的线程安全，将以多个线程之间存在共享数据访问为前提。因为如果根本不存在多线程，又或者一段代码根本不会与其他线程共享数据，那么从线程安全的角度上看，程序是串行执行还是多线程执行对它来说是没有什么区别的。

我们可以将 Java 语言中各种操作共享的数据分为以下五类：不可变、绝对线程安全、相对线程安全、线程兼容和线程对立：
1. 不可变
    不可变 （Immutable）的对象一定是线程安全的，无论是对象的方法实现还是方法的调用者，都不需要再进行任何线程安全保障措施。
    
    Java 语言中，如果多线程共享的数据是一个基本数据类型，那么只要在定义时使用 final 关键字修饰它就可以保证它是不可变的。如果共享数据是一个对象，由于 Java 语言目前暂时还没有提供值类型的支持，那就需要对象自行保证其行为不会对其状态产生任何影响才行。
2. 绝对线程安全
    一个类要达到“不管运行时环境如何，调用者都不需要任何额外的同步措施”可能需要付出非常高昂的，甚至不切实际的代价。
    
    例如 Vector 对象虽然是线程安全的，但是它并不是绝对线程安全的。若一个线程删除索引位置 i 的元素，而另一个线程修改索引位置 i 的元素，即使可以满足先修改再删除这样的顺序，但是反过来执行的时候就会抛出异常。所以针对这种并发操作的情况它并不是绝对安全的。
3. 相对线程安全
    相对线程安全就是我们通常意义上所讲的线程安全，它需要保证对这个对象单次的操作是线程安全的，我们在调用的时候不需要进行额外的保障措施，但是对于一些特定顺序的连续调用，就可能需要在调用端使用额外的同步手段来保证调用的正确性。
    
    在 Java 语言中，大部分声称线程安全的类都属于这种类型，例如 Vector、HashTable、Collections 的 `synchronizedCollection()` 方法包装的集合等。
4. 线程兼容
    线程兼容是指对象本身并不是线程安全的，但是可以通过在调用端正确地使用同步手段来保证对象在并发环境中可以安全地使用。我们平常说一个类不是线程安全的，通常就是指这种情况。Java 类库 API 中大部分的类都是线程兼容的，如与前面的 Vector 和 HashTable 相对应的集合类 ArrayList 和 HashMap 等。
5. 线程对立
    线程对立是指不管调用端是否采取了同步措施，都无法在多线程环境中并发使用代码。由于 Java 语言天生就支持多线程的特性，线程对立这种排斥多线程的代码是很少出现的，而且通常都是有害的，应当尽量避免。
    
    一个线程对立的例子是 Thread 类的 `suspend()` 和 `resume()` 方法。如果有两个线程同时持有一个线程对象，一个尝试去中断线程，一个尝试去恢复线程，在并发进行的情况下，无论调用时是否进行了同步，目标线程都存在死锁风险——假如 `suspend()` 中断的线程就是即将要执行 `resume()` 的那个线程，那就肯定要产生死锁了。也正是这个原因，`suspend()` 和 `resume()` 方法都已经被声明废弃了。常见的线程对立的操作还有 `System.setIn()`、`Sytem.setOut()` 和 `System.runFinalizersOnExit()` 等。

### 3.4.1. 互斥同步

互斥同步（Mutual Exclusion & Synchronization）是一种最常见也是最主要的并发正确性保障手段。**同步是指在多个线程并发访问共享数据时，保证共享数据在同一个时刻只被一条（或者是一些，当使用信号量的时候）线程使用**。而互斥是实现同步的一种手段，**临界区（Critical Section）、互斥量 （Mutex）和信号量（Semaphore）**都是常见的互斥实现方式。因此在“互斥同步”这四个字里面，互斥是因，同步是果；互斥是方法，同步是目的。

在 Java 里面，最基本的互斥同步手段就是 synchronized 关键字，这是一种块结构（Block Structured）的同步语法。synchronized 关键字经过 Javac 编译之后，会在同步块的前后分别形成 monitorenter 和 monitorexit 这两个字节码指令。这两个字节码指令都需要一个 reference 类型的参数来指明要锁定和解锁的对象。如果 Java 源码中的 synchronized 明确指定了对象参数，那就以这个对象的引用作为 reference；如果没有明确指定，那将根据 synchronized 修饰的方法类型（如实例方法或类方法），来决定是取代码所在的对象实例还是取类型对应的 Class 对象来作为线程要持有的锁。

根据《Java 虚拟机规范》的要求，在执行 monitorenter 指令时，首先要去尝试获取对象的锁。如果这个对象没被锁定，或者当前线程已经持有了那个对象的锁，就把锁的计数器的值增加一，而在执行 monitorexit 指令时会将锁计数器的值减一。一旦计数器的值为零，锁随即就被释放了。如果获取对象锁失败，那当前线程就应当被阻塞等待，直到请求锁定的对象被持有它的线程释放为止。

从执行成本的角度看，**持有锁是一个重量级（Heavy-Weight）的操作**。Java 的线程是映射到操作系统的原生内核线程之上的，**如果要阻塞或唤醒一条线程，则需要操作系统来帮忙完成，这就不可避免地陷入用户态到核心态的转换中**，进行这种状态转换需要耗费很多的处理器时间。尤其是对于代码特别简单的同步块（譬如被 synchronized 修饰的 `getter()` 或 `setter()` 方法），**状态转换消耗的时间甚至会比用户代码本身执行的时间还要长**。因此才说， synchronized 是 Java 语言中一个重量级的操作。

重入锁（ReentrantLock）是 Lock 接口最常见的一种实现，顾名思义，它与 synchronized 一样是可重入的。在基本用法上，ReentrantLock 也与 synchronized 很相似，只是代码写法上稍有区别而已。不过，ReentrantLock 与 synchronized 相比增加了一些高级功能，主要有以下三项：等待可中断、可实现公平锁及锁可以绑定多个条件。
- 等待可中断
    指当持有锁的线程长期不释放锁的时候，正在等待的线程可以选择放弃等待，改为处理其他事情。可中断特性对处理执行时间非常长的同步块很有帮助。
- 公平锁
    是指多个线程在等待同一个锁时，必须按照申请锁的时间顺序来依次获得锁；而非公平锁则不保证这一点，在锁被释放时，任何一个等待锁的线程都有机会获得锁。synchronized 中的锁是非公平的，ReentrantLock 在默认情况下也是非公平的，但可以通过带布尔值的构造函数要求使用公平锁。不过一旦使用了公平锁，将会导致 ReentrantLock 的性能急剧下降，会明显影响吞吐量。
- 锁绑定多个条件
    指一个 ReentrantLock 对象可以同时绑定多个 Condition 对象。在 synchronized 中，锁对象的 `wait()` 跟它的 `notify()` 或者 `notifyAll()` 方法配合可以实现一个隐含的条件，如果要和多于一个的条件关联的时候，就不得不额外添加一个锁；而 ReentrantLock 则无须这样做，多次调用 `newCondition()` 方法即可。

在 synchronized 与 ReentrantLock 各有优势：
- synchronized 是在 Java 语法层面的同步，足够清晰，也足够简单。每个 Java 程序员都熟悉 synchronized，但 JUC 中的 Lock 接口则并非如此。因此在只需要基础的同步功能时，更推荐 synchronized。
- Lock 应该确保在 finally 块中释放锁，否则一旦受同步保护的代码块中抛出异常，则有可能永远不会释放持有的锁。这一点必须由程序员自己来保证，而使用 synchronized 的话则可以由 Java 虚拟机来确保即使出现异常，锁也能被自动释放。
- 尽管在 JDK 5 时代 ReentrantLock 曾经在性能上领先过 synchronized，但这已经是十多年之前的胜利了。从长远来看，Java 虚拟机更容易针对 synchronized 来进行优化，因为 Java 虚拟机可以在线程和对象的元数据中记录 synchronized 中锁的相关信息，而使用J.U.C 中的 Lock 的话，Java 虚拟机是很难得知具体哪些锁对象是由特定线程锁持有的。

### 3.4.2. 非阻塞同步

互斥同步面临的主要问题是进行线程阻塞和唤醒所带来的性能开销，因此这种同步也被称为阻塞同步（Blocking Synchronization）。从解决问题的方式上看，互斥同步属于一种悲观的并发策略，其总是认为只要不去做正确的同步措施（例如加锁），那就肯定会出现问题，无论共享的数据是否真的会出现竞争，它都会进行加锁。这将会导致**用户态到核心态转换、维护锁计数器和检查是否有被阻塞的线程需要被唤醒等开销**。

随着硬件指令集的发展，我们已经有了另外一个选择：**基于冲突检测的乐观并发策略**，通俗地说就是不管风险，先进行操作，如果没有其他线程争用共享数据，那操作就直接成功了；如果共享的数据的确被争用，产生了冲突，那再进行其他的补偿措施，**最常用的补偿措施是不断地重试，直到出现没有竞争的共享数据为止**。这种乐观并发策略的实现不再需要把线程阻塞挂起，因此这种同步操作被称为非阻塞同步（Non-Blocking Synchronization），使用这种措施的代码也常被称为无锁（Lock-Free） 编程。

在硬件指令集的支持下，某些从语义上看起来需要多次操作的行为可以只通过一条处理器指令就能完成，这类指令常用的有：
- 测试并设置（Test-and-Set）
- 获取并增加（Fetch-and-Increment）
- 交换（Swap）
- 比较并交换（Compare-and-Swap，下文称 CAS）
- 加载链接/条件储存（Load-Linked/Store-Conditional，下文称 LL/SC）

CAS 指令需要有三个操作数，分别是内存位置（在 Java 中可以简单地理解为变量的内存地址，用 V 表示）、旧的预期值（用 A 表示）和准备设置的新值（用 B 表示）。CAS 指令执行时，当且仅当 V 符合 A 时，处理器才会用 B 更新 V 的值，否则它就不执行更新。但是，不管是否更新了 V 的值，都会返回 V 的旧值，上述的处理过程是一个原子操作，执行期间不会被其他线程中断。

尽管 CAS 看起来很美好，既简单又高效，但显然这种操作无法涵盖互斥同步的所有使用场景，并且 CAS 从语义上来说并不是真正完美的，它存在一个逻辑漏洞：如果一个变量 V 初次读取的时候是 A 值，并且在准备赋值的时候检查到它仍然为 A 值，那就能说明它的值没有被其他线程改变过了吗？这是不能的，因为如果在这段期间它的值曾经被改成 B，后来又被改回为 A，那 CAS 操作就会误认为它从来没有被改变过。这个漏洞称为 **CAS 操作的“ABA 问题”**。JUC 包为了解决这个问题，提供了一个带有标记的原子引用类 AtomicStampedReference，它可以通过控制变量值的版本来保证 CAS 的正确性。不过目前来说这个类处于相当鸡肋的位置，大部分情况下 ABA 问题不会影响程序并发的正确性，如果需要解决 ABA 问题，改用传统的互斥同步可能会比原子类更为高效。

### 3.4.3. 无同步

要保证线程安全，也并非一定要进行阻塞或非阻塞同步，同步与线程安全两者没有必然的联系。同步只是保障存在共享数据争用时正确性的手段，**如果能让一个方法本来就不涉及共享数据，那它自然就不需要任何同步措施去保证其正确性**，因此会有一些代码天生就是线程安全的。

可重入代码（Reentrant Code）：这种代码又称纯代码（Pure Code），是指可以在代码执行的任何时刻中断它，转而去执行另外一段代码（包括递归调用它本身），而在控制权返回后，原来的程序不会出现任何错误，也不会对结果有所影响。在特指多线程的上下文语境里（不涉及信号量等因素 ），我们可以认为可重入代码是线程安全代码的一个真子集，这意味着相对线程安全来说，可重入性是更为基础的特性，它可以保证代码线程安全，即所有可重入的代码都是线程安全的，但并非所有的线程安全的代码都是可重入的。

可重入代码有一些共同的特征，例如，不依赖全局变量、存储在堆上的数据和公用的系统资源，用到的状态量都由参数中传入，不调用非可重入的方法等。我们可以通过一个比较简单的原则来判断代码是否具备可重入性：如果一个方法的返回结果是可以预测的，只要输入了相同的数据，就都能返回相同的结果，那它就满足可重入性的要求，当然也就是线程安全的。

线程本地存储（Thread Local Storage）：如果一段代码中所需要的数据必须与其他代码共享，那就看看这些共享数据的代码是否能保证在同一个线程中执行。如果能保证，我们就可以把共享数据的可见范围限制在同一个线程之内，这样，无须同步也能保证线程之间不出现数据争用的问题。

## 3.5. 锁优化

高效并发是从 JDK 5 升级到 JDK 6 后一项重要的改进项，HotSpot 虚拟机开发团队在这个版本上花费了大量的资源去实现各种锁优化技术，如适应性自旋（Adaptive Spinning）、锁消除（Lock Elimination）、锁膨胀（Lock Coarsening）、轻量级锁（Lightweight Locking）、偏向锁（Biased Locking）等，这些技术都是为了在线程之间更高效地共享数据及解决竞争问题，从而提高程序的执行效率。

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220423000737.png)

### 3.5.1. 自旋锁与自适应自旋

前面我们讨论互斥同步的时候，提到了互斥同步对性能最大的影响是阻塞的实现，挂起线程和恢复线程的操作都需要转入内核态中完成，这些操作给 Java 虚拟机的并发性能带来了很大的压力。同时，虚拟机的开发团队也注意到在许多应用上，共享数据的锁定状态只会持续很短的一段时间，为了这段时间去挂起和恢复线程并不值得。现在绝大多数的个人电脑和服务器都是多路（核）处理器系统，如果物理机器有一个以上的处理器或者处理器核心，能让两个或以上的线程同时并行执行，我们就可以**让后面请求锁的那个线程“稍等一会”，但不放弃处理器的执行时间，看看持有锁的线程是否很快就会释放锁**。为了让线程等待，我们只须让线程执行一个忙循环（自旋），这项技术就是所谓的自旋锁。

自旋等待不能代替阻塞，且先不说对处理器数量的要求，自旋等待本身虽然避免了线程切换的开销，但它是要占用处理器时间的，所以如果锁被占用的时间很短，自旋等待的效果就会非常好，反之如果锁被占用的时间很长，那么自旋的线程只会白白消耗处理器资源，而不会做任何有价值的工作，这就会带来性能的浪费。因此自旋等待的时间必须有一定的限度，如果自旋超过了限定的次数仍然没有成功获得锁，就应当使用传统的方式去挂起线程。自旋次数的默认值是十次，用户也可以使用参数 `-XX:PreBlockSpin` 来自行更改。

在 JDK 6 中对自旋锁的优化，引入了自适应的自旋。**自适应意味着自旋的时间不再是固定的了，而是由前一次在同一个锁上的自旋时间及锁的拥有者的状态来决定的**。如果在同一个锁对象上，自旋等待刚刚成功获得过锁，并且持有锁的线程正在运行中，那么虚拟机就会认为这次自旋也很有可能再次成功，进而允许自旋等待持续相对更长的时间，比如持续 100 次忙循环。另一方面，如果对于某个锁，自旋很少成功获得过锁，那在以后要获取这个锁时将有可能直接省略掉自旋过程，以避免浪费处理器资源。有了自适应自旋，随着程序运行时间的增长及性能监控信息的不断完善，虚拟机对程序锁的状况预测就会越来越精准，虚拟机就会变得越来越“聪明”了。

### 3.5.2. 锁消除

锁消除是指虚拟机即时编译器在运行时，对一些代码要求同步，但是对被检测到不可能存在共享数据竞争的锁进行消除。锁消除的主要判定依据来源于逃逸分析的数据支持，如果判断到一段代码中，在堆上的所有数据都不会逃逸出去被其他线程访问到，那就可以把它们当作栈上数据对待，认为它们是线程私有的，同步加锁自然就无须再进行。

### 3.5.3. 锁粗化

原则上，我们在编写代码的时候，总是推荐将同步块的作用范围限制得尽量小——只在共享数据的实际作用域中才进行同步，这样是为了使得需要同步的操作数量尽可能变少，即使存在锁竞争，等待锁的线程也能尽可能快地拿到锁。

大多数情况下，上面的原则都是正确的，但是如果一系列的连续操作都对同一个对象反复加锁和解锁，甚至加锁操作是出现在循环体之中的，那即使没有线程竞争，频繁地进行互斥同步操作也会导致不必要的性能损耗。如果虚拟机探测到有这样一串零碎的操作都对同一个对象加锁，将会把加锁同步的范围扩展（粗化）到整个操作序列的外部。

### 3.5.4. 轻量级锁

轻量级锁是 JDK 6 时加入的新型锁机制，它名字中的“轻量级”是相对于使用操作系统互斥量来实现的传统锁而言的，因此传统的锁机制就被称为“重量级”锁。不过，需要强调一点，轻量级锁并不是用来代替重量级锁的，它设计的初衷是**在没有多线程竞争的前提下，减少传统的重量级锁使用操作系统互斥量产生的性能消耗**。

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220422223237.png)

轻量级锁的工作过程如下：
1. 拷贝 Mark Word 到线程中
    在代码即将进入同步块的时候，如果此同步对象没有被锁定（锁标志位为“01”状态），虚拟机首先将在当前线程的栈帧中建立一个名为锁记录（Lock Record）的空间，用于存储锁对象目前的 Mark Word 的拷贝（官方为这份拷贝加了一个 Displaced 前缀，即 Displaced Mark Word）。
    
    ![image.png](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202308192334845.png)
2. 使用 CAS 操作更新对象头信息
    然后，虚拟机将使用 CAS 操作尝试把对象的 Mark Word 更新为指向 Lock Record 的指针。如果这个更新动作成功了，即代表该线程拥有了这个对象的锁，并且对象 Mark Word 的锁标志位（Mark Word 的最后两个比特）将转变为“00”，表示此对象处于轻量级锁定状态。
    
    ![image.png](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202308192334834.png)
    
    如果这个更新操作失败了，那就意味着至少存在一条线程与当前线程竞争获取该对象的锁。虚拟机首先会检查对象的 Mark Word 是否指向当前线程的栈帧，如果是，说明当前线程已经拥有了这个对象的锁，那直接进入同步块继续执行就可以了，否则就说明这个锁对象已经被其他线程抢占了。如果出现两条以上的线程争用同一个锁的情况，那轻量级锁就不再有效，必须要膨胀为重量级锁（自旋一定次数后触发），锁标志的状态值变为“10”，此时 Mark Word 中存储的就是指向重量级锁（互斥量）的指针，后面等待锁的线程也必须进入阻塞状态。
3. 解锁
    解锁过程也同样是通过 CAS 操作来进行的，如果对象的 Mark Word 仍然指向线程的锁记录，那就用 CAS 操作把对象当前的 Mark Word 和线程中复制的 Displaced Mark Word 替换回来。假如能够成功替换，那整个同步过程就顺利完成了；如果替换失败，则说明有其他线程尝试过获取该锁，就要在释放锁的同时，唤醒被挂起的线程。

**轻量级锁能提升程序同步性能的依据是“对于绝大部分的锁，在整个同步周期内都是不存在竞争的”这一经验法则**。如果没有竞争，轻量级锁便通过 CAS 操作成功避免了使用互斥量的开销；但如果确实存在锁竞争，除了互斥量的本身开销外，还额外发生了 CAS 操作的开销。因此在有竞争的情况下，轻量级锁反而会比传统的重量级锁更慢。

### 3.5.5. 偏向锁

偏向锁也是 JDK 6 中引入的一项锁优化措施，它的目的是**消除数据在无竞争情况下的同步原语，进一步提高程序的运行性能。如果说轻量级锁是在无竞争的情况下使用 CAS 操作去消除同步使用的互斥量，那偏向锁就是在无竞争的情况下把整个同步都消除掉，连 CAS 操作都不去做了**。

偏向锁中的“偏”，就是偏心的“偏”、偏袒的“偏”。它的意思是这个锁会偏向于第一个获得它的线程，如果在接下来的执行过程中，该锁一直没有被其他的线程获取，则持有偏向锁的线程将永远不需要再进行同步。

假设当前虚拟机启用了偏向锁（启用参数 `-XX:+UseBiased Locking`，这是自 JDK 6 起 HotSpot 虚拟机的默认值），那么当锁对象第一次被线程获取的时候，虚拟机将会把对象头中的标志位设置为“01”、把偏向模式设置为“1”，表示进入偏向模式。同时使用 CAS 操作把获取到这个锁的线程的 ID 记录在对象的 Mark Word 之中。如果 CAS 操作成功，持有偏向锁的线程以后每次进入这个锁相关的同步块时，虚拟机都可以不再进行任何同步操作（例如加锁、解锁及对 Mark Word 的更新操作等）。

一旦出现另外一个线程去尝试获取这个锁的情况，偏向模式就马上宣告结束。根据锁对象目前是否处于被锁定的状态决定是否撤销偏向（偏向模式设置为“0”），撤销后标志位恢复到未锁定（标志位为“01”）或轻量级锁定（标志位为“00”）的状态，后续的同步操作就按照上面介绍的轻量级锁那样去执行。

![image.png](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202308192340313.png)

> [!question] 使用偏向锁时 hashcode 存到哪去了？
> 
> 当对象进入偏向状态的时候，Mark Word 大部分的空间（23 个比特）都用于存储持有锁的线程 ID 了，这部分空间占用了原有存储对象哈希码的位置，那原来对象的哈希码怎么办呢？
> 
> 在 Java 语言里面一个对象如果计算过哈希码，就应该一直保持该值不变（强烈推荐但不强制，因为用户可以重载 `hashCode()` 方法按自己的意愿返回哈希码），否则很多依赖对象哈希码的 API 都可能存在出错风险。而作为绝大多数对象哈希码来源的 `Object::hashCode()` 方法，返回的是对象的一致性哈希码（Identity Hash Code），这个值是能强制保证不变的，它通过在对象头中存储计算结果来保证第一次计算之后，再次调用该方法取到的哈希码值永远不会再发生改变。因此，当一个对象已经计算过一致性哈希码后，它就再也无法进入偏向锁状态了；而当一个对象当前正处于偏向锁状态，又收到需要计算其一致性哈希码请求时，它的偏向状态会被立即撤销，并且锁会膨胀为重量级锁。在重量级锁的实现中，对象头指向了重量级锁的位置，代表重量级锁的 ObjectMonitor 类里有字段可以记录非加锁状态（标志位为“01”）下的 Mark Word，其中自然可以存储原来的哈希码。

偏向锁可以提高带有同步但无竞争的程序性能，但它同样是一个带有效益权衡（Trade Off）性质的优化，也就是说它并非总是对程序运行有利。如果程序中大多数的锁都总是被多个不同的线程访问，那偏向模式就是多余的。在具体问题具体分析的前提下，有时候使用参数 `-XX:UseBiasedLocking` 来禁止偏向锁优化反而可以提升性能。

