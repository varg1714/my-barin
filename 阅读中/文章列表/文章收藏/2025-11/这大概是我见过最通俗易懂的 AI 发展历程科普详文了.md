---
source: https://mp.weixin.qq.com/s/hFejZCXex_nw0Nrg-egP0g
create: 2025-11-23 22:28
read: false
knowledge: false
---
作者： 丁亮亮 / smallniding 

本文将求根溯源，从历史时间线发展的角度一起来看看 AI 是如何诞生的，以及在诞生后这几十年的发展过程中经历了哪些变革和演进，在演进的过程中我会介绍大量涉及到的细分技术模块，希望大家对 AI 整体的技术模块及互相之间的关系都有系统化的了解（由于篇幅有限，不会非常深入垂直的去讲某一个技术，大家对任一细分技术感兴趣可以单独交流）。最后，我会结合自己亲自参与的一些 AI 案例来一起看如何更好的应用 AI 来帮助我们的业务提升价值，同时，展望 AI 未来的前景及发展。

在如今信息爆炸及互联网飞速发展的时代，AI 成为了这几年全世界最炙手可热的科技话题，而我们每天会面对无数的 AI 资讯，无论是大模型、智能体还是各行各业的 AI 应用，而这都被动的把大家推到了 AI 的风口浪尖上：在这个时代，不去拥抱 AI，可能就会被 AI 淘汰。但随着 AI 的发展，涉及到了大量的细分技术模块及专业术语，大部分想去尝试 AI 的同学可能都没有好好思考过：到底什么是 AI？AI 到底怎么去结合自己的业务更好的应用？AI 未来的发展前景如何？

### **一、前世：AI 出现前**

#### **1、唯一的高等智慧动物 - 人类**

人类在地球上最早以人猿的方式出现，经历了几百万年的进化和发展，人类成为了地球食物链的顶端，也成为了唯一的高等智慧动物：具备了复杂的多语言、推理和抽象思维、创造、发明以及最重要的多文明体系。

而人类能发展成为这样，背后最重要的原因是：人类大脑的独特性，人类大脑能支撑抽象思维、推理思维、语言创造、知识学习和传承、发明和创造、文明建立和传承等多维度能力，而这些正是人类的 “智能” 能力。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/j3gficicyOvavSZsbSmdnMumMXZibBPGboKibbzsjMVHTR6c42lhUicCKqqYuoGgKbaHtgjeXzQs13k2uCHylp9CTag/640?wx_fmt=jpeg&from=appmsg#imgIndex=0)

（人类大脑的简要结构图 - 图片来源于 AI 生成）

而通过上图我会发现除了左脑、右脑这些分区结构外，还有树突、轴突这些专业名词，而这些专业名词正是大脑里 **“神经元”** 的一部分，而人类大脑拥有将近 **860 亿个 “神经元”**，这些 **“神经元”** 承担了所有信息的处理和传递工作，所有大脑的活动都依赖其协同工作。这也很显而易见的让人类具备了**感知、思维、情绪、运动控制、语言交流**等多维度的**智能**能力。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/j3gficicyOvavSZsbSmdnMumMXZibBPGboKwHNSQadzwuPl6SStIAnY4YdIdXwIFFNuPAiaoNACha3KQqfJQGQn4MQ/640?wx_fmt=jpeg&from=appmsg#imgIndex=1)

（人类大脑 “神经元” 的工作原理 - 图片来源于 AI 生成）

在人类几百万年的发展过程中，人类依靠大脑具备了非常强大的智能能力，这是地球上其他动物无法比及的，人类也在发展的过程形成了各个地区的语言和文明。人类用各自的语言交流、建设和发展各自的文明，在数百万年中都非常稳定，但后来人类发现虽然大脑很强大，拥有几百亿的神经元，但记性和效率太差了：比如正常人背 1000 个数字会显得非常困难，算 100 遍乘法会懵。那么人类就在想是不是可以解放大脑、让机器代替人类去打工？

#### **2、第一台计算机诞生**

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/j3gficicyOvavSZsbSmdnMumMXZibBPGboKVzoia0kfcql9wtujQRO4TuKPpPXgAib456E249AcjpggQuYL8o5icib3Yg/640?wx_fmt=jpeg&from=appmsg#imgIndex=2)

（第一台计算机诞生 - 图片来源于 AI 生成）

这就催生了第一台计算机的出现，第一台计算机在 1946 年被莫奇利和埃克特发明，开创性的解决了 “快速算、精准存” 的问题，但有一个问题是这台计算机只是听话但不会思考，比如你让他计算 1000 遍乘法可以迅速给你算出来，但他不会思考这些乘法之间的规则或关联以便下次更好的计算。直到后来，科学家发现人类大脑的厉害之处不在于有 860 亿个神经元，而在于 860 亿个神经元像 “亿级路由器” 一样互相联通，形成了极其复杂的 **“神经网络”**，而 **“神经网络”** 可以让大脑具备自我学习、提炼规律的智能能力，于是他们想：能不能模仿神经网络，造一个 **“机器神经网络”**？这其实就有了 AI 的雏形，我们在下一章节和大家一一道来。

### **二、今生：AI 初生期（1956-1989）**

#### **1、AI 概念定义**

在 1956 年达特茅斯会议上，约翰 · 麦卡锡等科学家首次提出 “**人工智能（Artificial Intelligence 缩写为 AI）**” 的专业术语，明确提出了 “**让机器模拟人类智能**” 的研究目标，这是 AI 成为独立学科的起点。

那么我们可以来看看，到底什么是 “人工智能（AI）”？

这里其实已经有明确定义了：**人工智能（AI）是让机器模拟人类智能的技术总称。**

那么问题来了，到底是什么 **“人类智能”**？我们基于上一章节的内容，可以简要概括为 **“人类智能” 即**是：**让机器具备 “感知、思考、决策、执行” 的能力。**

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/j3gficicyOvavSZsbSmdnMumMXZibBPGboKlEJElAbKnXGkibBDuWW8ialrwYlVmdK0yYsz0iaSaIVWzCjBXQwvtgAug/640?wx_fmt=jpeg&from=appmsg#imgIndex=3)

（过马路示意 - 图片来源于 AI 生成）

我们来通过一个 “过马路” 的例子来解释到底什么是“**感知、思考、决策、执行**”：

● **“感知”**：对人类而言，有耳朵和眼睛等器官，我们在过马路时可以看到红绿灯在变化，也可以听到汽车经过时的鸣笛声，而这些 “看” 和“听”其实就是我们的 “感知” 能力，我们可以通过感知能力获取到这些信息。

● **“思考”：**当有了 “感知” 能力获取到信息后，我们会用我们的大脑进行分析和推理，这其实就是我们的 “思考” 能力，比如我们在过马路时看到红绿灯是红灯后，我们会停下来等待变成绿灯后再通过，这其实就是一个 “思考” 的过程

● **“决策”：**我们还以过马路例子来分析，我们遇到红绿灯时，我们其实有多个选择，比如 “闯红灯”，亦或者是 “等待绿灯再通过”，但我们最后选择了后者，也是为了我们的安全第一，这其实就是 “决策”

● **“执行”：**仍然以过马路这个例子来分析，当我们最终 “决策” 了等待绿灯后再通过的决定后，等绿灯亮起时，这时我们迈开腿，走过人行道，到达马路对面，这个过程其实就是“执行”

人类以上的 “感知、思考、决策、执行” 构成了 “智能” 能力，但如果要让机器具备这些 “智能” 能力会有什么难点？

最大的难点其实首先是机器不懂我们的语言，所以就更别谈分析、推理、思考之类的能力了，这时候另一个学科就可以很好的结合进来：**自然语言处理（Natural Language Processing 缩写为 NLP）。**

#### **2、自然语言处理（NLP）**

其实 “**自然语言处理（NLP）**” 并不是 AI 出现后才出来，第一台计算机在 1946 年出现，在之后的 1950 年，图灵就提出 “**如果一台机器能通过文本对话让人类无法分辨它是人还是机器，那它就具有了智能**”，这其实便是 “自然语言处理（NLP）” 的目标。只不过 AI 诞生后，刚好有了这个契机，“**自然语言处理（NLP）**” 也成为了 AI 早期发展最重要的相辅相成的模块。

那么我们来解释一下到底什么是 **“自然语言处理（NLP）”：**

首先看什么是 “自然语言”，“**自然语言**” 是 “**人类在日常生活中自然而然发展和使用的语言**”，比如早期的甲骨文、象形文字等，以及发展到现在的各种语言（包含地方方言等），但我们通常涉及的编程语言就不算是 “自然语言”。而 **“自然语言处理（NLP）”** 就是：**让计算机能够理解、解释、操纵和生成人类自然语言**，通俗点讲就是**教计算机 “听懂人话、说人话、看懂人写的字、写出人能看懂的内容”。**

**我们可以来通过例子简单分析下：**

1、人与人之间通常是这么语言沟通的：

![](https://mmbiz.qpic.cn/sz_mmbiz_png/j3gficicyOvavSZsbSmdnMumMXZibBPGboK2q6mdXTDwXicD1HRC5UYrs8nNNAxbOK5Nbfbib1gYBHiaOrtofvw81fhw/640?wx_fmt=png&from=appmsg#imgIndex=4)

（图片来源于《深度学习进阶 - 自然语言处理》）

2、而人和动物之间，动物其实是听不懂我们的话的（这也是科学难题之一，期待未来有所突破）：

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/j3gficicyOvavSZsbSmdnMumMXZibBPGboKXZ8EibZvO9Cwic1HibuquczfibtHHm3zH24nK5n4pKE5j7rK1ZxibCqbkLw/640?wx_fmt=jpeg&from=appmsg#imgIndex=5)

（图片来源于《深度学习进阶 - 自然语言处理》）

3、而试想一下，有了**自然语言处理（NLP）**，人类和机器之间就可以沟通了：

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/j3gficicyOvavSZsbSmdnMumMXZibBPGboKibiaR4VWAtbOqfY8XB8yt0s7cvjSTpjmOuXib8RQ3zVBxiarvEic6ULKWRg/640?wx_fmt=jpeg&from=appmsg#imgIndex=6)

（图片来源于《深度学习进阶 - 自然语言处理》）

#### **3、AI 初生期案例分析**

在**自然语言处理（NLP）**的加持下，AI 初步在一些场景取得了应用，比如早期的机器翻译：

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/j3gficicyOvavSZsbSmdnMumMXZibBPGboKOIJpnbxZMmhDRtNmtLuEaeKmQo5ATxRI2awaj4Qf9ckJPrhq1dMJNQ/640?wx_fmt=jpeg&from=appmsg#imgIndex=7)

（早期机器翻译原理示意 - 图片来源于 AI 生成）

大家可以发现，在早期机器翻译中，有词典、语法规则库、转换规则等等约定好的规则库，而这些规则库决定了翻译功能的具体实现。

**下面我们以一个具体的例子来说明：**

**The apple is red.**

这个英文句子相信大家都能翻译出来，也比较简单，我们看看机器翻译的原理：

**第一步：查词典**

计算机会把句子拆成单词，然后去一个巨大的电子词典里查每个词的意思。

<table><thead><tr><th><section><span leaf="">英文单词</span></section></th><th><section><span leaf="">词典里给的主要中文意思</span></section></th></tr></thead><tbody><tr><td><section><span leaf="">The</span></section></td><td><section><span leaf="">这 / 这个 / 那（通常放在最前面）</span></section></td></tr><tr><td><section><span leaf="">apple</span></section></td><td><section><span leaf="">苹果</span></section></td></tr><tr><td><section><span leaf="">is</span></section></td><td><section><span leaf="">是</span></section></td></tr><tr><td><section><span leaf="">red</span></section></td><td><section><span leaf="">红色的</span></section></td></tr></tbody></table>

好，现在计算机得到了一堆中文词：【这】【苹果】【是】【红色的】。

**第二步：调整顺序**

计算机会用上一条非常简单的语法规则：英语的 **[主词] + [is] + [形容词]** 结构，对应中文的 **[主词] + [是] + [形容词] + 的**。

它发现 The apple 是主词，is 是系动词，red 是形容词。完美匹配规则！

所以，它把这些词按照规则排列起来。 最终翻译结果：**这苹果是红色的。**

**问题暴露：哪里不对劲？**

这个翻译对吗？其实按字面意思都对了，本身语法也都对，但有一个地方做的不够好：

● **不地道的表达：**虽然 “苹果是红色的” 语法正确，但在日常口语中，我们可能更常说“这个苹果是红的”，或者直接说“苹果很红”，而机器无法理解这种语言习惯和微妙差别。

**这个简单例子揭示的根本缺陷：**

● **缺乏灵活性：**机器只会死板地应用规则，无法像人一样根据语境或上下文或情感翻译出更合适的内容

● **没有 “语感”：**它不知道什么样的中文听起来更地道和自然，所以翻译出来也会比较死板

#### **4、AI 初生期小结**

通过这个机器翻译的例子，我们会发现在 **AI 初生期（1956-1989）**，虽然有**自然语言处理（NLP）**的加持，但 AI 基本都是死板的按人类制定的规则去执行，比较死板，不够灵活。

如果把 AI 比作一个人类，我认为这一阶段，他最多算是**一个只会死记硬背的小学生，不懂变通**，一旦遇到超出自己死记硬背以外的其他内容，就一无所知，我们暂且把这一阶段的 AI 称作 **“规则式 AI”**。

**而这也是 AI 进一步后续发展很重要的原因，我们将在下一章节详细展开。**

### **三、今生：AI 成长期（1990-2016）**

在 AI 初生期（1956-1989），AI 基本都以既定规则去应用，这也导致了 AI 成长期（1990-2016）的进一步发展。

#### **1、机器学习出现**

这一阶段，一个很重要的概念出现：**机器学习（Machine Learning 缩写为 ML）。**

那么，什么是机器学习：**让机器从数据中自己学习规律，而不是仅仅依靠人类为它编写固定的指令。**

大家会发现，相比之前的 “规则式 AI”，机器学习的方式会让机器不再死板，不通过人类给定的既定规则，而是通过机器自己学习大量的内容，在这些内容中找到规律，然后再去应用。

#### **2、AI 成长期案例分析**

**我们以一个大家日常工作中场景都在用的例子来去说明：垃圾邮件过滤系统。**

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/j3gficicyOvavSZsbSmdnMumMXZibBPGboKoWGuXMsSib3sbwmsEUeibjtDs7ZtHYyqTUUrQwsTbpFATReHfSBYQc9w/640?wx_fmt=jpeg&from=appmsg#imgIndex=8)

（垃圾邮件示意 - 图片来源于 AI 生成）

**如果在 AI 初生期（1956-1989），**

**只能按照既定规则来，比如：**

● 如果邮件标题里出现 “免费” 这个词，就标记为垃圾邮件

● 如果发件人地址包含 “spam” ，就标记为垃圾邮件

● 等等...

**这种方法的缺点非常明显：**

● 变种很难防范：比如除了 “免费” 外，可能会有 “免 - 费” 或“Free”，这时你的规则就失效了。你必须不停地发现新套路，然后手动添加新规则

● 可能会误杀：比如你的好朋友发了一个标题是 “有个免费的讲座你想参加吗？” 的邮件。

**那么，如果在 AI 成长期（1990-2016），我们可以怎么做？**

**第一步：准备 “学习资料”**

你给机器一大堆已经分好类的邮件：

● 1000 封已知的垃圾邮件 （标为 “垃圾”）

● 1000 封已知的正常邮件 （标为 “正常”）

**第二步：让机器自己 “找规律”**

机器会开始埋头苦读这些邮件，并进行统计分析。

他会自动发现：

● 在 “垃圾邮件” 里，词语 “免费”、“优惠”、“发票” 出现的概率非常高。

● 在 “正常邮件” 里，词语 “会议”、“项目”、“放假”、“通知” 出现的概率非常高。

最终，机器形成了一套自己的判断标准 。

**第三步：实际运作**

这时，一封新邮件来了，标题是 “关于国庆放假的通知” 。

这时，机器会分析这封邮件的内容。

他发现，“放假”、“通知”这些词在他的记忆里，和 “正常邮件” 的关联度非常高。

而 “免费”、“优惠” 这些垃圾邮件高频词一个都没出现。

于是，机器认为：这是一封正常邮件。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/j3gficicyOvavSZsbSmdnMumMXZibBPGboK7R2ia9buXT9iaV183ibZEgG7niaGduQibnj2wQxLYEDRUodazK7WsJ83P5g/640?wx_fmt=jpeg&from=appmsg#imgIndex=9)

（垃圾邮件技术原理 - 图片来源于 AI 生成）

通过这个案例，我们会发现有了机器学习的加持，AI 从 “规则式” 的死板应用加上了“AI 模型分析”，机器会自己学习、自己总结规律了。

#### **3、AI 模型出现**

那么机器通过自己学习，自己总结出的规律，这其实就是 **AI 模型（Model）！**

那么到底什么是 AI 模型：**一个通过大量数据训练出来的、能够识别特定模式或规律的数学函数或程序。**通俗点讲就是**从数据中提炼出的 “规律” 或“经验”本身。**

**AI 模型的三大核心要素：**

● 输入：接收新的数据（如收到一封邮件）

● 处理：运用学到的规律进行计算或判断这个邮件

● 输出：产生结果（判断这个邮件是不是垃圾邮件）

通过上面的案例我们点出了 AI 模型这个基础的概念。

#### **4、机器学习方法：监督学习**

还有一个概念，大家可以发现，我们给了机器 2000 个分类好的邮件（“正常” 或 “垃圾”），让机器根据我们标注好的结果去学习和总结规律，这其实便是**机器学习的方法之一**！

我们把这种方法称为 **“监督学习”，即：给机器学习的训练数据都带有明确的 “标签”（如标注好“垃圾” 还是“正常”）。**

那么还有没有其他机器学习的方法？当然有，这个会在下一章节中和大家逐步介绍。

#### **5、AI 成长期小结**

**如果还是把 AI 比作一个人类，我认为这一阶段，他可以算是一个靠刷题总结规律的中学生了：**比如可能针对中学的生物这门课，刷了大量的题（包含答案），能自己总结出规律和方法，再遇到同样类型题的时候，这位中学生能得心应手的回答上来。

而通过统计大量题或内容然后总结规律，我们可以暂且把这阶段的 AI 称为 **“统计式 AI”**。

**但是，有一个很重要的问题：**

这位中学生刷的是某一学科的题，虽然很厉害，但他**可能偏科**，比如没有去刷物理的题，在遇到物理学科一个他从没做过的题，他可能还是做不出来。

回到本章节我们讲的 AI 成长期（1990-2016）这一阶段，虽然 AI 通过机器学习变得强大了：当我们给到一定的学习数据的时候能自己学习自己总结规律了。

但有一个问题：一旦超出了我们原来给的那些学习数据，AI 就不会了。这也是这阶段 AI 发展面临最大的问题，而这个问题将在接下来的 AI 发展阶段中得到解决，也就是我们接下来要讲的下一章节，而这一章节也是我这篇文章要和大家分享的重中之重。

### **四、今生：AI 爆发期（2017 年至今）**

我们在 AI 成长期（1990-2016）这一阶段发现，AI 虽然出现了机器学习的概念，并且通过机器学习的方式训练出了 AI 模型，但这个 AI 模型太 “偏科” 了，一旦涉及到它训练数据以外领域的知识，他就可能不知道了。那么又如何解决这个问题？

#### **1、AI 模型架构演进**

**我们还是以上一章节中的 “垃圾邮件过滤系统” 来整体回顾和延展分析一下：**

**如果在 AI 初生期（1956-1989）：**

按照既定规则来，比如：如果邮件标题里出现 “免费” 这个词，就标记为垃圾邮件。这种方法非常死板，也没有用到模型的概念。

**如果在 AI 成长期（1990-2016）：**

我们通过**机器学习（监督学习）**训练出了 **AI 模型**（通常用 **“朴素贝叶斯模型架构”**），让模型自己判断收到的邮件是不是垃圾邮件，看起来更高效了。

**但实际上有个缺点：**

他是个 “拆词专家”，他会把邮件拆成一堆零散的词，不会关心词的顺序和句子意思！ 比如“钱转给你” 和“你把钱转走”，对他来说都是一堆含有 “钱”、“转” 的词，意思差不多。他无法理解前者是正常收款，后者可能是诈骗预警。

**这时候 RNN 架构（循环神经网络）出来了：**

他不再拆散邮件，而是尝试逐词阅读整个句子，并努力记住前面读过的内容。

他终于有了初步的 “上下文” 概念，能理解一些简单的句子结构了。

但他有个关键的问题：他有 “健忘症”！如果邮件很长，他读到结尾时，早就忘了开头说了什么。比如，邮件开头说 “关于上次开会的项目报告...”，结尾说 “... 请支付费用”，他可能就忘了开头是正经事，只记得结尾要钱，从而误判。

**因为有 “健忘” 的问题，所以 CNN 架构（卷积神经网络）出来了：**

他每次只关注相邻的几个词。比如，看到 “难以置信的” 和“优惠”时，他能敏锐地感觉到这是个广告短语。看到 “验证您的” 和“账户”时，他知道这可能是个安全提示。通过这种方式可以让他的效率变得更高（可以同时处理多个相邻词），擅长捕捉局部短语特征。通过这种方式，其实可以变相解决 “健忘” 的问题，

**但有一个关键问题，他无法同时看到邮件全文：**

比如，一封邮件可能开头很长一段都是正常的商务沟通，只在最后一句巧妙植入诈骗链接，他可能因为前面都是正常局部信息而放过它。而这也导致了他难以理解邮件整体的逻辑和核心意图。

**我们简单总结一下，以上的 AI 模型通过架构的优化和演进，能力在逐步提升，但他们也有明显的缺陷：**

● “不懂语法”：只看零散关键词。

● “认真但健忘”：处理长文效率低。

● “眼光狭隘”：缺乏全局观。

#### **2、Transformer 架构出现**

而因为有了这些缺陷，**2017 年**，Google 的研究团队发表了一篇名为《Attention Is All You Need》的论文，正式提出了 **Transformer 架构**。

**Transformer 架构**因此诞生！

**我们来看看 Transformer 架构到底是怎么工作的，我们还是以邮件垃圾过滤为例：**

假设有一封可疑邮件，内容是：“尊敬的客户，恭喜您获得 10W 奖金！请点击唯一链接 http://xxx.com 领取”

**第一步：同时查看所有关键信息（并行处理）**

以前的 RNN 架构要一个字一个字读，而 **Transformer 架构**可以一瞬间看到所有词

**第二步：划重点并分析（自注意力机制）**

他会给词与词之间画上 “关联线”，比如 “奖金” 和哪个词关联最强？他发现 “奖金” 和 “链接” 、“领取” 关联非常紧密。这种 “中奖 - 链接 - 领取” 的模式，经典得就像它的办案手册里写的“诈骗三件套”。

**第三步：全局推理，看穿意图**

他看清了整封邮件的逻辑 ：“这是一封群发邮件（尊敬的客户），用虚假的好消息（巨额奖金）作为诱饵，其最终意图是诱导收件人点击一个可疑链接（http://xxx.com）。”

它理解的是邮件的整体意图，而不是机械的匹配关键词。

**第四步：做出最终决定**

他非常有把握地得出结论： “这是一封钓鱼诈骗邮件！” 然后将它扔进垃圾箱。

通过这个例子，我们看到用了 Transformer 架构的垃圾邮件过滤器，通过 **“自注意力机制”** 可以做出精准的判断，那么什么是 “**自注意力机制**”，通俗点理解即是：**模型在处理一句话时，能瞬间看到所有的词，并智能地判断出哪些词之间关系更重要。**

**正是 Transformer 架构的革命性突破，成为了引爆 AI 爆发期最关键的技术基石。**

#### **3、AI 大模型出现**

有了 Transformer 架构，那么 AI 模型就可以得到革命性的改进和优化，基于这个契机，OpenAI 在 **2018 年**推出了生成式模型：**GPT-1**，**GPT-1 拥有 1.17 亿参数**，那这里的 “参数” 是指什么？大家还是否记得本文的第一章节介绍了人类大脑的强大之处在于有数百亿个 “神经元” 构成了 “神经网络”，而 AI 模型这里的“参数” 就类似大脑神经网络里的“神经元”。

在诸如之后快速发展，OpenAI 相继在 2019 年推出了 **GPT-2（参数扩大到 15 亿）、**在 2020 年推出了 **GPT-3（参数规模达到 1750 亿 ）。**

随着参数规模不断扩大的 AI 模型出现，这也正是**为了解决 AI 成长期（1990-2016）AI 模型 “偏科” 的问题**，让 AI 模型具备更通用更强大的知识储备，可以覆盖多个领域。

**基于此，大模型（ Large Model 缩写为 ****LM）**由此而生！

那么，什么叫大模型？基础定义为：**大规模人工智能模型。**

那么这里的 **“大规模”** 具体是什么？即泛指**参数规模巨大**的模型。通常我们把**参数规模在 10 亿以上**的可以算是**入门级的大模型**，但发展到今天（2025），我们通常把**参数规模在 100 亿以上算作大模型，**类如混元大模型旗下的 TurboS 大模型参数量为 5600 亿。

#### **4、大模型、中模型、小模型**

**那么既然有了大模型，是否有小模型中模型？**

**答案是：当然有！我在下表会简要比较一下大模型、中模型和小模型的差异：**

![](https://mmbiz.qpic.cn/sz_mmbiz_png/j3gficicyOvavSZsbSmdnMumMXZibBPGboKFS9NVkyBf3J5XRvicqqGicgA3DdSeBI4eEsia2U3rGCpTsasVGiaLiaxic6w/640?wx_fmt=png&from=appmsg#imgIndex=10)

大家其实发现，中小模型在特定场景也有非常高效，但对于大部分正在读这篇文章的大家而言，无论是工作还是生活场景，大模型的应用是最多的，所以我们还是回到大模型本身继续探讨。

#### **5、大语言模型**

在 AI 初生期（1956-1989），大家发现 AI 和 “自然语言处理（NLP）” 相辅相成发展，那是因为人类在 AI 的探索上最初都是以自然语言处理为切入，我们和机器沟通、让机器理解我们的语言都算是在语言层面最直接的应用，那在大模型出来后，最初的形式就是**大语言模型（ Large Language Model 缩写为 LLM ）**：

● Large（大）：指参数数量巨大，另外还指训练的数据量巨大

● Language（语言）：自然语言。

● Model（模型）： 能识别特定模式或规律的计算模型

而我们之前提到的在 2020 年推出的 GPT-3 可谓是实实在在的大语言模型（拥有 1750 亿参数），再到后来继续演进，OpenAI 在 2023 年正式推出了 GPT-4（参数量相比 GPT-3 更大），而且更强大的是：**GPT-3 只能处理文本**，而 **GPT-4 既可以处理文本也可以处理图像**。类似的大语言模型还有我们鹅厂的 Turbos、Deepseek 等等。

#### **6、除了大语言模型还有哪些模型？**

上面提到了大语言模型的具体定义和介绍，大语言模型作为 AI 大模型最早期也是最核心和基础的形式，在后面的内容我们还会提到文生图、图生视频等大模型，从类别上来看，现在已经远远不止是大语言模型的应用了，整体大模型宇宙我会在下图和大家分享：

![](https://mmbiz.qpic.cn/sz_mmbiz_png/j3gficicyOvavSZsbSmdnMumMXZibBPGboKa1dibC37YfXpDznHKDTGMaU7tL7JmE2oQvpOefPBknP0eCCk4akg26Q/640?wx_fmt=png&from=appmsg#imgIndex=11)

（大模型宇宙）

#### **7、机器学习方法：无监督学习**

大家是否还记得在 AI 成长期（1990-2016），“机器学习” 的概念开始出现，通过机器学习（当时用了监督学习的方法）可以让机器自己学习和总结规律，来得到我们的 AI 模型。

**那么 GPT 之类的 AI 大模型是怎么训练（这阶段通常称为 “预训练”）出来的呢？**

其实，同样也是用了机器学习，但可能会更 “深度”，这里的 “**深度**” 主要指：**在机器学习的过程中分很多步骤，每一步骤学到一些不同的规律，从简单到复杂，逐步深入**。因为大模型的参数量非常大，给到模型训练的数据量也巨大，我们不能再通过只给模型 2000 个数据（标注）的方式让他自己学习和总结规律。GPT 之类的通用大模型基本要吃掉互联网当下存在的所有知识，人工不可能给某一条知识都去做标记告诉模型哪些是对的哪些是错的，只能塞给机器无数的知识，但要让机器自己去总结规律，得出相应的 “正确” 或“错误”（在监督学习下，原来应该是人工标注的工作）。

那么这种机器学习的方法，我们称之为 “**无监督学习**”。

#### **8、深度神经网络、深度机器学习和传统机器学习**

而因为由于大模型巨大的参数量和训练量，需要更复杂的网络结构。之前提到的 RNN、CNN、Transformer 等，都属于 “**深度神经网络”** 的范畴。有了 “**深度神经网络**” 的支撑，我们通常把对大模型预训练时的机器学习范式称为 “**深度机器学习**”，也可以简称为 “**深度学习**”，那么再回到 AI 成长期（1990-2016）当时其实也用了机器学习，我们把这阶段的机器学习范式称之为 “**传统机器学习**”。

#### **9、以 ChatGPT、SD 等案例分析**

OK，聊到这里，其实大模型已经比较完善了，不仅仅是 GPT 系列，还有诸如 Google 的 Gemini 、百度的文心一言、阿里的通义千问以及腾讯的混元等等，但光有这些大模型，好像和我们普通互联网从业者没有啥关系，直到 2023 年 **ChatGPT** 正式问世！

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/j3gficicyOvavSZsbSmdnMumMXZibBPGboK52Tm3ibtsKQLO8QDjmSS0pICIdJ1uWMoxlRCQ2KYLc3zVUj4rZnvLTg/640?wx_fmt=jpeg&from=appmsg#imgIndex=12)

（ChatGPT 聊天界面 - 图片来源于 AI 生成）

大家可以通过 **ChatGPT** 去和 AI 聊天，感受 AI 的渊博和强大，同时又不乏趣味，大家也第一次真正意义在应用层感受到了大模型带来的帮助和作用，大家可以用 **ChatGPT** 去问任何自己感兴趣的未知领域的话题，也可以让他帮我们工作做一定提效。

而这也奠定了近几年 AI 爆发的切入点：近几年大部分基于大模型的 AI 应用基本都以对话形式出现，如 ChatGPT、豆包、元宝等等，很重要的原因在于对话体验是最直接也是最简单的方式。

而大家对话体验聊多了之后，是不是觉得 AI 大模型能力可能就这样了，归根到底也始终只是在对话？

几乎在 **ChatGPT** 出来的同一时期，**Stable Diffusion（缩写为 SD）** 面世，而 SD 是一个**文生图大模型**，已经不再是传统的对话生成文本的体验，SD 已经可以根据输入的文本生成图。（同时期还有 Midjourney，不过前者是开源，后者是闭源）

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/j3gficicyOvavSZsbSmdnMumMXZibBPGboKdDxk7kCDZ4FMgsYbn8dia2dHk1PjH1EprOEwwSobF8PK4Y1giaVyeaIA/640?wx_fmt=jpeg&from=appmsg#imgIndex=13)

（Stable Diffusion 界面 - 图片来源于 AI 生成）

而类似 SD、Midjourney 这些文生图大模型，我们需要输入一段文本来让 AI 生成图，这个文本其实就是**提示词（Prompt）**，我们以一个例子来看看：

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/j3gficicyOvavSZsbSmdnMumMXZibBPGboKhg9nIMMwEbcViaqkEKL0cs9zyH0EicBGqqLNABxTcCrR8Qdh26rvRGKQ/640?wx_fmt=jpeg&from=appmsg#imgIndex=14)

（**Prompt：一只猫在吃饼干** - 图片来源于 AI 生成）

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/j3gficicyOvavSZsbSmdnMumMXZibBPGboKh7YARuaAOOr52bzPABgOrKZl9TiariayuLqZu3Z5yLZM0uf36jHZzWyg/640?wx_fmt=jpeg&from=appmsg#imgIndex=15)

（**Prompt：写实风格，在一个阳光明媚的早晨，一只金渐层猫在草地上，用爪子拿着一块饼干往嘴里吃** - 图片来源于 AI 生成）

#### **10、提示词工程**

**提示词工程**是一门与 AI 有效沟通的链接方式，通过以上例子我们可以稍微总结下整体原则：**你给 AI 的提示词越清晰、越具体，你得到的结果就越好**。掌握这项技能，你将能真正释放大模型的巨大潜力。

通过文生图体验，大家可以发现我们基于 AI 大模型不再是只能生成文本，也能生成图片了！

**但是有一个很关键的问题：**

只能输入文本，我怎么得到一定是我想要的图片呢？比如我希望上面例子里的猫和我家的猫长一样，但光通过提示词（文本描述）很难做到生成的猫和我家的猫一样...

**那么，有没有解决办法？**

**当然有，我们来直接看例子：**

这是我家的猫（一只美短，名叫 “小白”）：

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/j3gficicyOvavSZsbSmdnMumMXZibBPGboKeygr5Aw0WnLkp2WqwxnBgmoicxCjLiaibiaQoMIeUibvE0kGDGvOoFLsEjA/640?wx_fmt=jpeg&from=appmsg#imgIndex=16)

我把 “小白” 这张照片发给了 AI 大模型，同时写了一段提示词，生成出相关图片：

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/j3gficicyOvavSZsbSmdnMumMXZibBPGboKq5cPFMriaR0xy3LfichMjbgurvfE1Lonln6YgiaF9ibics9ESIaZwaDnCQA/640?wx_fmt=jpeg&from=appmsg#imgIndex=17)

（**Prompt：写实风格，在一个阳光明媚的早晨，“小白” 在草地上，用爪子拿着一块饼干往嘴里吃** - 图片来源于 AI 生成）

可以看出，生成的图片更符合预期，但大家有没有发现一点，我们向 AI **既输入了文本又输入了图片**（小白的照片），而 AI 给我们输出了一个最终的图片，这和之前的 ChatGPT 体验可完全不一样啊！（笔者注：其实，最新的 ChatGPT 已经支持既输入文本又输入图片，基于 GPT-4o 大模型）

我们甚至可以既输入图片又输入文字然后来让 AI 生成视频：

[0832d0136be5b18281f3779ba9f01562.mov](https://drive.weixin.qq.com/s?k=AJEAIQdfAAol5K1fEsAEoAoAbdAFw)

#### **11、多模态、单模态**

而这种既能输入文本又能输入图片的方式其实就是 AI 大模型的**多模态（Multimodal）**！

那么到底什么是**多模态**？我们先来回忆一下 AI 模型的三要素：

● 输入：接收数据

● 处理：运用学到的规律进行思考、推理

● 输出：产生结果

我们关注到 AI 模型有很重要的输入、输出两个环节，我们可以把多模态这么定义：**输入或输出端能同时处理、理解和关联多种不同类型信息。**像我们刚刚的例子其实就是大模型在输入端同时接受到了文本和图像的信息，然后去理解、推理，在输出端生成了一张新的图片或一个新的视频。

那其实讲了**多模态**，对应的还有**单模态（Unimodal），**对应多模态的定义，其实我们就比较好理解单模态了，即：**在输入和输出端分别专注于一种类型的信息处理。**像我们之前提到的 GPT-3 GPT-4 以及腾讯的混元 Turbos 等都是单模态大模型，因为他们不论在输入端还是输出端都只能处理文本。我们下面用一张图来简单解释 “单模态” 和“多模态”：

![](https://mmbiz.qpic.cn/sz_mmbiz_png/j3gficicyOvavSZsbSmdnMumMXZibBPGboKsw84icmELywQKdCzq7oRibZFpUUbrlaRWzbAHpvUob6gJRL3FvHBC6bg/640?wx_fmt=png&from=appmsg#imgIndex=18)

（“单模态”和 “多模态” 对比）

#### **12、开源、闭源**

大模型除了 “单模态” 和“多模态”的区分外，还有个很重要的维度区分，即是 “开源” 还是“闭源”，如之前同时期的文生图大模型既有 SD 又有 Midjourney，前者是开源，后者是闭源，那么我们来看看开源和闭源到底有什么区别：

![](https://mmbiz.qpic.cn/sz_mmbiz_png/j3gficicyOvavSZsbSmdnMumMXZibBPGboKuI4uhKCz6Jepfx6j5KU8UgwYrAC9ziaNpZH53DcouE7fvDfZ7TibgqJg/640?wx_fmt=png&from=appmsg#imgIndex=19)

其实 “开源” 或“闭源”并非谁对谁错，也并非谁更好谁更差，都是共同推动了整个 AI 领域的飞速发展。开源是创新的源泉，闭源是商业化应用的标杆。如果大家个人要去尝试到底用什么模型来练手，最重要的是根据自身的需求、技术能力和资源，做出最合适的选择。

#### **13、智能体的出现**

看完大模型整体的介绍，其实我们发现不管是用单模态的 ChatGPT（基于 GPT-4）对话，还是用多模态的 ChatGPT（基于 GPT-4o）去 “文 + 图” 生成图，都已经算是在应用层使用上 AI 大模型了。

**那么我们是否可以用 ChatGPT 去策划一次旅行并做好预算呢：**

**你：**“帮我策划一次三亚旅行”

**ChatGPT：**“好的，为您规划一个三亚 5 日游的行程框架供参考...”

它可能会输出一个非常笼统的模板式行程：

● Day 1: 抵达三亚，入住酒店，附近海滩漫步。

● Day 2：...

**你发现的问题：**这太泛泛而谈了，完全没有考虑你的预算、偏好（比如是否喜欢潜水、是穷游还是舒适游），而且信息是静态的，没有实时价格。

**你：**“这个行程太简单了。我需要一个更详细的计划，包括具体的航班时间、酒店名称和价格参考。我的预算人均是 8000 元。”

**ChatGPT：**“好的，基于人均 8000 元的预算，这是一个更详细的计划示例...”

● “航班：可选择北京 - 三亚的 XX 航空，参考价格 1500 元往返。”

● “酒店：可入住 XX 酒店海景房，参考价格 600 元 / 晚。”

● “...”

**你发现的问题：**价格是过时的：它无法联网获取真实实时价格，这些价格毫无意义。

**需要你验证：**你得自己打开携程或航司官网，去逐个查询这些航班和酒店的真实价格。

**决策点又抛回给你：**“XX 酒店” 真的好吗？它看不到真实用户的评价。

**你：**“现在，帮我查一下下个月从上海出发到三亚，最便宜的非红眼航班是哪天？列出时间和价格。”

**你：**“等等，把预算表单独做出来，分机票、酒店、餐饮、门票、市内交通几项。”

**你：**...

你会发现所有的规划（先查什么、后查什么、如何取舍）都需要你来思考，你不断在发出 “下一步做什么” 的指令。

那么，是否存在一种 AI，你只需要告诉它一个目标，它就能自己规划、执行，直到把结果呈现在你面前？

当然有，这时候就不得不提到 “**智能体（Agent）”** 这个概念了！其实 “智能体” 的概念出现不是这几年才有，在 AI 成长期（1990-2016）前，已经有了基础的定义（来源于学者伍尔德里奇和詹宁斯）：**智能体是一个位于特定环境中的计算机系统，它能够自主的行动，以实现其设计目标。**

![](https://mmbiz.qpic.cn/sz_mmbiz_png/j3gficicyOvavSZsbSmdnMumMXZibBPGboKn2McqXyAic432nM2G0IeBbofSrkiaYNLyXO5QLZZZAsJRVffnvK3BDTA/640?wx_fmt=png&from=appmsg#imgIndex=20)

（“智能体” 演进）

还有一个更好理解的定义是：**能够感知环境、进行决策，并自主采取行动以实现某种目标的系统或程序。**

我们发现 **“智能体”** 的**几个关键因素**是：**"感知"、“决策”、“目标”、“自主行动”**，而大家可以思考一下，之前我们用 ChatGPT 输入一个提示词来生成了一张图，那么这阶段的 ChatGPT 是智能体吗？我们可以来简单分析一下：

![](https://mmbiz.qpic.cn/sz_mmbiz_png/j3gficicyOvavSZsbSmdnMumMXZibBPGboKdNBV0Q9niaia1DPszKFaXgoGttktJXPtV2tSXDxExWvJFYO055v7ckBA/640?wx_fmt=png&from=appmsg#imgIndex=21)

显而易见，如果仅仅是通过 “文生图” 的 ChatGPT 并不算是一个 “智能体”，我们最多算它是“智能体雏形”，或者可以称作是一个“应用” 或“工具”。

**“智能体”的 “自主行动” 通常不太好理解，我们再通过一个生活化的例子来帮助大家更好的理解：**

**场景一：指挥一个 “听话的助手”（这是非自主的）**

你对他说：“小 C，打开冰箱。”

“拿出西红柿和鸡蛋。”

“打开燃气灶。”

“把锅烧热，倒油。”

“先把鸡蛋炒熟盛出来。”

“再炒一下西红柿。”

“最后把鸡蛋倒回去，放盐，翻炒几下出锅。”

你会发现，每一步具体的动作都需要你精确地下达指令。这个助手很能干，但他没有自己的主意，完全依赖你的指挥。这就是 “自动化”，缺乏自主行动。

**场景二：交给一个 “靠谱的私人助理”（这是有“自主行动” 的智能体）**

你对他说：“小王，我有点饿了，帮我做顿饭吃吧。然后你就可以去忙别的事了。

这个 “小王”（智能体）会展现出真正的 “自主行动”：

● 他内心会盘算：“老板饿了。我得先看看冰箱里有什么，然后决定做什么，再动手。”

● 他自己决定打开冰箱看看有什么食材。（感知）

● 他发现有意面、西红柿和牛肉，自己决定做番茄肉酱意面。（决策）

● 他自己决定先后顺序：先烧水、同时切西红柿、再炒肉酱……（规划）

● 发现盐用完了，他不会卡住，而是自己决定用酱油代替。（动态调整）

● 发现意面煮多了，他会自己决定先盛出一部分作为明天的午餐。（灵活处理）

● 最终交付：过了一会，他端上一盘香喷喷的意面，并告诉你：“老板，饭做好了。盐用完了，我用了点酱油调味，你看合口味吗？”

我们可以简要总结下，**“自主行动”** 即是：**“扔给它一个目标，它自己能变出一套计划、搞定过程、应对变化，最终给你结果” 的能力**。

那我们可以试想一下，如果 ChatGPT，同样你给了他一段提示词 “生成一张猫吃饼干的图片”，这时他经过思考：“猫是什么猫比较好？饼干是什么饼干？应该在什么场景更好？”，然后通过各种工具或方法生成了多张猫吃饼干的图片，然后让我们选择，并且选择后可以让他继续优化。**如果是这样的方式，是 “智能体” 吗？**

**答案是：当然是！**完美满足了 “智能体” 的“目标”、“感知”、“决策”、“自主行动”几个关键维度的定义。（请参阅最新豆包的文生图的体验，所以我们可以暂且把豆包当作是一个智能体）

聊了这么多智能体的基础定义，还是为了大家能清晰的了解到什么是智能体。

而我们再看本文之前提了大量关于大模型的基础知识，那么 “大模型” 和“智能体”到底有什么关系呢？

通俗点讲**大模型**就像一个**无所不知、超级博学的大脑**，而**智能体**则是拥有这个大脑后，还**拥有了手和脚**，能通过感官获取到信息进行思考决策后主动去完成一个复杂任务的 **“全能机器人”**。

**简要总结：**

● **大模型是智能体的 “能力基础”**：没有大模型，智能体就不会理解和思考，只能机械执行固定指令。

● **智能体是大模型的 “落地延伸”**：光有大模型只能 “纸上谈兵”，智能体通过搭配工具、设定目标、让大模型的能力从 “说” 变成 “做”。

● **两者是 “分工协作”**：大模型负责 “想清楚”，智能体负责 “做到位”。

#### **14、如何开发一个智能体应用？**

**那么如果我们自己想做一个智能体应用该怎么做呢，以及在过程中是否会遇到一些问题？**

下面我以今年参与的三个 AI 项目来简要分享，分别是：**瓦手 AI 放号官、瓦手 AI 抢 ID、英雄联盟 AI 赛事助手。**

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/j3gficicyOvavSZsbSmdnMumMXZibBPGboKTYXwdFz6r1ib1m1A3U5XxHPFMrvZVfx9SPHyI0ePGVwFVaY5oAVzs9g/640?wx_fmt=jpeg&from=appmsg#imgIndex=22)

而今天我会主要分享在之前文章没有提及的一些内容，**以我负责的三个项目为例，如果要开发一个智能体应用，大概可以是以下流程：**

1. **需求确认及策划 ：**明确项目到底要做什么，想通过 AI 解决什么问题或者提升什么体验

2. **技术选型及架构设计 ：**用什么大模型（是智能体的大脑）？智能体平台 / 框架选择？用什么工具链？

3. **核心开发 ：**核心开发过程

4. **智能体调优及测试 ：**智能体调优的方式？

5. **项目上线运营与迭代：**持续运营及优化迭代

我们将我负责的三个案例按以上流程简要分析：

![](https://mmbiz.qpic.cn/sz_mmbiz_png/j3gficicyOvavSZsbSmdnMumMXZibBPGboKhcwicVVjz1hXdEwjVpVcePicLxrfJmFYt3UAL1ycQXop8UUSVzJH7N9Q/640?wx_fmt=png&from=appmsg#imgIndex=23)

大家会发现整体开发一个智能体项目，最关键的就在于**需求确认及策划**、**技术选型**及**智能体调优**三个环节，而**智能体调优**又是我们三个环节中最重要的环节。

为什么说 “**智能体调优**” 这个环节最重要，是因为我们做一个 AI 智能体应用，AI 效果有没有达到我们的预期，智能体调优有没有做到最好是最关键的，而在上面三个项目的智能体调优方法基本一致，这也说明对于大部分智能体应用调优的方法是具备一定通用性的，这里面的提示词工程在前面已经提到过，我们不再赘述。我们会重点关注在新出现的两个专业术语：“RAG” 和 “微调”，我们在下面慢慢来探讨。

#### **15、检索增强生成（RAG）**

**检索增强生成（Retrieval-Augmented Generation 缩写为 RAG），**拆分一下解释即是：

● **检索：**从外部知识库中查找与问题相关的信息。

● **增强：**用检索到的信息来 “增强” 或“补充”大模型的知识。

● **生成：**大模型基于这些补充的信息，生成更准确、更可靠的答案。

**通俗易懂的一句话解释就是：**智能体的大脑（大模型）进行输出内容之前，先让它主动去一个庞大的知识库（如文档、数据库、互联网）中 “查阅资料”，然后根据查到的资料来组织和生成答案。

**如果一个智能体没有 RAG**，就有点像一个闭卷考试的学生，他只能依靠记忆（预训练阶段学到的知识）来答题。如果问题超出了他的记忆范围，他就可能答错或 “胡编乱造”。

**如果给一个智能体加上 RAG**，就有点像一个开卷考试的学生，当遇到问题时，他可以先去翻阅指定的教科书和笔记（检索外部知识库），然后结合自己的理解（模型的推理能力），写出一个有据可查、内容准确的答案。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/j3gficicyOvavSZsbSmdnMumMXZibBPGboKxWCicVlCsCwaJy4waR7Qibzu69ibagGTLnk2ibycjML4bnNq4uC9Jtzv4w/640?wx_fmt=jpeg&from=appmsg#imgIndex=24)

（瓦手 AI 项目建立的知识库 - 小部分示意）

#### **16、微调：基于监督学习和强化学习**

我们在智能体调优的过程中，提示词工程、RAG 等方式其实都只是改变的是模型的输入阶段，让输入更有效，而想更好的去优化智能体的输出，还要用到：**微调。**提示词、RAG 等改变了输入环节，而微调本质上则改变了 AI 模型（对于开源大模型而言是模型副本，对于闭源大模型而言是 “适配层”）。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/j3gficicyOvavSZsbSmdnMumMXZibBPGboKSTKmic2gurkmS2oTVnicaLvRreIciaFlapXzDOOsia1bLuG1CyAVia1R8ZA/640?wx_fmt=jpeg&from=appmsg#imgIndex=25)

（瓦手 AI 项目基于人工反馈的监督学习）

而**强化学习**则是让智能体通过试错，自己学会一整套 “决策链” 或“策略”，以最大化长期奖励。

我们以一个生活中训练狗狗的例子来告诉大家什么是强化学习：

小狗听到到主人的指令 “坐下”。

它尝试趴下（动作）。

驯兽师没有给零食（没有奖励）。

小狗又尝试坐下（新动作）。

驯兽师立刻给予零食（给了奖励）。

通过无数次尝试，小狗学会了策略：当听到 “坐下” 指令时，执行 “坐下” 动作能最大化获得零食的长期收益。

监督学习是给了模型标准的答案（比如我们告诉模型是 goodcase 还是 badcase），但大家试想一下如果这个数据量特别巨大，每次让人工去标注所有的将非常不现实。而基于人工反馈的强化学习（RLHF）则是更高效的方法，用一个评分奖励模型，我们让 AI 学习这个评分标准，然后通过一次又一次的学习生成高分，从而达到我们想要的效果。

而通过以上的智能体调优方法，目的就是为了让我们做出来的这个 AI 应用更符合我们的预期，比如 AI 回答的更准确、更趣味。

#### **17、大模型的幻觉问题**

但其实，我们在项目过程中有时还是会发现 AI 回答的不是 100% 正确，而这正是大模型的 “幻觉” 问题：**大模型生成看似合理但事实上错误、荒谬或虚构信息的行为，**简单来说就是 AI 在一本正经地胡说八道。

而我们上面用到了一些智能体调优的方法一定程度上本质就是为了解决 “幻觉” 问题，而 “幻觉” 问题产生的原因主要就是我们期待 AI 去输出一些内容，但这些内容又超出了 AI 的认知时，他可能就会乱说或说错。

除了通过 RAG、提示词工程、微调等方法调优智能体，提升我们 AI 输出的准确性外，我们还可以比如：

● 答案溯源：让模型增加二次校验（要求模型在生成答案时，注明引用的源文）

● 自我批判：让模型对自己生成的答案进行一次自我审查

● 高准确性信息采用固定信源：比如我们的 AI 赛事助手的赛程、赛事等信息让 AI 去查固定的接口，而不是走联网搜索

● 等等

简要总结，幻觉是当前大模型的通用问题，包括我们之前谈到的 GPT 系列以及现在所有的通用大模型都会有 “幻觉” 问题，而我们当前所有的调优手段，如 RAG、提示词工程、SFT、RLHF 等，其重要目标之一就是最大限度地管理和减少幻觉，但其实并不能完全消除它。因此，我们除了做好 AI 输入阶段的优化外，对 AI 的输出阶段同样应该保持谨慎，这也是每个 AI 从业者应有的重要意识。

#### **18、AI 爆发期小结**

AI 爆发期，从 2017 年到现在，短短的几年时间，AI 经历了飞速的发展，大模型百花齐放，相关的智能体应用也层出不穷。这阶段（也是我们正在经历的）的 AI，我认为他已经是一个读遍天下书的大学生，拥有了丰富的知识积累加上了一定量的实习经验，下一阶段就是未来，他应该会走上社会，成为一个职场上的专业人员，把多年积累下来的知识和实习期积累的经验更好的应用在职场上，同样的，我们可以把这阶段的 AI 称为 “深度学习 / 大模型 AI”。

### **五、未来**

不知道大家有没有去看 2025 英伟达 GTC 大会，这个大会点出了很多未来和 AI 更有想象空间的模块，如 AGI、具身智能、量子计算、6G、人机协同等，感兴趣的同学可以去详细了解。

而我也聊聊我个人的 AI 观：AI 从历史时间线的发展来看已经有几十年，但真正爆发的也就这几年，这背后原因其实是数据、算力、算法三个模块的逐渐成熟：

● “数据” 即是我们这几十年所有的生活方式、工作方式等都向数字化转变，积累了大量的数据

● “算力” 即是云计算、GPU 等不断迭代和革新，为 AI 提供了有效基础支撑

● “算法” 即是以 Transformer 架构为代表的深度学习给大模型提供了无限可能

而在这三个模块加持下，AI 从最初的自然语言处理发展到了多维度物理世界（图像、视频、音频等等）的处理，在具体应用形态上也从最初的内容生成（AIGC）到辅助办公（编码提效、美术生产提效、产研提效等）再到各个垂直行业（如医疗、教育等）的初步探索，

在这整个发展过程中，

AI 从最初的 “规则式 AI” 发展到 “统计式 AI” 再到今天的“深度学习 / 大模型 AI”，

如果把 AI 想象成一个人类，他也从最初的小学生成长成了现在的大学生，

而大部分互联网从业者可能还只是停留在 AI 的应用阶段上，或者看到 AI 出来立马就想去用，

但很多时候更需要我们思考的是：为什么要用 AI？AI 现在能做什么以及未来能做什么？用了 AI 后可以改变什么？如果不用 AI 会怎么样？

在未来，AI 不再是一个工具，更是我们重要的 “伙伴”。

![](https://mmbiz.qpic.cn/sz_mmbiz_gif/j3gficicyOvasVeMDmWoZ2zyN8iaSc6XWYjZ7Hx6Udjjk2BGLzC9ahJq7ibxDd1RGA0c9NYZc1husEsvb3tY4FcWPQ/640?wx_fmt=gif&from=appmsg#imgIndex=26)

  

  

![](https://mmbiz.qpic.cn/sz_mmbiz_png/j3gficicyOvasVeMDmWoZ2zyN8iaSc6XWYj5q5PQEOc5ibURPb03vnRibrxC3UR8xzdyATfiawTYRV2vJvBnAIcE1FeQ/640?wx_fmt=png&from=appmsg#imgIndex=27)