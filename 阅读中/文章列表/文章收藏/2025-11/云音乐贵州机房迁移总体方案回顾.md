---
source: https://mp.weixin.qq.com/s?__biz=MzI1NTg3NzcwNQ==&mid=2247491821&idx=1&sn=573dcc464a690a5b9a0a991c6f3c74e2&chksm=ea2d97cbdd5a1edd3f3ce882cbbd2b9e3f3c60120f0920df3d6ac2acc18ed84417d0b1e3877d&scene=21#wechat_redirect
create: 2025-11-03 17:55
read: true
knowledge: true
knowledge-date: 2025-11-04
tags:
  - 系统架构
  - 系统运维
summary: "[[跨机房迁移方案]]"
---
![](https://mmbiz.qpic.cn/mmbiz_jpg/KV8By3euEQgLVHTvYBTd2FElqnibSLxXXtLflVfyj3ul8PGSZkQFEzrTTQl4OBTD2jNJyiaGU9YoWicXicwAyFYJ1Q/640?wx_fmt=jpeg&from=appmsg#imgIndex=0)

## 一、背景

2023 年确定要将云音乐整体服务搬迁至贵州机房，项目需要在各种限制条件下，保障 2000 + 应用、100w+QPS 的服务稳定迁移，是云音乐历史上规模最大、人员最多、难度最高的技术项目。在此过程中，解决了大量历史技术债务，同时化解了大量新增系统性风险。以下为总体方案回顾。

## 二、项目难点

*   迁移规模大
    

*   此次需要云音乐以及旗下独立 App 的服务均整体迁移至贵州。涉及 2000 + 应用、100w+QPS 的稳定迁移，同时涉及中间件、存储、机房、三方依赖服务等整体的搬迁，搬迁规模大。
    

*   业务复杂度高
    

*   场景复杂。迁移规模大，带来更广的业务场景覆盖。而不同的场景对数据一致性要求不同、延迟敏感度不同。迁移方案需要考虑各种场景带来的问题，并提供标准化的解决方案。
    
*   服务间依赖复杂。此次带来约 2000 + 应用的搬迁，各服务间的调用和依赖情况复杂，在分批迁移方案中需要协调，以及解决迁移期间跨机房 30msRT 上升带来的问题。
    

*   历史积弊多
    

*   贵州迁移前，存在诸多历史技术积弊，影响着全站整体的稳定性。
    

*   新增风险大
    

*   贵州迁移带来诸多新增风险，且风险大、解决难度高。
    
*   部分场景无法做到真实环境全流程预演。
    
*   在基础技术建设上，也有一些不足的情况，影响整体搬迁执行效率、迁移准确性。
    

*   限制条件严苛
    

*   云音乐有着大量的用户基数，此次搬迁要求：不停机迁移、不产生 P2 及以上事故。除此之外还有机器、网络带宽、网络稳定性、网络 RT、迁移方案等限制条件。
    

*   事项推进 & 协调难度大
    

*   此次搬迁规模大，同样，参与人员规模大，整体协调难度大
    
*   此外带来较多的人因风险。可能因极小的细节未执行到位，就会造成全局事故。
    

## 三、重点限制 & 要求

*   尽可能少采购或不采购额外的机器，贵州和杭州无法完全对等部署。
    
*   杭州与贵州的长传带宽控制在 200Gbps 以内，且存在闪断的可能性，各迁移方案需要重点考虑闪断带来的影响。
    
*   贵州机房与杭州机房之间网络延迟约 30ms，各方迁移方案需重点考虑机房延迟带来的影响。
    
*   业务可用性要求：不影响核心重点业务场景的可用性，不出现 P2 及以上事故。
    
*   控制迁移方案对业务代码的侵入。
    

## 四、分批方案

### 1. 分批的原则

#### 1.1 团队 / 领域间解耦

大团队 / 领域之间的迁移方案尽可能解耦，分不同批次搬迁。好处：

*   可以将问题拆分、领域清晰。
    
*   大数据、算法、云音乐技术中心串行搬迁，可以实现机器资源池共享，降低机器采购成本。
    
*   降低单一团队 / 领域切流时问题处理复杂度。
    

#### 1.2 服务端流量自闭环

云音乐服务端需要将流量闭环在同一个机房，避免产生跨区域调用。

云音乐经过微服务之后，目前存在千 + 服务，各服务间依赖复杂。在贵州机房与杭州机房之间网络延迟约 30ms 的背景下，每产生一次跨区域调用，则 RT 上升 30ms。

#### 1.3 C 端优先

优先迁移 C 端相关的应用及其资源，其次 B 端。

关于此处，会有同学认为优先 B 端可能会更稳，但优先采用 B 端优先，会有如下问题：

*   B 端服务搬迁后，腾挪的机器有限。
    
*   B 端服务与 C 端服务相差较大，即使 B 端服务先行搬迁无问题，也不足以证明 C 端服务就一定没问题。
    

对于如何保障 C 端服务搬迁的稳定性，在文章后续章节展开。

#### 1.4 在可用资源范围内

迁移期间，需要在贵州准备与杭州同等规模的机器资源，因此批次不可能不受到资源的限制。其主要受限制资源为：

*   机器资源
    
*   贵州 & 杭州的长传带宽资源
    

因此，按照以上原则进行分批后，若资源仍不足，再根据团队 / 领域拆分出第二批

### 2. 最终分批方案

基于以上原则，最终分批方案如下所示

![](https://mmbiz.qpic.cn/mmbiz_png/KV8By3euEQgLVHTvYBTd2FElqnibSLxXXtdIJ2DribGRF6YsDyMOia5lKGViceK0JTLUKtH8ZlyYhQSic4HepXp8lKg/640?wx_fmt=png&from=appmsg#imgIndex=1)

*   大数据、算法、技术中心串行搬迁。
    
*   心遇因强依赖云信 IM 服务，与云信服务独立搬迁
    
*   技术中心应用基本一批次全部搬迁完成。
    
*   技术中心的转码、公技侧后台、质量侧系统在第二批次搬迁完成。
    

## 五、切流方案

### 1. 切流的原则

#### 1.1 可灰度

能够按照用户 ID、设备 ID、IP、流量标几个维度逐步灰度切流。

*   利于预热。在服务启动后，缓存、连接池需要随请求逐步预热，若流量直接全部打过来，可能会将服务打垮。
    
*   利于测试。能够灰度测试整体功能，避免大面积异常。
    

#### 1.2 可回滚

尽管做了各种稳定性保障来避免回滚，但是如遇到极端情况，仍有整体回滚的可能性。因此搬迁方案必须可回滚。

#### 1.3 控制长传带宽

在切流过程中，杭州和贵州之间会有大量的服务访问、数据传输，从而可能突破长传带宽 200Gbps 的限制。因此切流方案中必须减少不必要的跨区域流量。

### 2. 切流方案

#### 2.1 切流点选择

![](https://mmbiz.qpic.cn/mmbiz_png/KV8By3euEQgLVHTvYBTd2FElqnibSLxXXIaF7rVzXSiaiaQ1md1TVRQjh0gPcCVL3FmP0icXbOamVqFWOZ1GCXTC7g/640?wx_fmt=png&from=appmsg#imgIndex=2)

贵州机房迁移切流点选择. png

服务端整体通用架构简化后，如上图所示，因此有如下几个切入点：

*   客户端切流。客户端通过动态切换域名配置，可实现流量的切换。切流算法可以与网关使用保持一致，我们在贵州迁移中就采用了此方案，从而大幅降低贵州与杭州的长传带宽。
    
*   DNS 切换。因 DNS 存在缓存过期，不适合作为流量控制的主要手段。在贵州迁移中，我们主要用其作为长尾流量的切换的手段。
    
*   四层 LB 切流、Nginx 切流。主要由 SA 侧负责，因自动化和操作复杂度等因素，在贵州迁移中，四层 LB 切流只用于辅助切流手段，Nginx 因过高的人工操作复杂度，不用于切流。
    
*   网关切流。网关作为服务端广泛接触的首要流量入口，其系统建设相对完善、自动化程度较高，因此作为主要切流手段。在此次迁移中，网关支持按用户 ID、设备 ID、IP 进行按比例切流。
    
*   定时任务、MQ 切换。主要用于定时任务、MQ 的流量切换。
    
*   RPC 流量控制。RPC 流量路由策略与网关保持一致，依据切流比例，进行 RPC 流量调用。从而避免跨机房 RT 的不可控。
    
*   存储层切换。主要负责存储的切换。
    

#### 2.2 存储层迁移策略

云音乐业务场景较多，不同场景下对数据一致性的要求也不一样，例如：营收下的订单类场景需要数据强一致性，而点赞需要数据最终一致性即可。

在涉及不同的存储时，也有着多种多样的迁移策略。对此，中间件以及各存储层支持了不同的迁移策略选择，各个业务基于不同的场景，选择正确的策略。迁移策略主要如下：

<table><thead><tr><th>类型</th><th>迁移策略</th></tr></thead><tbody><tr><td>DB</td><td>读本地写远程、读远程写远程、读本地写本地、禁写</td></tr><tr><td>Redis</td><td>读写远程 + 需要禁写、读本地写远程 + 需要禁写、读写本地</td></tr><tr><td>Memcached</td><td>异步双写、同步双写、不同步</td></tr></tbody></table>

#### 2.3 切流步骤

对以上切入点再次进行分类，可再次简化为流量层切流、存储层切换。在正式切流时，我们按照如下步骤进行切流。

![](https://mmbiz.qpic.cn/mmbiz_png/KV8By3euEQgLVHTvYBTd2FElqnibSLxXXqC4ur8QApia8p286nTicIMXRGA0PMllMEnn9a0w2WRtTaO8OWkK7FouQ/640?wx_fmt=png&from=appmsg#imgIndex=3)

### 3. 回滚方案

先存储层按序切换，然后流量层按序切换。

![](https://mmbiz.qpic.cn/mmbiz_png/KV8By3euEQgLVHTvYBTd2FElqnibSLxXXtic0ibgpltRDbjzChU8SItLeB0aY7mUicwSYqYkSgxkxrtJ60a3ZhaQTw/640?wx_fmt=png&from=appmsg#imgIndex=4)

## 六、稳定性保障 & 治理

### 1. 全域的稳定性风险

*   全域的稳定性风险。我们在做一般的活动稳定性保障时，一般从活动的主链路出发，再梳理相关依赖，从而整理出稳定性保障 & 治理的重点。而这种方法确不适用于贵州机房迁移，从前面的分批概览图可得知：此次贵州机房迁移带来全域的稳定性风险。
    
*   墨菲定律："如果一件事情有出错的可能性, 那么它最终一定会出错。"
    
*   业界没有类似的经验可参考
    

因此整个项目组也在摸着石头过河，在此过程中，既有大的方案的设计，也有细枝末节的问题发现和推进处理。总结起来，我们总共从以下几个方面着手进行稳定性保障：

*   信息梳理 & 摸查
    
*   新增风险发现 & 处理
    
*   历史技术债务处理
    
*   标准化接入
    
*   监控告警增强
    
*   应急预案保障
    
*   业务侧技术方案保障
    
*   杭州集群下线保障
    

### 2. 信息梳理 & 摸查

盘点梳理机器资源情况、网络带宽、迁移期间服务可用性要求等全局限制条件，从而确定分批方案、迁移思路。

#### 2.1 机器资源盘点

主要盘点核数、内存。在此过程中，也推进了资源利用率优化、废弃服务下线等事宜。通过如下公式计算机器资源缺口：_搬迁机器缺口 = 搬迁所需数量 -（可用数量 + 可优化数量）_

#### 2.2 长传带宽盘点

需要控制云音乐的长传带宽总量 <= 相对安全的带宽量_相对安全的带宽量 = (长传带宽总量 / 2 x 0.8) - 已被占用带宽量_

#### 2.3 迁移期间服务可用性要求

若业务允许全站停服迁移、或仅保障少量核心服务不挂，那么整体迁移方案会简单很多。因此业务对迁移期间的可用性要求，关乎着搬迁方案如何设计。最终讨论后确定，需要：迁移不产生 P2 及以上事故

#### 2.4 服务间跨区域调用 RT 摸查

基于 Trace 链路，预测分批情况下 RT 增长情况。

### 3. 新增系统性风险

此次贵州迁移主要带来的新增系统性风险是：

*   因公网质量问题，带来迁移后用户体验差的风险。
    
*   因跨机房延迟 30ms ，带来的业务侧应用雪崩风险。
    
*   因跨机房传输网络不稳定，带来的整体系统性风险。
    
*   因杭州和贵州机房同时部署，带来的服务节点数量、API 数量、RPC 数量翻倍风险
    
*   因大规模数据变更，带来的系统性能风险。
    
*   因新机房建设、搬迁，带来的底层基础设施风险。
    
*   因全域团队协作、大范围配置变更 & 发布，带来的人因操作、协作风险。
    

#### 3.1 因公网质量问题，带来迁移后用户体验差的风险

贵州公网质量如何？迁移至贵州之后是否会因公网质量问题，导致用户体验差？由于云音乐用户基数大，且注重用户体验，这个是必须提前摸清的问题。若公网质量真的存在较大问题，云音乐可能会停止贵州迁移项目。

对此，我们通过如下方式进行了公网质量验证和保障：

1.  通过客户端预埋逻辑，抽样检测同时请求杭州和贵州机房的 RT 差异。
    
2.  通过 RT 的差异，再下钻分析杭州和贵州机房的差异点。
    
3.  解决或排除机房、客户端、域名配置等差异，最终得出公网质量的差异。
    
4.  在正式切流前，解决完成客户端、机房等差异，保障整体网络请求质量。
    
5.  通过 QA 侧的整体测试。
    

#### 3.2 因跨机房延迟 30ms ，带来的业务侧应用雪崩风险

云音乐 C 端服务当前的 RT 普遍在 5~70ms 之间，若增加 30ms，可能会导致请求堆积、线程池打爆等风险。为避免此风险，我们从如下几个方面入手：

*   尽可能同一批次搬迁，避免长期跨机房调用。
    
*   同一批次应用，基于用户 ID、设备 ID、IP 进行 Hash，实现同机房调用优先。
    
*   无法同一批次搬迁的应用。
    

*   确保会只跨一次，避免因循环调用等原因导致的多次跨机房。
    
*   需提供降级方案，对服务弱依赖。
    

*   服务需通过 QA 侧的测试。
    

#### 3.3 因跨机房传输网络不稳定，带来的整体系统性风险

跨机房网络的现状和参考数据：

*   共计 2 条线，单条带宽为：100Gbps，但建议保持单条利用率在 80% 及以下。
    
*   参考网易北京与杭州的长传带宽质量。
    

*   可能会出现单条中断的情况，在网络侧的表现为网络抖动。若单条线中断，那么发生故障的请求会重连至另一条线。
    
*   极低概率出现 2 条线全部中断的情况。
    

基于以上现状，需要重点考虑并解决：

*   各中间件、存储在切流期间，长传网络出现问题时的表现、应对和兜底措施。例如 ZK 重连、重连失败后的重连风暴问题。
    
*   各服务在切流完成后，若仍长期使用长传网络，若长传网络出现问题的表现、应对和兜底措施。
    

在贵州迁移项目中，我们对以上重点问题进行了梳理和解决，并制定了各种应急预案和极端情况下的回滚方案。

#### 3.4 因杭州和贵州机房同时部署，带来的服务节点数量、API 数量、RPC 数量翻倍风险

因杭州和贵州机房同时部署，带来的服务节点数量、API 数量、RPC 数量翻倍风险

在服务节点数量、API 数量、RPC 数量翻倍后，主要对底层依赖带来连接、重连上的冲击，以及原有连接数上限的冲击。

在我们实际搬迁中，也因遗漏了这一点，导致线上 ZK 出现瓶颈，进而 ZK 挂掉的问题。其主要表现为在网关场景下存在数据推送瓶颈。最终通过网关侧的 ZK 拆分解决该问题。

除此之外，DB、Memcached、Redis、MQ 等资源的连接数也可能会超过原先设定的上限，需要评估后进行调整。

#### 3.5 因大规模数据变更，带来的系统性能风险

大规模数据变更的场景包含但不限于：

*   批量调整配置中心值，因达到配置中心的性能瓶颈，导致配置变更时间过长，或服务挂掉。
    
*   批量的服务部署、重启，因达到 K8S、构建机的性能瓶颈，导致部署、重启时间过长，或服务挂掉。
    
*   对迁移当晚核心路径上的服务进行集中访问、操作，因达到服务的性能瓶颈，导致访问超时、白屏、数据延迟、或服务挂掉的问题。
    

针对以上风险，我们重点对配置中心、K8S、贵州迁移管控平台等系统进行了性能优化，以支撑整体迁移。

#### 3.6 因新机房建设、搬迁带来的底层基础设施风险。

因新机房建设、搬迁带来的底层基础设施风险包含但不限于：

*   同城双活能力的缺失。为应对此风险，我们在逻辑上继续保留同城双活的能力，并暂时通过机房不同楼层的部署架构，来尽可能弥补同城双活能力的缺失。
    
*   机器上架、环境搭建、网络传输等需确保达到验收标准。为应对此风险，运维侧提供相关方案保障整体环境，并最终通过业务侧 QA 验收。
    

#### 3.7 因全域团队协作、大范围变更 & 发布，带来的人因操作、协作风险

在贵州迁移前，已经有多次发生因配置变更错误带来的事故。而此项目带来从未有过的全域迁移，全域协作，大范围变更 & 发布，风险不可谓不高。在此过程中，通过了许多方式来保障事项的落地，其中比较关键的点，也是项目成功的关键点包括：

*   各部门领导与同事的支持。
    
*   分工明确。在战略、战术、细节、事项推进等多个点均有相关人员把控，各司其职。
    
*   各项信息的细化梳理 & 定位。
    
*   定期的沟通协作会议，通过敏捷式项目管理，进行滚动式问题发现。
    
*   问题发现、治理、验证必须闭环。
    
*   尽可能中心系统化、自动化处理。无法自动化的，则提供标准化实施手册。
    
*   重点问题，case by case，one by one。
    

### 4. 历史技术债务处理

在贵州迁移项目中，比较突出的历史债务处理有：

*   ZK 强依赖问题
    
*   在线业务 Kafka 迁移 Nydus。
    
*   配置硬编码
    
*   服务间依赖改造
    
*   资源优化 & 控制
    
*   心遇依赖拆分
    
*   元信息不准确
    
*   组件版本过于陈旧问题
    
*   测试环境自动化部署成功率低
    
*   租户多集群拆分为多应用
    

#### 4.1 ZK 强依赖问题

ZK 的不稳定已导致云音乐最高出现 P1 级事故，在贵州迁移项目中，因网络环境、机房环境、迁移复杂度等因素，ZK 服务挂掉的概率极大，因此必须不能对其强依赖。

最终中间件侧对其改造，支持 ZK 发生故障时，其注册信息降级到本地内存读取。并推进相关依赖方进行升级改造。

#### 4.2 在线业务 Kafka 迁移 Nydus。

Nydus 作为云音乐主力 MQ 产品，相较开源 Kafka 有更好的监控、运维等能力，Kafka 在云音乐在线业务中已不再推荐使用。在贵州迁移中，MQ 也需要进行两地切换 / 切流。

主要收益：

*   在线业务稳定性
    
*   Kafka 机器资源回收
    
*   MQ 切流特性 & 历史债务收敛
    

在推进层面：

*   第一里程碑：生产者完成双写
    
*   第二里程碑：消费者完成双消费
    
*   第三里程碑：完成废弃 TOPIC 下线、代码下线等收尾工作
    

#### 4.3 配置硬编码

在贵州迁移项目中，需要做大量的配置迁移、变更。其主要为：机房名、集群名、机器 IP、机器 Ingress 域名的变化。而这些在配置中心、代码、自动化脚本、JVM 参数中均有存在，此外，IP 黑白名单还可能涉及到外部厂商的改造变更。

在具体推进上，采用自动化扫描 + 人工梳理结合，并辅以标准化改造指引文档。

*   自动化扫描：通过代码扫描、配置中心扫描、JVM 参数扫描、连接扫描等方式进行问题发现。
    
*   人工梳理：外部厂商、不受 Git 管控的脚本、以及运维侧的配置（例如：存储层访问权限的黑白名单等）、以及自动化扫描可能的遗漏，由各研发、运维人员再次自行梳理。
    

#### 4.4 服务间依赖改造

核心应对杭州与贵州跨机房 30ms RT 和长传网络不稳定的风险。对循环调用、不合理依赖、强依赖进行改造。

*   减少不必要依赖。
    
*   必须不能出现服务跨机房强依赖。
    
*   不能因循环调用导致跨机房 RT 飙升。
    

#### 4.5 资源优化 & 控制

因贵州需要与杭州同等容量部署，可能存在资源不足的情况。对此需要：

*   统一服务的资源利用率标准，推进资源利用率改造
    
*   对部分服务进行合并、下线、缩容处理。
    

#### 4.6 心遇依赖拆分

因心遇强依赖云信，且云信 IM 为心遇核心业务功能，最终确定心遇为独立批次搬迁。因此心遇依赖的中台服务、存储、算法 & 大数据相关任务，均需拆分出来，不能与云音乐耦合，否则会产生跨机房调用，影响服务稳定性。

#### 4.7 元信息不准确

在此次迁移中，存在较多的元信息不准确的问题，例如：

<table><thead><tr><th>不足项</th><th>解释</th></tr></thead><tbody><tr><td>应用的元信息需要补充、更新</td><td>1. 应用归属的团队信息不准确<br>2. 应用的废弃、待废弃状态未知<br>3. 测试应用、非业务应用信息偏杂乱</td></tr><tr><td>应用团队归属信息多处维护，未统一</td><td>应用在多个平台均有维护，且均存在维护不准确的问题</td></tr><tr><td>应用的各项依赖信息不全</td><td>应用依赖的 db、redis、memcached 资源，以及在配置中心的 key 无法全面准确拉取</td></tr><tr><td>应用的各项依赖信息可视化、系统化建设不足</td><td>1. 应用依赖的组件版本、依赖的存储资源等，缺乏友好的可视化查询能力。<br>2. 各项信息之间的关联性建设不足</td></tr><tr><td>底层中间件、存储元信息不全</td><td>1. 不同的 ZK 集群的用处缺乏统一维护。<br>2. 各项元信息反查调用源 IP、集群、应用、团队、负责人的能力不足</td></tr></tbody></table>

以上问题在迁移中，通过脚本、1 对 1 沟通确认、手动梳理等多种方式进行了临时处理，在贵州迁移后，仍需再全面的系统性规划。

#### 4.8 组件版本过于陈旧问题

有较多的应用长期不升级，与最新版本跨度较大，存在较多的兼容性问题，需要人工进行升级处理。升级流程大致如下：

![](https://mmbiz.qpic.cn/mmbiz_png/KV8By3euEQgLVHTvYBTd2FElqnibSLxXXDKUytZib9S9onA9EcP9e67MsVbNTTUZEfXh80xSZPaiaswYyhQWuMQfA/640?wx_fmt=png&from=appmsg#imgIndex=5)

在迁移中期，我们进行了自动升级平台建设，基本支持以上升级流程自动化。

#### 4.9 测试环境自动部署成功率低

因此次迁移涉及全部的应用在不同环境的部署，全部人工操作的效率过低，因此我们在非线上环境均由脚本自动化部署，而测试环境由于维护不足，部署成功率较低。

#### 4.10 租户多集群拆分为多应用

当前贵州迁移时整体会按照应用维度进行迁移、切流到贵州。因此对于中台租户型应用、多地域注册类型的应用需要拆分。

### 5. 标准化接入

除了以上提到的历史技术债务处理和新增系统性风险，公共技术侧大都提供了标准化的接入、改造治理方式。例如：

*   贵州迁移中间件方案汇总。涵盖所有涉及中间件的迁移、切流、迁移策略、接入等指导方案。
    
*   贵州迁移升级指导。涵盖自动升级与手动升级、脚手架应用与非脚手架应用的升级方案。
    
*   贵州迁移线上部署指导。涵盖贵州线上部署前的各项必要准备事项，以及特殊应用的注意事项。
    
*   贵州迁移监控大盘观测指导。涵盖各类迁移监控的观测指导。
    
*   中台、多地域注册拆分指导。涵盖中台租户、多地域注册类型应用的拆分指导方案，以及整体的拆分流程、验证要点等。
    
*   ddb、redis、memcached、KSchedule 等非标治理。涵盖各中间件、存储的非标风险列表、处理办法等。
    
*   杭州集群下线指导。涵盖杭州集群如何观察、缩容、下线、机器回收的指导方案。
    

### 6. 监控告警

在监控告警层面，主要提供了：

*   贵州迁移整体大盘监控。提供了迁移相关全局比例，异常流量，异常比例，能够区分是迁移导致的还是本身杭州服务就有问题导致。同时集成资源层相关指标，判断是单个资源有问题还是全部资源有问题。
    
*   贵州迁移应用监控。提供了单个应用的贵州迁移监控，应用贵州杭州流量比例，异常流量，异常比例，能够区分是贵州还是杭州的问题。同时有资源相关的指标。
    
*   杭州集群与贵州集群的哨兵监控对比分析。提供指定应用的杭州和贵州集群在 CPU 利用率、线程池满、异常比例、RT 超时等维度的对比。
    
*   全局 / 应用的 SLO 监控。提供核心指标受损监控。
    
*   应用层面的系统监控。研发可通过哨兵、APM 来查看定位具体的问题。
    

### 7. 应急预案

在贵州迁移期间，基于以上风险，主要准备如下应急预案：

*   客户端截流。在开启后，客户端将访问本地或 CDN 缓存，不再向服务端发送请求。
    
*   全站服务 QPS 限流至安全阈值。在开启后，全站的后端服务将限流调整至较低的安全阈值上，在极端情况下，避免因跨机房 RT、跨机房传输、跨机房访问等因素的性能瓶颈引起服务端雪崩。
    
*   长传带宽监控 & 限流。在开启后，部分离线数据传输任务将会被限流。保障在线业务的带宽在安全水位下。
    
*   回滚方案。当出现重大问题，且无法快速解决时，逐步将存储、流量切回杭州。
    
*   外网逃生通道。当出现长传网络完全中断，需要回滚至杭州。通过外网逃生通道实现配置、核心数据的回滚。
    
*   业务领域内的应急预案。各业务领域内，需要考虑切流前的主动降级预案、切流中的应急预案。
    
*   批量重启。当出现局部服务必须通过重启才能解决的问题时，将会启用批量重启脚本实现快速重启。当出现全局服务必须通过重启才能解决问题时，需要当场评估问题从而选择全量重启或全量回滚至杭州。
    

### 8. 业务技术侧方案

业务技术侧方案重点包含但不限于：

*   应用搬迁范围、搬迁批次梳理明确。当上下游依赖的应用处于不同批次时，需要跨团队沟通协调。
    
*   明确业务影响，从而确定各应用的中间件、存储迁移策略。
    
*   历史技术债务处理
    
*   标准化接入
    
*   核心场景稳定性保障方案
    
*   核心指标监控建设完善。
    
*   切流 SOP。包括切流前（前 2 天、前 1 天、前 5 分钟）、切流中、切流后各阶段的执行事项。
    
*   切流降级方案、应急预案
    
*   切流停止标准
    

### 9. 杭州集群下线

在服务迁移至贵州后，若杭州仍有流量调用，需排查流量来源，并推进流量下线或转移至贵州。先缩容观察，无正常流量、CDN 回源等之后，再做集群下线。

## 七、测试 & 演练

此次贵州迁移，在各应用标准化治理之后，通过系统批量工具完成贵州各项环境的搭建、测试环境的批量部署。

### 1. 测试环境演练

#### 1.1 准备事项

在测试演练开始前，我们重点做了如下准备：

*   贵州测试环境批量创建。通过迁移工具，实现贵州测试集群的批量创建、配置批量迁移等。
    
*   应用自动化升级。通过自动升级平台，实现大规模应用的批量升级，支持了各组件、各应用的多次快速验证、快速升级。
    
*   测试环境自动化部署。通过自动化部署脚本，为支持测试环境能够多次、高效演练。
    
*   SOP 梳理 & 平台建设。通过 SOP 平台，将 SOP 文档沉淀为系统能力，实现各 SOP 能力的系统化。
    
*   迁移监控大盘建设。通过细化梳理监控指标，构建监控大盘，掌握各应用、各组件在切流期间的表现。
    

#### 1.2 执行步骤

![](https://mmbiz.qpic.cn/mmbiz_png/KV8By3euEQgLVHTvYBTd2FElqnibSLxXXUYs6GknMVBW4GqOoicuoK4a90uhjPY6A1AIVp95ib7IJrdQUSEH7W30Q/640?wx_fmt=png&from=appmsg#imgIndex=6)

在测试环境演练，总体思路是逐步扩大验证范围，最终达到全局基本功能基本验证通过。以下为主要演练顺序，每一步视执行结果，再选择是否重复执行。

<table><thead><tr><th>顺序</th><th>验证事项</th></tr></thead><tbody><tr><td>1</td><td>验证中间件内部逻辑是否正确：<br>1. 网关、RPC、存储层路由策略是否正确。<br>2. 验证监控大盘是否正确<br>3. 验证 SOP 平台是否正确<br>4....</td></tr><tr><td>2</td><td>验证存储层切换是否正确</td></tr><tr><td>3</td><td>逐一对各业务团队进行演练：<br>1. 加深各团队对切流能力的感知。<br>2. 验证收集中间件、存储在各领域的表现。<br>3. 验证各团队、各领域迁移策略的合理性</td></tr><tr><td>4</td><td>对 BFF、FaaS 等特殊应用类型进行演练</td></tr></tbody></table>

### 2. 线上环境演练

因测试环境和线上环境仍存在较大的差异，需要摸清线上真实情况，在演练原则和演练目标上均较测试环境演练有更严格、细致的要求。

#### 2.1 演练原则

*   不对线上数据产生污染；
    
*   不产生线上 P2 以上事故。
    

#### 2.2 演练目标

<table><thead><tr><th>分类</th><th>目标内容</th></tr></thead><tbody><tr><td>公技演练目标</td><td>1. 切流验证，网关，rpc，贵州迁移大盘监控<br>2. 网关切流比例、快慢，数据库 ddb 贵州跨机房建连对业务影响<br>3. 端上切流，网关切流验证</td></tr><tr><td>业务演练目标</td><td>1. 流量切换，贵州跨机房对业务影响<br>2. 业务指标和 SLO<br>3. 业务预案有效性验证<br>4.RT 变化情况</td></tr><tr><td>存储演练目标</td><td>1.ddb 复制延迟，连接数 (由于跨机房创建 DDB 连接非常慢, 主要观察流量到贵州后新建连接对应用和数据库影响及恢复情况)<br>2.redis 数据同步、整体表现</td></tr><tr><td>网络演练目标</td><td>1. 跨机房延迟情况<br>2. 跨机房带宽实际占用<br>3. 网络带宽占用监控</td></tr></tbody></table>

#### 2.3 演练终止条件

*   P0、P1 核心场景 SLO 95% 以下；
    
*   用户舆情增长波动明显；
    
*   跨机房网络大规模异常；
    
*   大量业务指标或者数据异常；
    
*   贵州流量达到预定 90%。
    

### 3. 独立 App 迁移验证

在云音乐主站正式切流前，先对云音乐旗下独立 App 进行了线上搬迁验证，保障云音乐迁移时的稳定性。

## 八、系统沉淀

### 1. SOP 平台

SOP 即标准作业程序（Standard Operating Procedure），源自传统工业领域，强调将某项操作以标准化、流程化的方式固化下来。

SOP 平台将标准化、流程化的操作进行系统化呈现，并对接各中间件平台，实现操作效率的提升。在贵州迁移过程中，能够实现多部门信息同步、信息检查，并显著降低批量操作的出错概率、执行效率，降低人因风险。同时也可为后续其他大型项目提供基础支撑。

### 2. 自动升级平台

自动升级平台串联代码升级变更、测试部署、测试验证、线上发布、线上检测，实现升级生命周期重要节点的自动化。在贵州迁移过程中，显著提升整体升级、验证、部署效率。同时可为后续的大规模组件升级、组件风险治理、组件兼容性摸查、Sidecar 式升级提供基础支撑。

## 九、不足反思

### 1. 元信息建设仍然不足

精准筛选出每项事宜涉及的范围，是顺利进行各项风险治理的前提条件。在此次贵州机房迁移中也暴露出元信息建设不足的问题。

<table><thead><tr><th>不足项</th><th>解释</th></tr></thead><tbody><tr><td>应用的元信息需要补充、更新</td><td>1. 应用归属的团队信息不准确<br>2. 应用的废弃、待废弃状态未知<br>3. 测试应用、非业务应用信息偏杂乱</td></tr><tr><td>应用团队归属信息多处维护，未统一</td><td>应用在多个平台均有维护，且均存在维护不准确的问题</td></tr><tr><td>应用的各项依赖信息不全</td><td>应用依赖的 db、redis、memcached 资源，以及在配置中心的 key 无法全面准确拉取</td></tr><tr><td>应用的各项依赖信息可视化、系统化建设不足</td><td>1. 应用依赖的组件版本、依赖的存储资源等，缺乏友好的可视化查询能力。<br>2. 各项信息之间的关联性建设不足</td></tr><tr><td>底层中间件、存储元信息不全</td><td>1. 不同的 ZK 集群的用处缺乏统一维护。<br>2. 各项元信息反查调用源 IP、集群、应用、团队、负责人的能力不足</td></tr></tbody></table>

### 2. 各项元信息的创建、更新、销毁标准化、系统化

在贵州迁移过程中，做了历史技术债务处理、标准化接入方式，后续可针对各项元信息的创建、更新、销毁进行标准化、系统化建设。例如：

*   应用、集群的创建和销毁需要前置校验、审批。以及后期的架构治理扫描。
    
*   借助组件升级平台，实现组件发布、升级的标准化、系统化。
    
*   DB、Redis、Memcached、ZK 的申请、使用、接入等标准化、防劣化。
    

### 3. 应用配置标准化

目前应用可做配置的入口有：配置中心、properties 文件、props 文件、JVM 参数、硬编码。不同的中间件提供出的配置方式也各有不同，所以各应用的配置比较五花八门。因此可做如下改进：

*   明确各种配置入口的使用标准。比如：什么时候建议用配置中心？什么时候建议用 JVM 参数？
    
*   在组件提供侧、应用研发侧均有一定的宣贯、提示。避免配置方式过于杂乱。
    
*   提供配置统一上报的能力。助力元信息的建设。
    

### 4. 批处理能力需再进一步增强

在贵州机房迁移中，除了 SOP 平台和自动升级平台的系统沉淀外，业务中间件、Horizon 部署平台都提供了一定的工具支撑，从而在一定程度上提升了整体迁移的效率。在之后，随着对效率、系统间融合的要求的提高。需要继续在功能、性能、稳定性等多个层面，继续对批处理、系统间融合进行系统化建设。例如：

*   批量拉取、筛选指定条件的应用以及相关依赖信息。
    
*   基于指定的环境、团队、应用、集群等维度，进行服务的批量重启、部署。此处需要进一步提升测试环境部署成功率
    
*   基于指定的应用、集群等维度，进行批量的服务复制、配置复制。
    

### 5. ZK 稳定性、可维护性优化

在贵州迁移中，ZK 的问题相对突出，对此也投入了比较多的人力去排查、解决以及推进风险治理。后续仍需要在 ZK 的稳定性、可维护性上探讨进一步优化的可能性：

*   ZK 元信息的维护和使用标准。明确各 ZK 集群的用处、各 ZK Path 的用处，ZK 集群间隔离、复用的标准，并推进相关标准化治理。
    
*   ZK 故障时，因开启降级至内存，业务无法重启服务。若故障期间叠加其他事故，则会导致其他事故被放大。
    
*   其他稳定性、可维护性梳理
    

### 6. 公技侧稳定性保障长效机制和系统化建设

尽管在贵州机房迁移中，做了大量的稳定性保障措施，但依赖每个研发对各自负责领域的理解、运维能力。是否能在团队管理、设施管理、服务管理、稳定性管理、架构设计等多方面，探索出一套可持续的长效保障机制？并进行一定的稳定性系统化建设？从而避免点状问题随机发生。

### 7. 组件生产、发布、治理能力增强

贵州迁移中涉及大量的组件变更与发布，以及业务侧组件升级与治理。组件可以从生产侧和使用侧进行分析，而组件生命周期主要由 2 条主线贯穿：

*   组件生产发布线：组件的生产、测试验证、发布。
    
*   组件风险治理线：风险定义、风险发现、升级推进、升级验证
    
    ![](https://mmbiz.qpic.cn/mmbiz_png/KV8By3euEQgLVHTvYBTd2FElqnibSLxXXecR3HueguqjXmUp3XLjvxh1ZEPLldibCaA43CGib9GKaPAQg6nEXPeDw/640?wx_fmt=png&from=appmsg#imgIndex=7)
    
    依据此分类，服务端的组件管理仍有较多可提升空间。
    

## 最后

![](https://mmbiz.qpic.cn/mmbiz_png/KV8By3euEQgLVHTvYBTd2FElqnibSLxXXypqBkpbt7ueL3l52wEHFDOc2ybL8RmBjg3A93KaaSGlHREGQkgkJlA/640?wx_fmt=png&from=appmsg#imgIndex=8)

更多岗位，可进入网易招聘官网查看（https://hr.163.com/）