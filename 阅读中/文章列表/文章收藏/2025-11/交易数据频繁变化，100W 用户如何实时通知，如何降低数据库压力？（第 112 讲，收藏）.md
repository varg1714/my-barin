---
source: https://mp.weixin.qq.com/s/iYY0LH04cOQXw3eB43KUBA
create: 2025-11-15 23:48
read: true
knowledge: true
knowledge-date: 2025-11-15
tags:
  - 系统架构
summary: "[[实时推送架构：长连接、MQ、缓存与异步计算]]"
---
《架构师之路：架构设计中的 100 个知识点》

112. 实时通知

  

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/YrezxckhYOz42wHRGGQ2LSKzAj5pxAr2ESOBiaQxA9j93pqC09Bq4kPPe36IHktvdS0riaHDdvBq3RfYQ0naOIrQ/640?wx_fmt=jpeg#imgIndex=0)

继续答星球水友提问：**交易数据频繁变化，如何做缓存与推送，如何降低数据库压力？**  
  
并没有做过相关的业务，结合自己的架构经验，**说说自己的思路和想法**，希望对大家有启示。

  

**一、业务抽象**

1. 有很多客户端关注交易，假设百万级别；

2. 数据量不一定很大，上市交易的股票个数，假设万级别；

3. 写的量比较大，每秒钟有很多交易发生，假设每秒百级别；

4. 计算比较复杂，有求和 / 分组 / 排序等操作；

  

**二、潜在技术折衷**

**客户端与服务端连接如何选型？**

![](https://mmbiz.qpic.cn/mmbiz_png/YrezxckhYOyP7YyJ0W3uiaZXAk63lolubj7N53ky7SicDkZGmoIoibdwxxLmpKcG3qZDPANbdtfm9libAn4kVOT3nQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=1)

首先，交易客户端与服务器建立 TCP 长连接，而不是每次请求都建立与销毁短连接，能极大提升性能，降低服务器压力。

  

**业务的实时性如何满足？**

交易业务，对数据实时性的要求较高，服务端可以通过 TCP 长连接推送，保证消息的实时性。

  

由于推送量级巨大，可以独立推送集群，专门实施推送。推送集群独立化之后，增加推送服务器数量，就可以线性提升推送能力。

![](https://mmbiz.qpic.cn/mmbiz_png/YrezxckhYOyP7YyJ0W3uiaZXAk63lolubewlKo88BHsgkYZHXMRwdpxdVTeBeBWhNibkGV0C0KM6IiavJdWnhE5Ig/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=2)

如上图所示，假设有 100W 用户接收实时推送：

1. 搭建专门的推送集群，维护与客户端的 tcp 长连接，实时推送；

2. 每台推送服务维护 10W 长连接，10 台推送服务即可服务 100W 用户；

**3. 推送集群**与**业务集群**之间，通过 MQ 解耦，推送集群只单纯的推送消息，无任何业务逻辑计算，推送消息的内容，都是业务集群计算好的；

  

推送服务最大的瓶颈是，**如何将一条消息，最快的推送给与之连接的 10W 个客户端？**

1. 如果消息量不大，例如几秒钟一个消息，可以开多线程，例如 100 个线程，并发推送；

_画外音：__对应水友提到的，如果量不大，可以成交一笔推送一笔。_

2. 如果消息量过大，例如一秒钟几百个消息，可以将消息暂存一秒，批量推送；

_画外音：对应水友提到的，如果消息量巨大，批量推送是很好的方法。_

**数据量，写入量，扩展性如何满足？**

股票个数较少，数据量不是瓶颈。

![](https://mmbiz.qpic.cn/mmbiz_png/YrezxckhYOyP7YyJ0W3uiaZXAk63lolubqSgU2SRsCkJiboFWoCBt6D0mkTCUwcAgxZJokfpiblxCe2SKV9rsanlQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=3)

流水数据写入量，每秒百级别，甚至千级别，数据库写性能也不是瓶颈，理论上一个库可以扛住。

  

![](https://mmbiz.qpic.cn/mmbiz_png/YrezxckhYOyP7YyJ0W3uiaZXAk63lolubgsn0XTzsRZ7micVkfRTWQvwnZSeG7SbIrhZicxzf5fKWVpMucF2YaoGA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=4)

假如每秒写入量达到万级别，可以在数据库层面实施水平切分，将不同股票的流水拆到不同水平切分的库里去，就能线性增加数据库的写入量。

_画外音：__水平拆分后，同一个股票，数据在同一个库里，不同股票，可能在不同的库里，理论上不会有跨库查询的需求。_

![](https://mmbiz.qpic.cn/mmbiz_png/YrezxckhYOyP7YyJ0W3uiaZXAk63lolub8Tp7FLLjMyUInibqTx1PH2vSpTQKL69blFmib67ibSmWpb9GCN358qiavQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=5)

如果每秒写入量达到十万，百万级别，还可以加入 MQ 缓冲请求，削峰填谷，保护数据库。

  

无论如何，根据本业务的数据量与写入量，单库应该是没有问题的。

**复杂的业务逻辑操作，如何满足？**

本业务的写入量不大，但读取量很大，肯定不能每个读取请求都 sum/group by/order by，这样数据库肯定扛不住。  
  
水友已经想到了，可以用缓存来降低数据库的压力，但担心 “随着时间的推移，这个偏差势必会慢慢放大”。

  

关于缓存的一致性的放大，可以这么搞：

1. 做一个异步的线程，每秒钟访问一次数据库，将复杂的业务逻辑计算出来，放入高可用缓存；

2. 所有的读请求不再耦合业务逻辑计算，都直接从高可用缓存读结果；

  

如此一来，**复杂业务逻辑的计算，每秒钟只会有一次**。

  

带来的问题是，一秒内可能有很多流水写入数据库，但不会实时的反应到缓存里，用户**最差情况下，会读到一秒前的交易数据**。

  

无论如何，这是一个性能与一致性的设计折衷。

  

上面的所有方案，都是基于在线客户量级巨大，推送消息巨大的前提下，采用推送方案。很多时候，工程师都会妄加猜测，把问题想得很复杂，把方案搞得很复杂。

**如果在线用户量很小**，用户能够接受的交易时延较长（例如 5s），完全可以采用轮询拉取方案：

![](https://mmbiz.qpic.cn/mmbiz_png/YrezxckhYOyP7YyJ0W3uiaZXAk63lolubx6LnvN85iaWGYOAVeH3stEeTSSy16M5ohWnkohHKaspbR7TdlAR6iaGA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=6)

1. 取消整个推送集群与 MQ 集群；

2. 交易数据，异步线程每 1s 写入高可用缓存一次；

3. 客户端每 5s 轮询拉取最新的交易数据，都只从缓存中拉取；

**搞定！**

  

反正，肯定不能每个读请求都 sum/group by/order by 扫库计算，这个是最需要优化的。

  

**三、总结**

1. 长连接比短连接性能好很多倍；

2. 推送量巨大时，推送集群需要与业务集群解耦；

3. 推送量巨大时，并发推送与批量推送是一个常见的优化手段；

4. 写入量巨大时，水平切分能够扩容，MQ 缓冲可以保护数据库；

5. 业务复杂，读取量巨大时，加入缓存，定时计算，能够极大降低数据库压力；

  

高并发推送，架构设计要点，你学废了吗？

知其然，知其所以然。

**思路比结论更重要。**

== 全文完 ==

有架构合集吗？

合集一：《[流量从 10 万到 10 亿，一定会遇到的 80 个架构问题（8000 字长文）](https://mp.weixin.qq.com/s?__biz=MjM5ODYxMDA5OQ==&mid=2651974945&idx=1&sn=58ff54415ddf2dd52d03f47a6790344b&scene=21#wechat_redirect)》

画外音：从理论到实践，15 年架构师生涯的系统性总结。

合集二：《[关于即时通讯架构的一切！](https://mp.weixin.qq.com/s?__biz=MjM5ODYxMDA5OQ==&mid=2651975468&idx=1&sn=54ab265bee4998da9a0d32091699cb1d&scene=21#wechat_redirect)》

画外音：职业生涯前 5 年，都在做 IM 架构。

如何提问？

免费加入星球，一起技术交流。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/YrezxckhYOyHicSCibf68mt4pGa2pPmkZNbq2icyf4jsbKwnJJrtlH2sg6nZdy37BC6IaYlNJbu9iahHWBmDrOib6XQ/640?wx_fmt=jpeg&from=appmsg&randomid=g1weg7nk&wxfrom=5&wx_lazy=1&tp=webp#imgIndex=17)

欢迎预约直播：技术人的副业规划（完结篇）！

欢迎预约！