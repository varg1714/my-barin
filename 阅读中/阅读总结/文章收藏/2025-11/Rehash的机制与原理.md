---
source:
  - "[[美团针对 Redis Rehash 机制的探索和实践 - 美团技术团队]]"
create: 2025-11-03
---

## 1. 核心摘要

本笔记深入探讨了 Redis 的 `Rehash` 机制在实际生产环境中可能引发的两个严重问题（“坑”），并结合源码与实践进行分析：

1. **内存抖动问题**：在 Redis 内存接近满容（`maxmemory`）时，`Rehash` 操作会触发一次性的、较大的内存分配，导致内存瞬间超出上限，引发大规模、非预期的 Key 驱逐，严重时可导致主从不一致和业务可用性下降。
2. **Scan 扫描遗漏问题**：在特定场景下（字典缩容 Rehash），使用 `SCAN` 命令遍历并清理数据时，由于旧版 Redis 的一个实现缺陷，可能导致部分 Key 被遗漏，无法被完全扫描到。

## 2. 案例一：满容状态下 Rehash 导致大量 Key 驱逐

### 2.1. 异常现象

在设置了 `maxmemory` 和驱逐策略的 Redis 集群中，监控到 Master 和 Slave 节点几乎在同一时间点发生大规模的 Key 驱逐。这与“通常只有 Master 先满、先驱逐”的预期不符，表明 Slave 节点也因某种原因发生了内存突增。

### 2.2. 根因分析：Rehash 的内存陷阱

问题的根源在于 Redis 的字典（Dict）扩容机制，即 Rehash。

* **字典结构**：Redis 的所有 KV 都存储在字典 `dict` 中。`dict` 内部包含两个哈希表 `dictht ht[2]`。正常情况下只使用 `ht[0]`，当需要扩容或缩容时，`ht[1]` 会被用来作为迁移的目标。
* **Rehash 触发**：当哈希表的负载因子（`used / size`）达到一定阈值时（通常是 `used >= size`），Redis 会触发 `dictExpand` 操作，准备进行 Rehash。
* **内存分配的关键点**：
    1. 当 Rehash 触发时，Redis 会为新的哈希表 `ht[1]` 分配内存。
    2. 这块内存是为 `ht[1].table`（一个指向 `dictEntry` 指针的数组）分配的，并且是**一次性完成**的。
    3. 在整个渐进式 Rehash 期间，`ht[0]` 和 `ht[1]` **同时存在于内存中**。
    4. 这意味着，Rehash 会带来一个**瞬时**的、额外的内存开销。

* **内存开销对照表**

| ht[0].size | 触发 Resize 时，ht[1] 需分配的内存 |
| :--- | :--- |
| ... | ... |
| 8,388,608 | 128M |
| 16,777,216 | 256M |
| 33,554,432 | 512M |
| ... | ... |

* **事件链还原**：
    1. **临界状态**：Redis 实例内存使用率已非常高（例如 95%），接近 `maxmemory`。
    2. **触发**：一个新的写命令到来，使得 Key 的总数达到了触发 Rehash 的临界点。
    3. **内存突增**：Redis 立即为 `ht[1]` 分配一块较大的内存（如 128MB）。
    4. **超出上限**：这块突增的内存如同“压垮骆驼的最后一根稻草”，使 Redis 的总内存瞬间超过 `maxmemory`。
    5. **大规模驱逐**：为了遵守 `maxmemory` 的限制，Redis 立即启动驱逐策略，大量淘汰 Key，从而引发监控图中的现象。

### 2.3. 解决方案与权衡

这并非一个简单的 Bug，而是 Redis 设计上的权衡。社区和业界主要有两种应对思路：

* **方案一：美团的源码优化（牺牲性能换稳定）**
    * **逻辑**：在触发 `dictExpand` 之前，增加一个判断：检查当前可用内存是否足够容纳即将为 `ht[1]` 分配的内存。如果不够，则**不执行 Rehash**。
    * **优点**：有效避免了因 Rehash 导致的内存突增和大规模驱逐。
    * **代价（Trade-off）**：不 Rehash 会导致哈希冲突增加，链表变长，从而**降低 Redis 的读写性能**。此方案未被官方采纳，因为它改变了 Redis 的核心行为。
* **方案二：社区主流的运维实践（预留资源保平安）**
    * **逻辑**：通过运维手段，确保 Redis 永远不要运行到危险的内存临界点。
    * **具体措施**：
        1. **设置内存缓冲**：将内存使用率告警阈值设置在 **75%-80%**，而不是 95% 以上。
        2. **提前扩容**：一旦触发告警，立即进行扩容，防患于未然。
        3. **容量规划**：在部署时就将 Rehash 可能的额外内存开销考虑在内。

## 3. 案例二（深度解析）：Scan 命令因 Rehash 导致数据清理不彻底

### 3.1. 问题的核心：`SCAN` 必须在动态变化的哈希表上保证完整性

`SCAN` 命令被设计用来在不长时间阻塞 Redis 的情况下，渐进式地遍历所有 Key。它的挑战在于，在多次 `SCAN` 调用之间，哈希表的大小可能已经发生了变化（扩容或缩容）。

* **简单线性扫描的缺陷**：
    * **缩容时漏掉 Key**：如果哈希表从 8 缩容到 4，而你的 `SCAN` 游标刚好在 3，下一次调用会认为遍历已结束（因为新表只有 0-3）。但原先在桶 4, 5, 6, 7 的数据已经被合并到了 0-3 中，这些数据你就永远扫描不到了。
    * **扩容时重复 Key**：如果哈希表从 8 扩容到 16，你扫描完 0-7 后，继续扫描 8-15。但原先 0-7 的数据有一部分已经分裂到了 8-15，这部分数据就会被重复扫描。

为了解决这个问题，Redis 采用了**高位序访问（High-Order Access）**，也称为**反向二进制迭代（Reverse Binary Iteration）**。

### 3.2. 高位序访问（你提到的“高位进位”）的原理

这个算法的核心思想是：**通过改变遍历桶的顺序，使其遍历模式与哈希表扩容/缩容的 Key 移动模式相匹配，从而保证完整性。**

让我们以一个大小为 8 的哈希表（索引 0-7）为例：

* **普通顺序（低位进位）**：`000` -> `001` -> `010` -> `011` -> `100` ...
    * 二进制的最低位先变化，像里程表一样。
* **高位序（高位进位）**：`000` -> `100` -> `010` -> `110` -> `001` ... (0 -> 4 -> 2 -> 6 -> 1 -> 5 -> 3 -> 7)
    * 二进制的最高位先变化。

**这个算法是如何实现的？**
它通过一个“翻转-加一-再翻转”的技巧实现：
`next_cursor = reverse_bits(reverse_bits(current_cursor) + 1)`

**示例：从游标 0 计算下一个游标 4 (表大小为 8，掩码 `m0` 为 `0b111`)**
1. 当前游标 `v = 0` (`0b000`)
2. `rev(v)`: `0b000` 翻转还是 `0b000`
3. `rev(v) + 1`: `0b000 + 1 = 0b001`
4. `rev(0b001)`: `0b001` 翻转得到 `0b100` (十进制 4)
5. 下一个游标就是 4。

**为什么这个顺序能保证不漏 Key？**

* **关键在于哈希表的伸缩规则**：当哈希表大小从 `N` 变为 `2N` 时，一个在旧表桶 `i` 的 Key，它的新位置要么还是 `i`，要么是 `i + N`。这取决于 Key 的哈希值新增的那个高位是 0 还是 1。
* **高位序的遍历模式**：高位序扫描的顺序（`0 -> N -> ...`）恰好是先访问一个旧桶 `i`，紧接着就去访问它可能分裂出的新桶 `i + N`。
    * 例如，从 8 扩容到 16，高位序会先访问 0，再访问 8；先访问 1，再访问 9。
    * **对于缩容**，这是逆过程。旧表的桶 `i` 和 `i + N` 会合并到新表的桶 `i`。高位序扫描是一种**全排列**，它最终总会扫描到桶 `i`。即使在扫描过程中，桶 `i+N` 已经合并过来，当扫描器最终到达桶 `i` 时，它会找到所有合并后的数据。

**结论：高位序扫描通过一种特殊的遍历全排列，保证了无论哈希表如何伸缩，它总能覆盖所有数据可能迁移到的位置。它用“可能返回重复元素”的代价，换取了“绝不遗漏”的可靠性。**

### 3.3. 源码 Bug 的深度剖析

现在我们来看旧版代码错在哪里。问题出在 `Rehashing` 状态下的 `do-while` 循环中。

* **场景**：正在从一个大表 `t1` (size=32) 缩容到一个小表 `t0` (size=8)。
* **游标**：客户端传来一个游标，比如 `v = 20` (`0b10100`)。
* **正确流程**：
    1. 扫描小表 `t0` 的桶：`v & m0` => `20 & 7` (`0b10100 & 0b00111`) => `4` (`0b100`)。所以扫描小表的 4 号桶。
    2. 扫描大表 `t1` 中所有**可能合并**到小表 4 号桶的桶。这些桶的索引低 3 位都应该是 `100`。在大表 (size=32) 中，它们是：
        * `0b00100` (4)
        * `0b01100` (12)
        * `0b10100` (20)
        * `0b11100` (28)
* **旧版 Buggy 代码的行为**：
    * 它在 `do-while` 循环里使用了这行代码来计算下一个游标：
      `v = (((v | m0) + 1) & ~m0) | (v & m0);`
    * 我们来模拟一下当 `v=20` 时会发生什么：
        1. `v | m0`: `0b10100 | 0b00111` => `0b10111`
        2. `+ 1`: `0b10111 + 1` => `0b11000`
        3. `& ~m0`: `0b11000 & ~0b00111` => `0b11000 & 0b11000` => `0b11000`
        4. `| (v & m0)`: `0b11000 | 0b00100` => `0b11100` (十进制 28)
    * **结果**：代码从游标 20 直接跳到了 28，**中间的桶 12 被完美地跳过了！** 这就是 Key 遗漏的直接原因。这个算法本质上是一种对高位的“低位进位”，它不是一个全排列，在特定条件下会跳过某些值。

### 3.4. 修复方案：逻辑统一

* **修复逻辑**：将 `do-while` 循环中那个有问题的、自己造的迭代算法，替换为**标准、健壮的高位序扫描算法**。
    `v |= ~m1; v = rev(v); v++; v = rev(v);`
* **效果**：这样，无论是扫描主表，还是在 Rehash 过程中扫描关联的表，都使用了同一种能保证完整性的遍历策略，从根本上修复了 Bug。

这个案例是 Redis 复杂性的一个缩影，展示了在追求高性能和低延迟的同时，要处理各种并发和状态变化是多么具有挑战性。

## 4. 总结与启示

1. **深入理解内部机制**：对于 Redis 这样的高性能组件，仅了解 API 是不够的。深入理解其内存管理、Rehash 等核心机制，是保障线上服务稳定性的关键。
2. **关注设计权衡（Trade-off）**：很多问题并非简单的 Bug，而是设计上的权衡。例如案例一中，Redis 坚守 `maxmemory` 的承诺，但带来了内存抖动风险。理解这些权衡有助于我们做出更合理的架构决策和运维规划。
3. **运维规划的重要性**：对于案例一这类问题，最佳实践往往是在运维层面进行规避，通过合理的容量规划和监控预警，避免系统运行在危险的临界区。
4. **版本意识**：笔记中提到的问题基于 Redis 3.2.8 版本。了解并跟进社区的新版本和修复记录，是避免踩坑的有效途径。