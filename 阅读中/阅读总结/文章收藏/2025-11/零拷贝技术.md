---
source:
  - "[[原来 8 张图，就可以搞懂「零拷贝」了]]"
create: 2025-11-26
---

## 1. 深度解析：I/O 优化与零拷贝技术

### 1.1. 背景：为什么需要优化 I/O？

磁盘是计算机系统中速度最慢的硬件之一（比内存慢 10 倍以上）。为了提升系统吞吐量，操作系统引入了多种优化技术，如零拷贝（Zero-Copy）、直接 I/O（Direct I/O）、异步 I/O（Async I/O）以及 PageCache（磁盘高速缓存）。

### 1.2. I/O 传输方式的演进

#### 1.2.1. 没有 DMA 的时代

在早期，I/O 数据传输完全依赖 CPU。

* **过程**：CPU 发指令 -> 磁盘准备数据 -> 磁盘发起中断 -> **CPU 亲自将数据从磁盘控制器读入寄存器，再写入内存**。
* **缺点**：传输期间 CPU 无法处理其他任务，资源被严重浪费。

![图片](https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZcHKiczImueTBjnrXSnRM13meibUYyVDUQKQNrp8qAERdb7v0SvHcDGMI6RoSE4y8ibpGqF1agxyDiaiag/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=2)

#### 1.2.2. DMA (Direct Memory Access) 技术

DMA 控制器接管了数据搬运工作。

* **定义**：在 I/O 设备和内存传输数据时，搬运工作交给 DMA，CPU 不再参与具体搬运，只负责控制和处理中断。
* **过程**：CPU 告知 DMA 传输参数 -> DMA 搬运数据 -> 搬运完成 DMA 通知 CPU。
* **意义**：解放了 CPU，使其可以并发处理其他事务。

![图片](https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZcHKiczImueTBjnrXSnRM13mF08dz9WOND7zqicNq8bs0mOmu2jSaJiaFEJru35teqvibMCxPCLsyp2mg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=3)

### 1.3. 传统文件传输的痛点

如果服务器执行“读取磁盘文件并发送给客户端”的操作，代码通常是：

```c
read(file, tmp_buf, len);
write(socket, tmp_buf, len);
```

虽然代码简单，但底层代价巨大：

![图片](https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZcHKiczImueTBjnrXSnRM13mZLjg2kJB3GQu4AVtqncaSnSV8YVhTMqkxO1Q5ZAWIVY8QkQDwVkW3w/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=4)

* **4 次上下文切换**：用户态 $\leftrightarrow$ 内核态（`read` 2 次 + `write` 2 次）。
* **4 次数据拷贝**：
    1. 磁盘 $\xrightarrow{DMA}$ 内核缓冲区 (PageCache)
    2. 内核缓冲区 $\xrightarrow{CPU}$ **用户缓冲区**
    3. 用户缓冲区 $\xrightarrow{CPU}$ Socket 缓冲区
    4. Socket 缓冲区 $\xrightarrow{DMA}$ 网卡
* **核心问题**：数据在用户态和内核态之间来回搬运是多余的，因为用户进程并没有对数据进行“再加工”。

### 1.4. 零拷贝技术 (Zero-Copy)

目标：减少上下文切换次数，减少数据拷贝次数。

#### 1.4.1. 方案一：mmap + write

使用 `mmap()` 替换 `read()`。

* **原理**：`mmap` 将内核缓冲区的数据**映射**到用户空间，用户进程与内核共享缓冲区。
* **效果**：减少了 1 次 CPU 拷贝。
    * **3 次拷贝**：磁盘 -> 内核 (DMA) -> Socket (CPU) -> 网卡 (DMA)。
    * **4 次上下文切换**：依然需要调用 `mmap` 和 `write`。

![图片](https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZcHKiczImueTBjnrXSnRM13mokPruysrVuhMBbPeLsoFylbxLo07NGXLqyzKZfHI3r29kdqkDaImsQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=5)

#### 1.4.2. 方案二：sendfile (Linux 2.1)

* **函数**：`ssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count);`
* **原理**：一个系统调用完成读写，数据直接在内核内部传递。
* **效果**：
    * **2 次上下文切换**。
    * **3 次拷贝**：磁盘 -> 内核缓冲区 -> Socket 缓冲区 (CPU) -> 网卡。

![图片](https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZcHKiczImueTBjnrXSnRM13mD19b7SCEuj1icTmFg5kg4xmIq0vqhqKVM1o7oISMaZxoUcKCl7yGwvg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=6)

#### 1.4.3. 方案三：真正的零拷贝 (Linux 2.4 + SG-DMA)

如果网卡支持 **SG-DMA** (Scatter-Gather DMA) 技术：

* **原理**：`sendfile` 调用时，只将**描述符**（地址和长度）传给 Socket 缓冲区，不传数据。网卡 SG-DMA 控制器直接将内核缓冲区（PageCache）的数据拷贝到网卡。
* **效果**：
    * **2 次上下文切换**。
    * **2 次拷贝**：磁盘 $\xrightarrow{DMA}$ PageCache $\xrightarrow{SG-DMA}$ 网卡。
* **结论**：全程没有 CPU 参与数据搬运，实现了真正的“零 CPU 拷贝”。性能可提升 65% 以上（如 Kafka、Nginx）。

![图片](https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZcHKiczImueTBjnrXSnRM13m9aUVVJ2BT9QBoPQqB1iaTSn4kSL1sR9sQYLGbsPxticvZgIptotGT3Ng/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=7)

### 1.5. PageCache 的双刃剑

零拷贝技术高度依赖 **PageCache**（磁盘高速缓存）。

* **优势**：
    1. **缓存热点数据**：利用时间局部性，减少磁盘 I/O。
    2. **预读功能**：利用空间局部性，减少磁头寻址耗时。
* **劣势（大文件传输场景）**：
    1. **缓存污染**：GB 级别的大文件会瞬间填满 PageCache，淘汰掉真正高频访问的小文件（热点数据）。
    2. **收益低**：大文件通常只读一次，缓存命中率极低，白白浪费一次 DMA 拷贝到 PageCache 的开销。

### 1.6. 大文件传输优化：异步 I/O + 直接 I/O

针对大文件，不能用零拷贝（因为零拷贝依赖 PageCache），而应该使用 **异步 I/O + 直接 I/O**。

#### 1.6.1. 为什么是“直接 I/O”？

* **普通 I/O (Buffered I/O)**：磁盘 -> **PageCache** -> 用户缓冲区。
* **直接 I/O (Direct I/O)**：磁盘 $\xrightarrow{DMA}$ **用户缓冲区**。
    * **绕过 PageCache**：避免了缓存污染，且减少了从 PageCache 到用户缓冲区的 CPU 拷贝。

#### 1.6.2. 补充：直接 I/O 的实现细节 (基于对话总结)

* **API 实现**：通过在 `open()` 系统调用中添加 **`O_DIRECT`** 标志来实现。

    ```c
    // 伪代码示例
    int fd = open("large_video.mp4", O_RDONLY | O_DIRECT);
    ```

* **严格限制**：使用直接 I/O 时，应用层必须处理**内存对齐**问题（缓冲区地址、读写长度、文件偏移量必须是磁盘扇区大小的整数倍），否则会报错。

#### 1.6.3. 异步 I/O (AIO) 的作用

* **同步 I/O**：`read` 时进程阻塞等待磁盘数据。
* **异步 I/O**：发起请求后立即返回，数据传输完成后内核通知进程。
* **结合**：异步 I/O 通常配合直接 I/O 使用，实现无阻塞的大文件读取。

![图片](https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZcHKiczImueTBjnrXSnRM13mmKwMQ1fXJDHkv5yolSib8waibP04xBXEPfibXoW6jFick0RmZOR77iaaeIw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=10)

### 1.7. 总结与应用策略

#### 1.7.1. 核心对比

| 方式 | 上下文切换 | CPU 拷贝 | DMA 拷贝 | 适用场景 |
| :--- | :---: | :---: | :---: | :--- |
| **传统 I/O** | 4 次 | 2 次 | 2 次 | 通用，低并发 |
| **mmap + write** | 4 次 | 1 次 | 2 次 | 需要修改数据时 |
| **sendfile** | 2 次 | 1 次 | 2 次 | 静态文件传输 |
| **sendfile + SG-DMA** | 2 次 | **0 次** | 2 次 | 高性能小文件传输 (Kafka, Nginx) |
| **直接 I/O + 异步 I/O** | 2 次 | 0 次 (无内核缓冲) | 1 次 (到用户态) | **大文件传输** (数据库, 视频流) |

#### 1.7.2. Nginx 的智能配置

Nginx 可以根据文件大小自动切换策略：

```nginx
location /video/ {
    sendfile on;       # 小文件：使用零拷贝 (利用 PageCache)
    aio on;            # 开启异步 I/O
    directio 1024m;    # 大文件 (>1GB)：使用直接 I/O (绕过 PageCache)
}
```

* **小文件**：走 `sendfile`，利用 PageCache 加速。
* **大文件**：超过阈值（如 1024MB），自动切换为 `O_DIRECT` 模式，防止缓存打爆。