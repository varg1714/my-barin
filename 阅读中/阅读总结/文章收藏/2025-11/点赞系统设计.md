---
source:
  - "[[每秒 30W 次的点赞业务，架构怎么优化？（第 113 讲，收藏）]]"
create: 2025-11-17
---

## 1. 高并发点赞系统架构演进与深度优化笔记

本文档旨在全面复盘一个高并发（30W QPS）点赞系统的架构设计与演进过程。内容分为四大部分：

1. **基础架构优化**：源于原始文章的核心思想，解决高并发读写和业务扩展性问题。
2. **完整业务逻辑实现**：结合我们的对话，补充点赞明细存储、解决数据库并发瓶颈。
3. **核心难题攻坚**：解决异步操作下的数据一致性、大 Key 等生产级难题。
4. **系统安全与防御**：探讨如何抵御恶意攻击，构建稳健的系统。

## 2. 第一部分：基础架构优化 (源于文章)

### 2.1. 核心挑战与朴素方案

* **挑战**:
    * **超高吞吐量**: 30W QPS 的读写请求，数据库无法直接承载。
    * **低一致性要求**: 计数有微小延迟或不准，业务上可以接受。
    * **业务扩展性**: 未来可能增加阅读、转发、评论等多种计数需求。
* **朴素方案**:
    * **引入缓存**: 使用 Redis 扛实时读写流量，异步线程将数据刷回 MySQL。
    * **服务化**: 将计数系统与主业务解耦。
    * **初始设计**:
        * MySQL: `t_count(msg_id, praise_count)`
        * Redis: `Key: msg_id`, `Value: praise_count`
* **朴素方案的瓶颈**:
    * 当需要获取一个页面的多条内容（如 100 条），每条内容又需要多种计数（点赞、评论、转发、阅读）时，会导致 `100 * 4 = 400` 次 Redis 查询，形成 **N+1 查询风暴**，效率极低。

### 2.2. 架构优化策略

#### 2.2.1. 优化一：数据库层面 - 提升扩展性

为了在增加新计数类型时避免频繁修改表结构 (`ALTER TABLE`)，我们有两种设计思路：

| 扩展方式         | 表结构                                              |                     优点 | 缺点                    |
| ------------ | ------------------------------------------------ | ---------------------: | --------------------- |
| **列扩展**      | `t_count(msg_id, praise_count, read_count, ...)` |                   结构直观 | 扩展性差，每次增加新业务都需要改表。    |
| **行扩展 (更优)** | `t_count(msg_id, count_key, count_value)`        | **扩展性极佳**，增加新业务只需插入新行。 | 数据行数增多，但可通过数据库水平扩展解决。 |

#### 2.2.2. 优化二：缓存层面 - 提升批量操作效率

为了解决 N+1 查询问题，核心思想是将同一 `msg_id` 的多个计数聚合存储。

* **优化前**: 多个 Key，每个 Key 存一个计数值。
    * `msg_id:praise` -> `100`
    * `msg_id:read` -> `5000`
* **优化后**: 单个 Key，用一个 Value 存储所有计数值。
    * **Key**: `msg_id`
    * **Value**: 使用 Redis Hash 或 JSON 字符串，如 `{"praise": 100, "read": 5000, ...}`
    * **优势**: 一次 Redis 查询即可获取目标内容的所有计数，极大提升效率。

## 3. 第二部分：完整业务逻辑实现 (对话补充)

文章只关注了“总数”，但实际业务还需要知道“谁点的赞”。

### 3.1. 点赞明细的存储

* **职责**: 记录和查询“谁赞了谁”，保证用户不能重复点赞。
* **方案**: 使用独立的 MySQL 表 `t_praise_detail`。
    * **表结构**: `(id, user_id, msg_id, create_time)`
    * **关键索引**: 在 `(user_id, msg_id)` 上建立**联合唯一索引**，用于快速查询和防止重复。

### 3.2. 解决点赞明细表的并发瓶颈

* **问题**: 如果每次点赞都直接写 `t_praise_detail` 表，高并发压力又回到了数据库。
* **解决方案：异步持久化**
    1. **引入消息队列 (MQ)**: 如 Kafka、RocketMQ。
    2. **实时流程**: 用户点赞时，API 服务仅执行两个操作：
        * 更新 Redis 中的**计数器**。
        * 将点赞这个“事件” `{user_id, msg_id, action}` 发送到 **MQ**。
    3. **异步流程**: 一个独立的消费者服务（Worker）从 MQ **批量拉取**消息，然后**批量写入**数据库。
* **优势**: 将前端高并发的实时写，转换成后端平稳的批量写，彻底为数据库减负。

## 4. 第三部分：核心难题攻坚 (对话深入)

### 4.1. 问题：异步操作下的数据不一致（重复点赞）

* **场景**: 在数据写入 MQ 但未消费前，用户快速连点，可能导致 Redis 计数被错误地增加多次。
* **解决方案：使用 Redis Lua 脚本保证原子性**
    * **核心思想**: 将“检查用户是否已点赞”和“更新点赞状态/计数”合并成一个**原子操作**。
    * **数据结构**:
        * 点赞明细缓存 (Set): `praised_by:{msg_id}`，存 `user_id`。
        * 计数器缓存 (Hash): `counts:{msg_id}`，存 `praise_count`。
    * **Lua 脚本逻辑**:
        1. 尝试用 `SADD` 将 `user_id` 加入 Set。
        2. 如果 `SADD` 返回 1 (表示首次添加成功)，则执行 `HINCRBY` 增加计数，并向 MQ 发送消息。
        3. 如果 `SADD` 返回 0 (表示已存在)，则不做任何操作。
    * **优势**: 彻底杜绝了并发下的数据不一致问题，保证了操作的**幂等性**。

### 4.2. 问题：热点内容导致的“大 Key”问题

* **场景**: 一条超级热点内容被千万用户点赞，`praised_by:{msg_id}` 这个 Set 会变得巨大。
* **大 Key 危害**: 内存倾斜、Redis 阻塞、集群迁移困难。
* **解决方案：分片 (Sharding)**
    * **核心思想**: 将一个大 Set 分解成 N 个小 Set。
    * **实现**:
        1. 预设分片数 `N` (如 100)。
        2. 通过 `hash(user_id) % N` 计算出分片索引 `shard_index`。
        3. 新的 Key 结构为 `praised_by:{msg_id}:{shard_index}`。
    * **优势**: 从根本上解决了大 Key 问题，且通过哈希将压力均匀分布到集群的不同节点。

### 4.3. 备选方案探讨：位图 (Bitmap)

* **优势**: 在 `user_id` 是连续小整数的理想情况下，内存占用极低。
* **致命缺陷**:
    1. **ID 类型限制**: 无法支持 Snowflake ID (64 位长整型) 或 UUID，因为会导致位图过大而无法创建。
    2. **稀疏 ID 灾难**: 内存占用取决于 `max(user_id)` 而非点赞数。一个巨大的 `user_id` 就会撑爆内存。
* **结论**: 对于通用点赞业务，**分片 Set 是远比位图更稳健、更通用的方案**。

## 5. 第四部分：系统安全与防御 (对话升华)

### 5.1. 攻击场景：有效数据攻击

* **场景**: 黑客利用一批**已经点过赞**的“肉鸡”账号发起高频查询请求。
* **问题**: 这种攻击可以轻易穿透布隆过滤器（因为数据真实存在），将压力直接打到后端的精确缓存查询层（Sharded Set），消耗 Redis 资源。

### 5.2. 防御策略：多维度、纵深式防御体系

单纯增加数据校验层（如两重布隆过滤器）无法防御此类攻击。必须引入**行为校验**。

* **第 1 层 (入口)：API 网关/WAF**
    * **手段**: 基于 **IP** 的速率限制。
    * **作用**: 拦截来自单一 IP 的简单攻击。
* **第 2 层 (应用)：服务内部（核心防御层）**
    * **手段**: 基于 **User ID** 或 **设备 ID** 的精细化速率限制。
    * **作用**: 限制单个账号或设备在单位时间内的行为频率，精准打击“肉鸡”账号的高频操作，是防御此场景的**最关键手段**。
* **第 3 层 (过滤)：概率性过滤器**
    * **手段**: 布隆过滤器或 Cuckoo 过滤器。
    * **作用**: 在通过速率限制后，快速过滤掉**无效/随机**数据的查询请求，减轻后端压力。Cuckoo 过滤器因支持删除而更具优势。
* **第 4 层 (精确校验)：精确缓存**
    * **手段**: 我们设计的分片 Set (Sharded Set)。
    * **作用**: 对通过了层层关卡的合法、低频请求，进行最终的业务逻辑判断。

通过这个集**行为限制**与**数据校验**于一体的纵深防御体系，系统才能在保证高性能的同时，具备抵御恶意攻击的稳健性。