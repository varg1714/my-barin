---
source: "[[架构设计：美国总统选举，要一个选票系统，要 100w tps，1000wqps，选票不可篡改，不可重复]]"
create: 2025-09-28
---

## 1. 核心需求与挑战

系统设计的出发点是解决四个维度的极限挑战：

1. **超高写入并发 (100 万 TPS)**: 应对投票截止前“最后一分钟”的流量洪峰。
2. **超高读取并发 (1000 万 QPS)**: 应对开票后全球用户、媒体的集中查询。
3. **数据不可篡改 (Immutability)**: 选票一经提交，其内容和状态必须锁定，具备可审计的公信力。
4. **数据不可重复 (Uniqueness)**: 严格保证“一人一票”，技术上实现精准的幂等性控制。

## 2. 总体架构设计哲学

架构的基石并非简单堆砌技术，而是遵循以下核心思想：

* **彻底的异步化与读写分离**: 这是应对超高并发的根本。承认同步处理的物理瓶颈，通过消息队列将写入和读取路径完全解耦，允许两者独立、异步地扩展。
* **命令查询职责分离 (CQRS) 思想**:
    * **命令 (Command)**: 写入路径 (`POST /vote`) 只负责接收请求并改变系统状态，追求极致的吞吐量。
    * **查询 (Query)**: 读取路径 (`GET /results`, `GET /votes/{id}`) 使用为查询优化的、预计算好的数据模型，追求极致的响应速度。
* **分层解耦**: 系统分为接入层、服务层、数据层，职责清晰，便于维护和扩展。

![总体架构图](https://mmbiz.qpic.cn/sz_mmbiz_png/xlgvgPaib7WMEUKbPmdcxnSpZRysdKlPIcz4Qe6DbamYdeSS0NibBQNT9gCAqiaRSqicI6CO34KA3XmcmDPpPrIepg/640?from=appmsg&watermark=1#imgIndex=2)

## 3. 写入路径 (`POST /vote`) 深度剖析

### 3.1. 设计目标

实现用户感知的**“瞬时”响应**。核心职责是**安全地接收并转交投票凭证**，而非“完成投票”。

### 3.2. 核心流程

1. **接入层 (L4/L7)**: 负载均衡、身份认证、安全防护、限流。
2. **写入服务 (Vote-Service)**:
    * 进行基础校验（格式、时间）。
    * 将合法投票请求构造成标准消息。
    * **核心**: 将消息发送至 Kafka，发送成功后立即向客户端返回成功响应。
3. **消息队列 (Kafka)**:
    * **角色**: 系统的“蓄能大坝”和**持久化事务日志**。它承接所有写入洪峰，并为后端处理提供可靠、可回溯的数据源。
4. **异步任务层 (Vote-Job)**:
    * 独立的消费者集群，从 Kafka **批量拉取**消息。
    * 执行核心业务逻辑：幂等性检查、数据持久化、区块链存证、更新统计数据。

### 3.3. 潜在问题与权衡

* **问题：Kafka 写入失败？**
    * **策略**: 采用“带重试的同步发送 + 失败降级本地日志 + 异步补偿”机制，确保前端请求不丢失。
* **问题：消息重复发送？**
    * **策略**: 开启 Kafka 幂等生产者 (`enable.idempotence=true`) + 消费端必须实现最终幂等性（由后续的查重机制保证）。
* **权衡：最终一致性**
    * 用户投票成功后，到能在系统中查询到结果，存在一个短暂的延迟。这是用异步换取高性能的必然代价，需要通过监控确保延迟在可接受范围内。

## 4. 读取路径 (`GET`) 深度剖析

### 4.1. 个人投票结果查询 (`GET /votes/{voter_id}`)

* **设计思路：缓存服务于“时间局部性”**
    * 该查询的最高频场景是用户**投票后立即确认**。
    * 缓存的真实目的不是容纳所有选民数据，而是**缓存最近一段时间内的活跃投票者数据**，为这个“热点”场景提供服务。
    * 采用**二级缓存**:
        * **L1 本地缓存 (Caffeine)**: 缓存单个服务实例上的超热点数据。
        * **L2 分布式缓存 (Redis)**: 缓存最近 15 分钟内投票的用户记录，设置较短 TTL。
* **潜在问题与应对**
    * **缓存穿透**: 对不存在的 `voter_id`，采用**缓存空值**的策略来抵御攻击。
    * **数据一致性**: 允许短暂不一致。缓存失效后，查询会穿透到数据库，加载最新数据并**自动回填**缓存，系统具备自我修复能力。

### 4.2. 选举总结果查询 (`GET /results`)

* **设计思路：读取路径零计算**
    1. **数据预聚合**: `Vote-Job` 在消费消息时，实时更新 Redis 中的分布式计数器（如 `HINCRBY`）。
    2. **物化视图**: 一个独立的 `Summary-Job` 定时（如每 5 秒）将 Redis 计数器的数据同步到 `vote_summary` 数据库表，并生成一个**完整的、最终形态的 JSON 对象**。
    3. **缓存最终结果**: L1 和 L2 缓存中直接存储这个预计算好的 JSON 字符串。查询请求命中缓存后，直接从内存返回，无需任何计算或数据库访问。
* **潜在问题与应对**
    * **缓存更新风暴**: 使用 **Redis Pub/Sub** 机制。当 `Summary-Job` 生成新 JSON 后，发布一个更新通知，所有服务实例订阅该通知并更新自己的本地缓存。
    * **数据时效性**: 返回的 JSON 对象中必须包含 `last_updated_timestamp` 字段，明确告知用户数据的新鲜度。

## 5. 核心难点攻坚：不可重复性 (一人一票)

这是整个设计的点睛之笔，展示了从常规方案到极致性能方案的演进。

* **问题本质**: 在 100 万 TPS 下，任何基于锁或单 Key 的精确判断方案（如分布式锁、Redis `SET`）都会因竞争或热点问题而崩溃。
* **解决方案：布隆过滤器 + 双层验证**
    * **角色定位**: 布隆过滤器是一个**概率性守门员**。它能以极高的效率和极低的内存成本，**快速过滤掉 99.99%的合法（首次）投票请求**。
    * **核心逻辑**:
        1. `Vote-Job` 处理消息时，先用 `voter_id` 查询布隆过滤器。
        2. **如果返回“不存在”**: 100%准确，说明是首次投票，直接进入后续流程。
        3. **如果返回“可能存在”**: 可能是真的重复投票，也可能是极小概率的误判。此时，再**查询数据库的唯一索引 (`uk_voter_id`)** 做最终的精确确认。
    * **效果**: 数据库的查重压力从百万级骤降至远低于 1%的水平，仅处理“可疑流量”。
* **工程化实现**:
    * **分片布隆过滤器**: 将一个大过滤器拆分为多个小过滤器（如 50 个 2MB），使用 `vote_bf:{index}` 的命名方式，让 Redis Cluster 根据 Key 的哈希自动路由和分片，实现了查重层的水平扩展。

## 6. 核心难点攻坚：不可篡改性

采用“内部约束 + 外部公证”的双重保障机制。

* **内部约束：Append-Only 数据模型**
    * **数据库层面**: 通过权限控制，对应用账号**禁用 `UPDATE` 和 `DELETE` 操作**，只授予 `INSERT` 和 `SELECT` 权限。
    * **数据层面**: 任何变更都以生成新记录的方式进行，保留完整的、不可变的历史轨迹。
* **外部公证：区块链存证**
    * **角色**: 区块链不作数据库，而是作为**不可篡改的公共账本**，提供事后审计的“铁证”。
    * **实现**:
        1. 对每张选票的核心字段计算 SHA-256 哈希。
        2. 将一批哈希值通过**Merkle 树**聚合成一个根哈希。
        3. 定期将 Merkle 根哈希写入区块链。
    * **优势**: 成本极低（一次链上交易为海量数据背书），且支持公开、独立的验证。

## 7. 数据存储架构

* **分层设计 (冷热分离)**:
    * **热数据层 (MySQL)**: 采用分库分表，服务于在线实时业务查询。
    * **冷数据层 (HBase)**: 存储全量、不可变的投票历史记录，用于离线分析、大数据处理和长期审计。
* **MySQL 分库分表策略**:
    * **一级分片 (按 `state_code`)**: 业务驱动，将同一州的数据物理聚合，优化按州查询，并实现故障域隔离。
    * **二级分片 (按 `vote_id` 哈希)**: 技术驱动，在州内将数据均匀打散到多张表中，解决单一州（如加州）的数据倾斜和写入热点问题。
* **数据同步**: 使用 **Canal 订阅 MySQL Binlog**，将数据变更实时同步到 Kafka，再由消费者写入 HBase，确保两层数据最终一致。

## 8. 总结与关键启示

1. **异步化是超高并发写入的银弹**: 将用户响应与后端处理解耦是突破性能瓶颈的关键。
2. **为查询模式设计数据，而非为数据设计查询**: 遵循 CQRS 思想，通过预计算和物化视图为高频查询场景创建最优的数据模型。
3. **用概率数据结构应对海量查重**: 布隆过滤器是处理大规模“是否存在”判断场景的利器，其“快速排除”的特性可极大保护核心系统。
4. **信任需要技术保障**: 不可篡改性不能只靠流程，必须结合内部数据库约束和外部密码学公证（区块链）来构建。
5. **没有完美的架构，只有合适的权衡**: 整个设计充满了权衡，如用最终一致性换取性能，用增加的运维复杂度换取系统的可扩展性。理解这些权衡是架构设计的核心。

## 9. 具体实现操作手册

本章节将理论架构转化为可操作的工程实践，详细阐述数据库分库分表、消息队列多 Worker 机制和缓存分片三大核心技术的具体实现方案。

### 9.1. 数据库分库分表机制 (基于 Sharding-JDBC)

**目标**: 将海量投票数据水平扩展到多个物理数据库和数据表中，同时保持业务代码的简洁性。

**核心实现**:
1. **物理布局**:
    * 创建 16 个独立的 MySQL 数据库实例 (`election_db_0` 到 `election_db_15`)。
    * 在**每个**数据库实例中，创建 64 张结构完全相同的物理表 (`vote_records_0` 到 `vote_records_63`)。

2. **配置驱动 (Sharding-JDBC `application.yml`)**:
    * **数据源定义**: 声明并配置所有 16 个物理数据库的连接信息。
    * **逻辑表定义**: 定义一个逻辑表名 `vote_records`，业务代码将只与此逻辑表交互。
    * **分片规则**:
        * **分库策略 (Database Strategy)**:
            * **分片键**: `state_code` (州代码)。
            * **算法**: 采用**一致性哈希**，确保相同州的投票数据路由到同一个数据库实例，便于按州查询和管理。
        * **分表策略 (Table Strategy)**:
            * **分片键**: `vote_id` (投票 ID)。
            * **算法**: 采用**哈希取模** (`vote_id.hashCode() % 64`)，将数据在库内均匀打散到 64 张表中，避免单表过大和写入热点。
    * **全局唯一 ID**: 配置使用**雪花算法 (SNOWFLAKE)** 为 `vote_id` 主键生成全局唯一的、趋势递增的 ID。

3. **代码实现 (透明化)**:
    * 业务代码（如 MyBatis Mapper）中的 SQL 语句**始终引用逻辑表名 `vote_records`**。
    * Sharding-JDBC 驱动在执行时会自动拦截 SQL，根据配置的分片键和算法，将 SQL**重写**为指向特定物理库和物理表的语句，然后执行。整个过程对应用层完全透明。

### 9.2. 消息队列多 Worker 机制 (基于 Kafka Consumer Group)

**目标**: 实现对 `vote_topic` 中海量消息的高吞吐、并行消费。

**核心实现**:
1. **Topic 设置**:
    * 创建 `vote_topic`，并设置**足够多的分区** (e.g., 64 或 128 个)。**分区数是决定最大并行度的天花板**。

2. **消费者组 (Consumer Group)**:
    * 所有 `Vote-Job` 实例（Workers）都配置**相同的 `group-id`** (e.g., `vote-processor-group`)。
    * Kafka 的**核心原则**: 在一个消费者组内，一个分区在同一时间最多只能被一个消费者实例消费。

3. **并行度扩展**:
    * **垂直扩展 (Scale Up)**: 在单个 Worker 实例的 `@KafkaListener` 注解中设置 `concurrency` 属性 (e.g., `concurrency = "3"`)。
        * **含义**: 这会在该 Worker 内部启动 3 个独立的 `KafkaConsumer` 实例（由 3 个线程驱动）。这 3 个消费者都属于同一个消费者组。
        * **作用**: 充分利用单台服务器的多核 CPU 资源。
    * **水平扩展 (Scale Out)**: 部署多个 Worker 应用实例（如多个 Kubernetes Pods）。
        * **机制**: Kafka 的**再均衡 (Rebalance)** 机制会自动将 Topic 的所有分区**均匀地分配**给消费者组内所有的消费者实例（包括由 `concurrency` 创建的内部消费者）。

4. **代码实现 (Spring Kafka)**:
    * **批量消费**: 开启批量监听 (`listener.type: BATCH`)，一次性拉取和处理多条消息，大幅提升吞吐量。
    * **手动 ACK**: 关闭自动提交偏移量 (`enable-auto-commit: false`)，设置为手动确认 (`ack-mode: MANUAL_IMMEDIATE`)。在代码中，只有当一个批次的消息**全部成功处理完毕**后，才调用 `ack.acknowledge()` 提交偏移量，确保消息处理的可靠性（At-Least-Once 语义）。

**关键公式**:
> **总并行度 = (Worker 实例数) × (每个实例的 `concurrency` 值)**
>
> 为避免资源浪费，此总并行度应 **≤** Topic 的分区数。

### 9.3. 缓存层分片机制 (基于 Redis Cluster)

**目标**: 构建一个可水平扩展、高可用的分布式缓存集群，自动分散海量缓存键。

**核心实现**:
1. **架构原理**:
    * **哈希槽 (Hash Slots)**: Redis Cluster 将所有 Key 的空间虚拟地划分为 16384 个哈希槽。
    * **节点分工**: 每个主节点（Master）负责管理一部分哈希槽。
    * **自动路由**: 客户端（如 Jedis, Lettuce）通过计算 `CRC16(key) % 16384` 来确定 Key 所属的哈希槽，然后将命令直接发送到负责该槽的节点。

2. **配置与代码 (透明化)**:
    * **应用配置**: 只需在配置文件中提供集群中**部分节点**的地址列表。客户端启动时会自动发现所有节点及其负责的哈希槽信息。
    * **代码实现**: 业务代码操作 Redis Cluster 的方式与操作单机版 Redis**几乎完全相同**。无论是 `SET` 一个个人投票结果，还是 `BF.ADD` 一个布隆过滤器元素，客户端驱动库都会在底层自动完成 Key 的哈希计算和命令路由。

3. **分片效果**:
    * **个人投票记录**: `vote:result:{voter_id}` 这样的 Key 会根据 `voter_id` 的不同，自然地分布到集群的不同节点上。
    * **布隆过滤器**: `vote_bf:{index}` (e.g., `vote_bf:0` ... `vote_bf:49`) 这种设计，不同的 `index` 值会使得这些布隆过滤器本身作为独立的 Key，被分散到集群的不同节点上，从而实现了查重压力的分布式承载。