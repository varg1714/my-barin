---
source: "[[希音面试：ClickHouse  Group By 执行流程  ？CK 能支持 十亿级数据 实时分析的原理 是什么？]]"
create: 2025-09-14
---

## 1. ClickHouse 高性能分析原理：全面深度总结

ClickHouse 能够支撑十亿级数据的实时分析，其核心在于一套**为在线分析处理（OLAP）场景深度优化的、环环相扣的系统设计**。它并非依赖单一技术，而是从架构、存储、索引到计算的每一个环节都做出了精准的取舍和极致的优化。

### 1.1. 宏观架构：为并行与效率而生

这是 ClickHouse 高性能的基础框架，原文首先阐明了其两大架构基石：

* **存储计算一体化**：
    与 Spark 等“计算存储分离”的引擎不同，Clickhouse 自带存储层（主要是 MergeTree 引擎）。这种设计允许存储层为计算层做深度优化，例如**物理数据排序**和**列式存储**，从源头上减少了计算时的数据读取和处理开销。
* **MPP (大规模并行处理) 架构**
    集群中所有节点对等，查询任务会被拆分并下发到所有相关节点并行执行，最后由协调节点汇总结果。这充分利用了整个集群的 CPU 和 I/O 资源，实现了计算能力的水平扩展。

### 1.2. 存储与数据访问：快查询的根基

这是 ClickHouse 性能的核心所在，其精髓在于**如何组织数据**以及**如何快速找到数据**。

* **2.1 核心引擎：MergeTree 与物理排序**
    * **列式存储**
        数据按列独立存储。分析查询通常只涉及少数几列，因此只需读取所需列的数据，极大减少了 I/O。同时，同列数据类型一致、特征相似，压缩率极高。
    * **数据预排序**：
        这是 MergeTree 引擎的灵魂。建表时通过 `ORDER BY` 指定的排序键，决定了数据在磁盘上的**物理存储顺序**。这个强约束是其高性能稀疏索引能够工作的前提。
* **2.2 写入模型：LSM-Tree 思想下的不可变性**
    为了维持物理排序，ClickHouse 并**不会**在写入时进行代价高昂的“级联调整”。它采用类似 LSM-Tree 的思想：
      
    1.  **写入即追加**
        新数据在内存中排序后，作为一个全新的、不可变的**数据部件 (Part)** 追加到磁盘。写入速度极快。
    2.  **后台合并 (Merge)**
        后台线程会定期将多个有序的小 Part 合并成一个更大的有序 Part，这个过程是异步的、批量的，从而维持了数据的整体有序性，并优化了查询性能。

    ClickHouse 通过**牺牲单行更新/删除的灵活性**，换来了极高的写入吞吐和简单高效的数据组织方式。

* **2.3 索引机制：以最小代价跳过最多数据**
    * **稀疏主键索引**
        * **原理**：基于数据的物理有序性，每隔 8192 行（一个颗粒）才建立一个索引标记。查询时通过对这个极小的索引文件进行二分查找，可快速将扫描范围缩小到极少数几个数据颗粒。
        * **与 MySQL B+树对比**：它是一种为大规模范围扫描优化的**宏观定位**机制，而非 MySQL B+树那种为点查优化的**精确制导**。其稀疏性带来了极低的存储开销。
    * **跳数索引 (辅助索引)**
        * **原理**：为数据块创建元数据“摘要”（如 `minmax` 值）。查询时，引擎先检查摘要，若数据块不可能满足条件，则直接**“跳过”读取该数据块的真实内容**，进一步减少 I/O。
        * **澄清关键点**：跳数索引作用的列本身无需有序，引擎会检查范围内所有数据块的摘要，独立判断是否跳过。

### 1.3. 计算引擎：榨干硬件性能的艺术

如果说存储和索引解决了“如何少读数据”，那么计算引擎则解决了“如何快算数据”。

* 向量化执行 (SIMD)
    ClickHouse 按“块”处理数据，而非一行一行处理。这使得它可以利用 CPU 的 SIMD（单指令多数据）指令集，用一条指令同时处理多个数据，极大提升了 CPU 的计算效率。
    
    > [!info] SIMD
    > SIMD 的实现方式是通过在 CPU 中增加向量寄存器和支持向量化操作的指令集扩展。例如，一个 128 位的 SIMD 寄存器可以同时存储 4 个 32 位的单精度浮点数。通过一条 SIMD 指令，这 4 个浮点数可以同时进行加法、乘法等操作，实现并行计算。

* **`GROUP BY` 聚合优化：并行、特化与自适应**
    * **并行化聚合**
        通过**两阶段聚合**（预聚合+最终聚合）和 **`Two-level HashMap`** 的引入，解决了高基数分组下的单点瓶颈，实现了从预聚合到最终聚合的全程并行化。
    * **特例化数据结构**：ClickHouse 内置了超过 30 种 HashMap。
        * **`FixedHashMap`**
            当 `GROUP BY` 的键是 `UInt8` 等小范围整数时，它会使用一个简单的**数组**作为哈希表，实现 O(1) 访问，无任何哈希开销。这体现了其“为特定场景提供极致优化”的思路。
        * **智能选择**
            引擎会根据分组键的类型、基数等特征，在运行时自动选择最优的 HashMap 实现。
    * **微观执行优化：“先占位，后填充” **：
        * **原理**：将聚合过程拆分为“为分组键分配内存”和“更新聚合状态”两步。
        * **目的**：将复杂的、带有分支判断的控制逻辑与简单的、可预测的计算逻辑分离。这使得计算阶段的循环极其规整，能有效避免 CPU **分支预测失败**带来的性能惩罚，并为**向量化 (SIMD)** 优化创造了条件。

### 1.4. 分布式能力：从单点强悍到集群协作

#### 1.4.1. 数据类型：为性能而生的精细设计

ClickHouse 提供了丰富的数据类型，其设计的核心思想是**用最紧凑的存储实现最高效的计算**。

| 类型分类      | 具体类型                 | 用途与性能优势                                                                                             |
| :-------- | :------------------- | :-------------------------------------------------------------------------------------------------- |
| **优化类型**  | `LowCardinality(T)`  | **低基数优化**。适用于值重复率极高的列（如：状态、城市、类型）。它通过字典编码将原始值（如字符串）映射为整数存储，极大减少了存储空间，并在分组聚合（GROUP BY）和过滤时带来数倍的性能提升。 |
|           | `Nullable(T)`        | **支持空值**。虽然提供了灵活性，但会带来性能开销，因为它需要一个额外的标记文件来记录哪些值是 NULL。因此，除非业务必须，否则应尽量避免使用，以保持极致性能。                  |
| **高精度类型** | `Decimal(P,S)`       | **高精度小数**。用于存储金额、汇率等需要精确计算的数值，避免了 `Float` 类型可能带来的浮点误差问题。                                            |
| **复杂类型**  | `Array(T)`           | **数组**。可以在一个字段中存储多个同类型的值（如商品标签 `['电子产品', '手机']`）。ClickHouse 提供了 `has()` 等专用函数来高效地查询数组内容。            |
|           | `Nested(k1 T1, ...)` | **嵌套结构**。类似于 JSON 对象数组，可以将多个相关的列组织在一起，保持其行级对应关系。                                                    |

**核心思想**：选择最贴合数据特征的类型，是 ClickHouse 优化的第一步，也是最重要的一步。

#### 1.4.2. 表引擎的多样性与协作机制

表引擎决定了数据的存储方式、并发控制和查询能力。ClickHouse 提供了多种引擎以适应不同场景，其中 **Distributed** 和 **MergeTree** 引擎的协作是其分布式能力的核心。

*   **引擎概览**：
    *   **MergeTree**：核心存储引擎，负责单节点上的数据存储、排序、索引和查询。是绝大多数业务表的最终选择。
    *   **Distributed**：分布式引擎，本身**不存储任何数据**，扮演着**逻辑表**和**请求路由器**的角色。
    *   **Memory**：内存引擎，数据存储在内存中，重启后丢失。适用于临时表或小数据量的高速查询。
    *   **Log**：日志引擎，轻量级存储，无索引，适合写入后很少查询的场景。
*   **Distributed 与 MergeTree 的协作机制**：
    1.  **角色定义**：
        *   `Distributed` 表是面向用户的**统一访问入口**（逻辑层）。
        *   `MergeTree` 表是分布在各个分片节点上的**实际数据存储**（物理层）。
    2.  **数据写入流程**：
        *   当数据写入 `Distributed` 表时，它会根据**分片键**（如 `user_id`）计算出目标分片。
        *   然后，它将数据**路由**到目标分片节点的本地 `MergeTree` 表中进行实际存储。
    3.  **数据查询流程**：
        *   当查询 `Distributed` 表时，它会将查询请求**分发**到集群中所有分片的本地 `MergeTree` 表上。
        *   各个节点并行在本地表上执行查询（预聚合）。
        *   最后，协调节点收集所有分片返回的中间结果，进行最终的合并（全局聚合），然后返回给客户端。

这种协作模式对用户屏蔽了底层的分布式细节，实现了透明的分布式查询。

#### 1.4.3. 分片与副本的具体策略

分片（Shard）和副本（Replica）是 ClickHouse 实现水平扩展和高可用的基石。

*   **分片 (Shard)：为了性能和扩展性**
    *   **目的**：将数据水平切分到不同节点，以实现并行查询，提升整体性能。
    *   **核心**：**分片键**的选择至关重要，需要保证数据均匀分布，避免数据倾斜。
    *   **常见策略**：
        *   **哈希分片**：按分片键的哈希值分配，最常用，能保证数据均匀。
        *   **随机分片**：写入时随机选择一个分片，数据绝对均匀，但无法按键关联查询。
        *   **范围分片**：按分片键的范围分配，适合按时间等范围查询的场景。
*   **副本 (Replica)：为了高可用和容灾**
    *   **目的**：在不同节点上存储相同分片的数据副本，当某个节点故障时，系统仍可对外服务。
    *   **核心**：依赖 **ZooKeeper** 来管理副本元数据、协调写入一致性和执行故障恢复。
    *   **查询负载均衡策略**：当一个分片有多个副本时，ClickHouse 会根据 `load_balancing` 参数选择一个副本进行查询。
        *   `Random` (默认): 优先选择错误最少的副本，然后在健康的副本中随机选择。
        *   `Round Robin`: 在副本之间轮询，均匀分配查询压力。
        *   `Nearest hostname`: 选择与当前节点主机名最相似的副本，以减少网络延迟。

## 2. 最终总结

ClickHouse 的高性能并非偶然，而是其设计哲学——**“专注 OLAP，压榨硬件，极致优化”**——在各个层面的体现。它通过**列式存储**和**数据预排序**奠定 I/O 基础，借助**稀疏索引**和**跳数索引**实现高效数据裁剪，利用**向量化执行**和**高度优化的并行聚合算法**榨干 CPU 性能，并通过**LSM-Tree 思想的写入模型**保障了高吞吐的数据摄入。这一整套紧密耦合、互相成就的设计，共同铸就了其在海量数据实时分析领域的王者地位。

## 3. 存储与计算是否分离的取舍

“高性能”的定义和瓶颈在不同系统中是完全不同的。架构的选择必须服务于系统的核心目标。Pulsar（消息队列）和 ClickHouse（分析型数据库）的目标和工作负载截然相反，因此它们选择了最适合各自场景的、截然相反的架构。Pulsar 计算与存储分离，而 ClickHouse 采取一体化的方式。

### 3.1. Pulsar 的世界：为“数据流动与弹性”而生的计算存储分离

#### 3.1.1. Pulsar 的核心任务是什么？

它的首要任务是作为一个**高可靠、高吞吐的数据管道**。它需要做到：

1.  **可靠地接收**海量生产者发来的消息。
2.  **持久化地存储**这些消息，确保不丢失。
3.  **高效地推送**给成千上万的消费者。
4.  在面对突发流量时，能够**灵活、快速地扩缩容**。

#### 3.1.2. 为什么“计算存储分离”对 Pulsar 来说是高性能？

Pulsar 的架构分为两层：

* **Broker（计算层）**：无状态，负责处理客户端连接、消息分发、负载均衡等逻辑。
* **BookKeeper（存储层）**：有状态，由多个 Bookie 节点组成，负责消息的持久化存储。

这种分离带来了决定性的优势：

1.  **极致的弹性 (Elasticity)**：这是 Pulsar 设计的王牌。
    * 如果你的瓶颈是**消费者太多**，导致 Broker 的 CPU 和网络不堪重负，你可以**只扩容 Broker**，几秒钟内新的 Broker 就能上线服务。
    * 如果你的瓶颈是**消息积压太多**，需要更大的存储容量和写入吞吐，你可以**只扩容 BookKeeper**。
    * 这种**独立伸缩**的能力，使得资源利用率最大化，运维成本极低，这对于云原生和流量波动大的场景至关重要。

2.  **高可用与快速恢复**：
    * 由于 Broker 是无状态的，任何一个 Broker 宕机，客户端可以立即连接到另一个 Broker，服务几乎不中断。因为真正的“状态”（消息数据）安全地存放在 BookKeeper 集群中。

3.  **数据分层存储**：
    * 分离架构使得 Pulsar 可以轻松实现 tiered storage（分层存储）。热数据（最近几小时）存在高性能的 BookKeeper 中，而冷数据（几天前）可以自动卸载到更便宜的对象存储（如 S3）中，极大地降低了存储成本。

**Pulsar 的性能瓶颈**：对于消息队列来说，瓶颈往往在于**连接数、并发处理能力和应对流量洪峰的弹性**。计算存储分离完美地解决了这些问题。它牺牲了一点点网络延迟（Broker 读写 BookKeeper 需要网络通信），但换来了无与伦比的弹性和可用性。

### 3.2. ClickHouse 的世界：为“数据分析与扫描”而生的计算存储一体化

#### 3.2.1. ClickHouse 的核心任务是什么？

它的首要任务是**在海量数据上，以最快的速度完成复杂的分析查询（OLAP）**。它需要做到：

1.  一次查询扫描数亿甚至数十亿行数据。
2.  进行大规模的聚合、排序、过滤操作。
3.  将查询延迟控制在亚秒或秒级。

#### 3.2.2. 为什么“计算存储一体化”对 ClickHouse 来说是高性能？

ClickHouse 的 MPP 架构中，每个节点都同时拥有 CPU（计算）和本地磁盘（存储）。

这种一体化设计带来了决定性的优势：

1.  **极致的 I/O 性能 (Data Locality)**：这是 ClickHouse 性能的命脉。
    * 当一个 ClickHouse 节点执行查询时，它直接从**本地挂载的 SSD** 上读取数据。这个速度比通过网络从远程存储读取数据要**快几个数量级**。
    * 对于 OLAP 查询来说，最大的瓶颈就是**数据扫描的 I/O 吞吐**。将网络这个最大的延迟源彻底消除，是实现亚秒级查询的关键。

2.  **存储感知的计算优化**：
    * 因为计算引擎“知道”数据在自己本地磁盘上的物理布局（哪个文件、哪个偏移量、如何排序、稀疏索引在哪里），所以它可以做出极致的底层优化，只读取绝对必要的数据字节。
    * 如果存储是分离的，计算引擎对数据的物理细节一无所知，只能发起较粗粒度的读请求，很多优化都无从谈起。

**ClickHouse 的性能瓶颈**：对于分析型数据库来说，瓶颈在于**海量数据的扫描速度和 CPU 的计算效率**。计算存储一体化通过最大化数据本地性，完美地解决了 I/O 瓶颈。

### 3.3. 总结与类比

| | **Pulsar (消息队列)** | **ClickHouse (分析数据库)** |
| :--- | :--- | :--- |
| **核心目标** | 数据的可靠**流动**与**弹性伸缩** | 数据的极速**扫描**与**分析计算** |
| **架构原则** | **计算存储分离** | **计算存储一体化** |
| **解决的关键瓶颈** | 应对流量波动、独立扩缩容、快速故障恢复 | 海量数据扫描的 I/O 延迟 |
| **高性能的体现** | 高吞吐、低消息延迟、高弹性、高可用 | 极低的复杂查询延迟 (Query Latency) |
| **主要取舍** | 牺牲微小的网络延迟，换取极致的弹性和可用性 | 牺牲架构的弹性，换取极致的 I/O 性能和数据本地性 |

**一个绝佳的类比：**

* **Pulsar 就像一个现代化的“中央仓储与物流系统”**。
    * **BookKeeper** 是巨大的、可无限扩展的自动化仓库。
    * **Broker** 是庞大的、可随时增减的快递车队。
    * 仓库和车队可以独立扩建，系统总能高效、可靠地完成货物的收发。它的“高性能”体现在**物流效率**上。
* **ClickHouse 就像一家“米其林三星餐厅的后厨”**。
    * 每个**厨师（计算）**身边都必须有自己的**食材柜（本地存储）**，里面放着处理好的、摆放有序的食材。
    * 为了做出最快的上菜速度，厨师不可能每炒一个菜都派人去几公里外的中央仓库取葱姜蒜。所有东西必须触手可及。它的“高性能”体现在**烹饪速度**上。

因此，Pulsar 和 ClickHouse 都没有错。它们都基于自己要解决的核心问题，选择了最优的、但截然相反的架构，并都实现了各自领域内的“高性能”。