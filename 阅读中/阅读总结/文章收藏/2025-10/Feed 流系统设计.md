---
source: "[[10 亿用户微博 Feed 流如何设计？ 鹿晗式 热点  100WQPS 如何 抵抗雪崩  ？]]"
create: 2025-10-07
---

## 1. Feed 流系统深度解析

### 1.1. 第一部分：基础概念与分类

#### 1.1.1. 什么是 Feed 流？

Feed 流是一个持续更新并展示给用户的信息流。它将用户主动或被动订阅的若干消息源组合在一起，形成一个个性化的内容聚合器。其核心作用是**信息聚合**，将用户可能感兴趣的信息以“流”的形式推送给用户，用户只需通过简单的滑动操作即可持续获取信息，实现了从“人找信息”到“信息找人”的转变。

#### 1.1.2. Feed 流的核心术语

| 名称 | 说明 | 备注 |
| :--- | :--- | :--- |
| **Feed** | Feed 流中的每一条消息或状态。例如，朋友圈的一个动态、微博的一条内容。 | - |
| **Timeline** | 一种按时间顺序展示的 Feed 流，由于其最为经典和广泛，有时也泛指 Feed 流。 | 时间轴 |
| **发件箱 (Outbox)** | 用户**自己**发布过的 Feed 消息集合，对应其个人主页。 | Personal Timeline |
| **收件箱 (Inbox)** | 系统为用户聚合的、来自其所有订阅源的 Feed 消息集合，对应其首页信息流。 | Home Timeline |
| **写扩散 (Push Model)** | 用户发布 Feed 后，系统**主动**将该 Feed 推送（写入）到其所有粉丝的“收件箱”中。 | 为读优化 |
| **读扩散 (Pull Model)** | 用户刷新信息流时，系统**临时**从其所有关注者的“发件箱”中拉取最新的 Feed，聚合后展示。 | 为写优化 |

#### 1.1.3. Feed 流的分类

我们可以从“信息源聚合依据”和“排序依据”两个维度对主流 Feed 流进行划分：

| 信息源聚合依据 \ 排序依据 | **权重推荐 (Rank 模式)** | **时间顺序 (Timeline 模式)** |
| :--- | :--- | :--- |
| **无需依赖关系** (隐含兴趣) | 抖音推荐页 | - |
| **单向依赖关系** (关注) | - | 微博关注页 |
| **双向依赖关系** (好友) | - | 微信朋友圈 |

这两种模式的核心挑战也截然不同：

* **Rank 模式**：核心挑战在于**算法模型的实时性与效果**，需要强大的数据与算法平台支撑。
* **Timeline 模式**：核心挑战在于**数据存储与分发的可扩展性**，如何处理海量数据的写入、存储和快速读取，是后端与分布式系统的重点。


### 1.2. 第二部分：三大架构模式深度剖析

#### 1.2.1. 推模式 (Write Diffusion) - 为读优化

* **深层原理**: 一种**空间换时间**的策略。通过为每个用户预计算并存储一份个性化的收件箱副本，将读取时的复杂计算（聚合、排序）前置到写入时完成。
* **详细工作流**:
    1. **API 接收**: 用户发布 Feed。
    2. **数据持久化**: Feed 内容写入 `t_feed` 表，获得 `FeedID`，存入**发件箱**。
    3. **内容缓存**: Feed 详细信息缓存至 Redis。
    4. **触发异步任务**: 向消息队列（如 Kafka）发送“新 Feed 发布”消息。
    5. **粉丝分发 (Fan-out)**:
        * 消费者获取消息，拿到发布者 `UserID` 和 `FeedID`。
        * 获取粉丝列表。**优化点**：可过滤掉超过 30 天未登录的**不活跃粉丝**，不对其推送。
        * 将 `FeedID` 批量 `ZADD` 到每个活跃粉丝的**收件箱**缓存 (`feed:timeline:{fanID}`) 中。
* **瓶颈与挑战**:
    * **推送风暴**: 大 V 发布内容，导致 MQ 瞬间积压，消费者和 Redis 负载飙升。
    * **存储成本高**: 千万级粉丝的大 V 发布一条 Feed，会产生千万次的写操作和数据冗余。
    * **删除/更新成本高**: 删除一条 Feed 需要逆向操作，从所有粉丝的收件箱中移除，成本巨大。

#### 1.2.2. 拉模式 (Read Diffusion) - 为写优化

* **深层原理**: 一种**计算换空间**的策略。系统只存储一份原始数据（发件箱），读取时通过实时计算生成用户的收件箱。
* **详细工作流**:
    1. **API 接收**: 用户请求刷新 Feed 流。
    2. **获取关注列表**: 从缓存中获取用户关注的列表。
    3. **并行拉取**: 并行地从所有关注者的**发件箱**缓存 (`user:outbox:{followedUserID}`) 中拉取最新的 N 条 `FeedID`。
    4. **内存中聚合排序**: 通过**多路归并排序**算法，将多个有序列表合并成一个全局有序的最终列表。这是拉模式中最消耗 CPU 的步骤。
    5. **分页与内容获取**: 对排序后的总列表进行分页，并批量获取 Feed 详细内容。
* **瓶颈与挑战**:
    * **拉取风暴**: 热点事件导致大量用户同时刷新，对少数大 V 的发件箱形成**读热点**。
    * **读取延迟高**: 涉及多次网络往返和密集的 CPU 计算，P99 延迟高。
    * **“关注狂人”问题**: 用户关注数千人，单次拉取聚合的计算量巨大。

#### 1.2.3. 推拉结合模式 - 务实的妥协

* **核心决策**: 以粉丝数（如 100 万）为阈值，划分普通用户和大 V。
    * **对普通用户**: 采用**推模式**，其粉丝能快速收到更新。
    * **对大 V**: 采用**拉模式**，粉丝在刷新时主动拉取其内容。
* **详细读取流程**:
    1. **获取两部分数据**:
        * **推模式数据**: 直接从用户自己的**收件箱** (`feed:timeline:push:{UserID}`) 获取。
        * **拉模式数据**: 执行“拉模式”流程，获取所有关注的大 V 的内容。
    2. **最终聚合**: 将上述两个有序列表再次进行**归并排序**，生成最终 Feed 流。
    3. **前端体验优化**: 可采用**异步加载**，首屏先快速返回推模式数据，同时后台异步加载拉模式数据，完成后动态插入信息流。

### 1.3. 第三部分：关键实现细节与优化

#### 1.3.1. 删除与更新的处理：软删除 + 懒删除

直接扩散删除/更新操作的成本极高。更优的方案是**读时检查**：

1. **软删除**: 当发布者删除一条 Feed 时，仅在消息主表中将其状态位标记为 `deleted`，不进行任何扩散操作。
2. **读时过滤**: 当粉丝拉取 Feed 流时，在通过 `FeedID` 获取内容详情后，检查其状态。如果状态为 `deleted`，则不展示给用户。
3. **懒删除**: 在发现某条 Feed 已被删除后，**顺便**从当前用户的收件箱缓存（Redis ZSET）中将该 `FeedID` 移除。这是一种被动的、逐步清理缓存的机制，避免了昂贵的写扩散。

#### 1.3.2. 分页机制：游标分页 (Cursor Pagination)

Feed 流是动态列表，使用传统的 `page_num` 和 `page_size` 会因内容更新导致数据错位或重复。正确的做法是使用**游标分页**：

* **请求**: 客户端请求下一页时，需带上上一页**最后一条 Feed 的 ID 或时间戳**作为 `cursor`。
* **处理**: 服务端根据 `cursor` 定位，查询“时间戳小于 `cursor` 的 N 条数据”，从而确保分页的连续性。

#### 1.3.3. 核心数据模型

* **消息表 (t_feed)**: 存储 Feed 的核心内容。

| 字段名           | 字段说明        | 备注                                  |
| :------------ | :---------- | :---------------------------------- |
| `msg_id`      | 消息 ID (主键)  |                                     |
| `sender_id`   | 发布者 ID      |                                     |
| `msg_content` | 消息内容 (JSON) |                                     |
| `msg_status`  | 消息状态        | 用于软删除 (e.g., 0: normal, 1: deleted) |
| `ctime`       | 发布时间        | 用于排序和分页游标                           |

* **关注关系表 (t_follow)**: 存储用户间的关注关系。

| 字段名 | 字段说明 | 备注 |
| :--- | :--- | :--- |
| `user_id` | 粉丝 ID | |
| `followed_id` | 被关注者 ID | |
| `status` | 关系状态 | (e.g., 1: followed, 0: unfollowed) |

* **收件箱缓存 (Redis ZSET)**:
    * **Key**: `inbox:{UserID}`
    * **Score**: `timestamp` (Feed 发布的时间戳)
    * **Value**: `FeedID`
    * 这个 ZSET 结构天然支持按时间排序和基于 Score 的游标分页。

---

### 1.4. 第四部分：缓存“大 Key”问题的根治方案

“大 Key”（如大 V 的粉丝列表、用户的收件箱）是 Redis 运维的噩梦。

#### 1.4.1. 方案一：按固定长度裁剪 (有损)

* **实现**: `ZADD` 后，执行 `ZREMRANGEBYRANK` 保留最新的 N 个（如 2000 个）元素。
* **分析**: 简单有效，但历史数据需回源数据库，查询慢，影响体验。

#### 1.4.2. 方案二：按时间窗口分片 (无损)

* **实现**: 将一个用户的收件箱按月或周分片存储在不同的 Key 中，如 `inbox:{UserID}:202410`, `inbox:{UserID}:202409`。
* **读逻辑**: 读取时，从当前月份的 Key 开始。当一个 Key 的数据读完后，根据分页游标中的时间信息，自动切换到上一个月份的 Key 继续读取，实现无缝的无限滚动。
* **优化**: 结合**空缓存**（缓存不存在的月份，避免穿透）和**元数据记录**（记录用户第一条 Feed 的时间，避免查询更早的空数据），可以做到极致优化。

---

### 1.5. 第五部分：100W QPS 雪崩防御体系

这是一个多层次、纵深防御的体系，目标是在灾难发生时，通过层层削减和隔离，保证核心服务的存活。

1. **入口层 (API Gateway)**: **流量整形与调度**
    * 使用**令牌桶/漏桶**算法削峰填谷。
    * 设置**请求排队**，用短暂延迟换取成功率。

2. **缓存层**: **防止失效连锁反应**
    * **热点数据双 Key 冗余**，主 Key 失效时备用 Key 顶上。
    * **互斥锁**防止缓存击穿，确保只有一个请求回源。

3. **存储层 (DB)**: **最后的壁垒**
    * **连接池隔离** (舱壁模式)，不同业务（读/写 Feed）使用不同连接池，避免相互影响。
    * **SQL 熔断**，当某个 SQL 响应慢或错误率高时，快速失败或返回降级数据，保护数据库。

4. **计算/消息层**: **自我调节**
    * **背压机制**，消费者处理不过来时，反向通知生产者降速。
    * **线程池隔离**，执行推/拉模式、API 请求的线程池相互隔离。

5. **降级预案**: **有计划地放弃，保全大局**
    * 通过**分布式配置中心**设置降级开关。
    * **L1 降级**: 关闭非核心功能（如推荐）。
    * **L2 降级**: 关闭部分核心功能（如拉模式，只看推模式内容）。
    * **L3 降级**: 开启只读模式，关闭所有写操作。
    * **L4 降级**: 返回静态缓存或兜底页。