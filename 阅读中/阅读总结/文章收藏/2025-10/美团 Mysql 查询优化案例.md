---
source:
  - "[[MySQL 索引原理及慢查询优化 - 美团技术团队]]"
create: 2025-10-20
---
## 第一部分：理论基石 - 为什么需要索引以及索引是什么？

这部分是理解一切优化的基础。

### 1. 核心痛点：磁盘 I/O

* **本质问题：** 数据库的数据存储在磁盘上，而磁盘的机械运动（寻道、旋转）速度远慢于内存的电信号速度。一次磁盘 I/O 的时间成本大约是内存访问的十万倍。
* **性能瓶颈：** 如果没有索引，查询数据（如 `WHERE id = 100`）可能需要遍历整张表，数据量越大，磁盘 I/O 次数越多，查询就越慢。
* **操作系统优化：** 操作系统通过“预读”机制，每次 I/O 会读取一整页（Page，通常为 4k 或 8k）的数据到内存，而不是只读取你需要的那一条。这个特性是索引结构设计的基础。

### 2. 解决方案：B+ 树

* **设计目标：** 找到一种数据结构，能将查找数据时的磁盘 I/O 次数控制在一个极小的、可预测的常数级别。
* **B+ 树结构特点：**
    * **多路平衡搜索树：** 每个节点可以有很多个子节点，这使得树的高度非常低。
    * **数据只在叶子节点：** 非叶子节点（内层节点）只存储用于指引方向的键值和指针，不存储完整的行数据。这使得每个磁盘块（页）能容纳更多的键值，进一步降低树的高度。
    * **叶子节点形成链表：** 叶子节点之间通过指针相连，便于进行范围查询。
* **查找过程：** 从根节点开始，通过一次磁盘 I/O 加载一个节点到内存，在内存中通过二分查找确定下一跳的指针，再通过该指针地址进行下一次磁盘 I/O。通常，一个三到四层的 B+ 树就能支撑千万级别的数据量，意味着查询只需要 3-4 次磁盘 I/O。

### 3. B+ 树的关键性质 (重点复习)

1. **索引字段越小，树的高度越低：** 因为一个磁盘页的大小是固定的，索引字段越小（如 `INT` vs `BIGINT`），一个页能存放的索引项就越多，树就越“矮胖”，查询所需的 I/O 次数就越少。
2. **最左前缀匹配特性：** 对于联合索引 `(a, b, c)`，B+ 树在构建时是严格按照 `a`、`b`、`c` 的顺序进行排序的。
    * **有效查询：** `WHERE a=1`、`WHERE a=1 AND b=2`、`WHERE a=1 AND b=2 AND c=3`。
    * **无效查询：** `WHERE b=2` 或 `WHERE c=3`。因为没有 `a` 作为第一层比较因子，数据库不知道从何查起。
    * **部分有效查询：** `WHERE a=1 AND c=3`。只能用到 `a` 的索引，在找到所有 `a=1` 的数据后，再逐条过滤 `c=3` 的记录。

---

## 第二部分：实践方法论 - 如何优化？

这部分是你可以直接应用的行动指南。

### 建立索引的五大黄金原则

1. **最左前缀匹配原则：** 联合索引必须从左到右使用。特别注意，MySQL 会一直向右匹配直到遇到**范围查询**（`>`、`<`、`BETWEEN`、`LIKE`）就停止。例如，索引 `(a, b, c, d)`，查询 `a=1 AND b=2 AND c>3 AND d=4`，索引只能用到 `a, b, c`，`d` 是用不到的。
2. **`=` 和 `in` 可以乱序：** 查询 `a=1 AND c=3 AND b=2`，如果索引是 `(a, b, c)`，MySQL 的查询优化器会自动帮你调整顺序，使索引生效。
3. **选择高区分度的列：** `区分度 = count(distinct col) / count(*)`。值越接近 1 越好。像性别、状态这类字段，区分度极低，建立单列索引效果很差。
4. **索引列不能参与计算：** `WHERE from_unixtime(create_time) = '2024-01-01'` 会导致索引失效。因为 B+ 树里存的是原始的 `create_time` 值，无法直接与计算后的结果比较。应改写为 `WHERE create_time = unix_timestamp('2024-01-01')`。
5. **尽量的扩展索引，不要新建：** 如果已有索引 `(a)`，现在需要 `(a, b)`，应该修改原索引为 `(a, b)`，而不是新建一个。

---

## 第三部分：案例深度剖析 (重点复押)

这是文章的精华，展示了理论在复杂现实中的应用。

### 案例一：复杂语句的逻辑优化

* **问题 SQL:** 一个复杂的 `INNER JOIN`，其 `ON` 条件中包含了 `OR`，连接了一个子查询。
* **问题分析:**
    1. `EXPLAIN` 结果显示，MySQL 先执行了 `id=2` 的 `DERIVED`（派生）查询，这个子查询 `LEFT JOIN` 了两张大表，生成了一个包含 63727 条记录的**巨大中间结果集**。
    2. 然后，MySQL 再用这个巨大的中间结果集去和 `cm_log` 表（只有 379 条记录）进行 `JOIN`。
    3. **核心问题：** 执行顺序错误，先做了大量无用功（生成了大部分最终会被丢弃的数据），再进行过滤。
* **优化思路:**
    * **逻辑重构：** 分析 `OR` 条件的业务含义，发现它其实是两种不同的关联情况。
    * **拆分与合并：** 将原 SQL 拆分成两个独立的、逻辑清晰的 `SELECT` 语句，每个语句只处理一种关联情况。然后用 `UNION` 将两个结果集合并。（`UNION` 自带去重，对应原 SQL 的 `DISTINCT`）。
* **核心启示 (笔记重点):**
    > **SQL 逻辑重构的威力远大于单纯加索引。** 当遇到复杂的 `JOIN` 和 `OR` 条件时，应首先思考是否能将查询拆解成更简单、更直接的多个查询，再将结果合并。这通常能避免数据库生成不必要的巨大中间表。

### 案例二：颠覆常规的场景化优化

* **问题 SQL:** 一个简单的 `SELECT`，`WHERE` 条件是 `accurate_result=1 AND (sync_status=0 OR sync_status=2 OR sync_status=4)`。
* **问题分析:**
    1. `EXPLAIN` 显示全表扫描 (`type=ALL`)，扫描了 361 万行，非常慢。
    2. 分析 `accurate_result` 和 `sync_status` 两个字段，发现它们的**区分度都极低**（只有两三个不同的值）。
    3. **常规理论陷阱：** 根据“选择高区分度列建索引”的原则，这两个字段似乎都不适合建索引。
* **优化思路 (关键转折):**
    * **了解业务场景：** 通过与业务方沟通得知，这个 SQL 是一个定时任务，每 5 分钟执行一次，用于处理少量（约 1000 条）的待同步数据。处理完后，`sync_status` 的值会改变。
    * **数据分布不均：** 这意味着，虽然 `sync_status` 的值种类少，但在任何一个时间点，符合条件（如 `sync_status=0`）的数据量其实非常小，而绝大部分数据的状态是“已处理”。
    * **场景化建索引：** 基于这个**数据分布极不均衡**的特点，为 `(accurate_result, sync_status)` 建立联合索引是高效的。索引可以快速过滤掉那几百万条“已处理”的数据。
* **核心启示 (笔记重点):**
    > **索引的有效性最终取决于实际的查询场景和数据分布，而非孤立的理论指标。** “低区分度不建索引”是一般性建议，但在数据分布严重倾斜的特定业务场景下，它可能成为性能优化的关键。**永远不要脱离业务去谈优化。**

### 案例三：`ORDER BY` + `LIMIT` 的优化陷阱

* **问题 SQL:** 一个多表 `JOIN` 查询，最后带有 `ORDER BY c.created_time DESC LIMIT 10`。
* **问题分析:**
    1. 初看 `EXPLAIN`，`rows` 都不大，似乎没有问题。但查询耗时 13 秒。
    2. **揭示真相：** 去掉 `ORDER BY` 和 `LIMIT`，执行 `COUNT(*)`，发现中间结果集高达 77 万条。
    3. **核心问题：** MySQL 需要先将这 77 万条记录全部 `JOIN` 出来，然后在临时表中进行**文件排序 (`filesort`)**，最后再取出前 10 条。排序这 77 万条记录是性能灾难。
* **优化思路 (初次尝试与失败):**
    * **颠倒执行顺序：** 能否先利用 `contact` 表的 `created_time` 索引，快速找出最新的 10 条记录，然后再去 `JOIN` 其他表进行条件过滤？这通过 `EXISTS` 子查询实现。
    * **优化陷阱：** 这个思路在“理想情况”下很快。但如果 `contact` 表最新的记录在 `JOIN` 时被过滤掉了（不满足 `EXISTS` 子查询的条件），MySQL 会继续从 `contact` 表取下一批记录，再进行 `JOIN` 判断，如此循环。
    * **极端情况：** 如果 `contact` 表最新的几十万条记录都无法满足 `JOIN` 条件，MySQL 会近乎遍历整张表，性能比优化前更差（案例中耗时超过 2 分钟）。
* **核心启示 (笔记重点):**
    > 1. **警惕 `ORDER BY ... LIMIT` 查询中的隐式 `filesort`**，它可能导致对巨大结果集的排序。
    > 2. 看似巧妙的优化（如颠倒 JOIN 和 ORDER BY 的顺序）可能存在**极端情况下的性能陷阱**，必须充分测试各种数据分布。
    > 3. **并非所有 SQL 都能在数据库层面完美优化。** 如此例，最优解可能是调整应用层逻辑，例如，不允许进行如此复杂的跨表排序查询，或者通过其他方式缓存/预计算结果。

---

## 总结与升华：超越 SQL 本身

文章最后指出，**任何数据库层面的优化都抵不上应用系统的优化**。这意味着，作为开发者，我们不仅要会写高效的 SQL，更要从业务和架构层面进行合理的设计，从源头上避免产生难以优化的“垃圾 SQL”。