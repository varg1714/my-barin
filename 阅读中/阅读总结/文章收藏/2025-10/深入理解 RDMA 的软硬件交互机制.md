---
source: "[[阅读中/文章列表/文章收藏/2025-09/深入理解 RDMA 的软硬件交互机制]]"
create: 2025-10-09
---

## 1. 学习笔记：深入理解 RDMA 的软硬件交互机制

### 1.1. RDMA 是什么？为什么需要它？

**RDMA (Remote Direct Memory Access)**，即远程直接内存访问。它的核心目标是解决网络传输中，服务器端处理数据时的延迟问题。

在数据中心，网卡速度（100G/200G/400G）的发展已远超 CPU，传统的网络处理方式成为了性能瓶瓶颈。RDMA 通过绕过操作系统内核，让应用程序可以直接与网卡硬件通信，从而实现超低延迟和超高吞吐。

#### 1.1.1. 对比：传统 Kernel TCP vs. RDMA

##### 1.1.1.1. 传统 Kernel TCP 接收流程（痛点分析）

1. **硬件操作**：网卡收到数据，通过 DMA 写入内核分配的内存（dma buffer）。
2. **中断**：网卡产生中断，通知 CPU。
3. **内核处理**：
    * 发生**中断上下文切换**。
    * 驱动将数据交给内核协议栈处理（TCP/IP 拆包、校验等）。
    * 内核通知用户程序有数据可读。
4. **用户程序操作**：
    * 用户程序发起 `read()` 系统调用，发生**用户态/内核态上下文切换**。
    * 内核将数据从内核内存**拷贝**到用户程序的内存中。
    * 系统调用返回，再次发生**用户态/内核态上下文切换**。

**痛点总结**：存在多次**上下文切换**和至少一次**内存拷贝**，CPU 大量时间被消耗在数据搬运和协议处理上，而非业务逻辑。

##### 1.1.1.2. RDMA 接收流程（优势体现）

1. **用户程序准备**：用户程序自己分配好内存（buffer），并提前告知网卡。
2. **硬件操作**：网卡收到数据，直接通过 DMA 写入用户程序指定的内存中。
3. **完成通知**：网卡在完成队列（CQ）中放置一个完成事件。
4. **用户程序处理**：用户程序通过**轮询 (Polling)** 的方式检查完成队列，发现新事件后，直接处理内存中的数据。

**优势总结**：**零拷贝**、**无上下文切换**、**内核旁路 (Kernel Bypass)**。CPU 被解放出来，专注于业务计算。

### 1.2. 核心机制一：内存管理 (Memory Management)

RDMA 允许硬件直接读写用户内存，这带来了三个严峻挑战：

1. **安全问题**：如何防止程序利用网卡读写任意物理内存？
2. **地址映射问题**：程序使用虚拟地址，而硬件 DMA 需要物理地址，谁来翻译？
3. **地址稳定性问题**：操作系统可能会移动内存（Swap、压缩），如何保证网卡访问的地址一直有效？

RDMA 通过 **PD (Protection Domain)** 和 **MR (Memory Registration)** 两个概念来解决。

#### 1.2.1. PD (Protection Domain - 保护域)

* **比喻**：一个“**安全沙箱**”或“**租户 ID**”。
* **作用**：所有 RDMA 资源（如内存区域、队列对）都必须归属于一个 PD。不同 PD 之间的资源是隔离的，无法交叉访问。这从机制上保证了一个进程无法通过 RDMA 操作另一个进程的资源，解决了跨进程的**安全问题**。

#### 1.2.2. MR (Memory Registration - 内存注册)

* **比喻**：一个“**授权和地址锁定**”的过程。
* **作用**：在使用一块内存进行 RDMA 操作前，必须先对其进行“注册”（调用 `ibv_reg_mr`）。这个过程是解决地址映射和稳定性问题的关键。

**内存注册（`ibv_reg_mr`）的背后逻辑：**

1. **锁定物理地址 (Pinning)**：驱动程序会调用内核的 `pin_user_pages_fast` 函数，通知操作系统：“这块内存要给硬件用了，请**不要移动它，也不要把它交换到硬盘上**”。这保证了**地址的稳定性**。
2. **获取物理地址**：驱动查询页表，获得这块虚拟内存对应的真实物理地址列表。
3. **在网卡上创建映射**：驱动将“虚拟地址 -> 物理地址”的映射关系发送给网卡。网卡内部的**TPT (Translation and Protection Table)** 会记录这个映射。
4. **生成并返回 `mkey`**：网卡为这条映射记录生成一个句柄（Handle），称为 **Memory Key (`mkey`)**，并将其返回给用户程序。`mkey` 分为 `lkey` (本地访问) 和 `rkey` (远程访问)。

**总结**：注册完成后，用户程序不再直接使用内存地址与网卡交互，而是使用 `mkey` 作为凭证。网卡收到 `mkey` 后，在自己的内部表中查询，即可获得权限和真实的物理地址去执行 DMA。

### 1.3. 核心机制二：软硬件交互 (Software-Hardware Interaction)

软硬件交互的基础是 **Work Queue (工作队列)**，这是一个单生产者、单消费者的环形队列。

* **比喻**：软件和硬件之间的“**共享任务清单**”。
* **主要队列**：
    * **SQ (Send Queue)**: 发送队列，程序往里放“发送任务”。
    * **RQ (Receive Queue)**: 接收队列，程序往里放“空的接收缓冲区”。
    * **CQ (Completion Queue)**: 完成队列，硬件往里放“任务完成回执”。

#### 1.3.1. 发送流程 (Send Flow)

1. **构造 WQE (Work Queue Element)**
    * 程序填写一张“发货单”（`ibv_send_wr` 结构体），内容包括：操作类型、数据长度、以及最重要的**`lkey`**（内存凭证）和任务 ID `wr_id`。
2. **提交 WQE 到 SQ**
    * 程序将这张“发货单”放入 SQ 这个环形队列中。
3. **敲门铃 (Ring Doorbell)**
    * 这是触发硬件的关键一步。Doorbell 是一个特殊的**硬件寄存器地址**，它通过 **MMIO (Memory-Mapped I/O)** 映射到了用户程序的地址空间。
    * 程序向这个地址执行一次写操作。这个写操作会被 CPU 通过 PCIe 总线直接发送给网卡。
    * 网卡硬件监测到这个地址被写入，就知道“来活儿了”，便主动去 SQ 中拉取新的 WQE。
4. **硬件处理**
    * 网卡解析 WQE，提取 `lkey`，在内部 TPT 中查询到物理地址。
    * 启动 DMA 引擎，从主内存抓取数据到网卡。
    * 将数据打包发送出去。
5. **生成 CQE (Completion Queue Element)**
    * 数据发送完成并收到对端确认后，网卡会生成一个“完成回执”（CQE），其中包含原始的 `wr_id`。
    * 网卡将 CQE 放入 CQ 中。
6. **软件轮询 (Polling)**
    * 程序通过 `ibv_poll_cq` 不断检查 CQ。
    * 发现新的 CQE 后，根据 `wr_id` 确认是哪个发送任务已完成。此时，程序才知道用于发送的内存可以被安全地重用了。

#### 1.3.2. 接收流程 (Receive Flow)

解决了“不知道会收到多大数据”的难题。

* **核心思想**：**预投递接收请求 (Pre-posting Receive Buffers)**。
* **比喻**：提前准备好一堆“**空篮子**”，并把它们排队放在传送带上，等待网卡来填充。

1. **准备“空篮子”**
    * 程序一次性分配一个包含多个**固定大小**缓冲区的**内存池**，并对整个池进行**内存注册 (MR)**。
2. **投递“空篮子”到 RQ**
    * 程序为每个空闲的缓冲区创建一个接收 WQE（相当于给篮子贴上标签，写明地址和 `lkey`），然后通过 `ibv_post_recv` 将它们批量投递到**接收队列 (RQ)**。
3. **网卡填充“篮子”**
    * 当数据到达时，网卡从 RQ 队头取下一个 WQE（一个空篮子）。
    * 通过 DMA 将网络数据直接写入该 WQE 指向的内存缓冲区。
4. **网卡通知**
    * 填充完毕后，网卡在 CQ 中生成一个 CQE。这个 CQE 除了包含 `wr_id`（告知哪个篮子被用了），还包含一个关键字段 `byte_len`，**精确告知实际接收了多少字节的数据**。
5. **程序处理与回收**
    * 程序轮询 CQ，得知某个缓冲区已填充数据。
    * 根据 `byte_len` 处理有效数据。
    * **关键一步**：处理完毕后，程序必须**将这个“空篮子”重新投递回 RQ**，以供下次接收使用。否则 RQ 中的可用缓冲区会耗尽，导致丢包。

### 1.4. 总结

RDMA 通过将内存管理、协议处理等任务从内核转移到用户态和硬件，实现了极致的性能。但它并非银弹，其代价是：

* **编程复杂**：资源管理责任完全落在开发者身上，如手动管理缓冲区生命周期。
* **调试困难**：问题可能出在软件逻辑、硬件配置或两者交互上。
* **硬件依赖重**：需要专门的 RDMA 网卡支持。

因此，RDMA 特别适用于对延迟和吞吐要求极高的**I/O 密集型业务**（如高性能存储、AI/HPC 集群），而对于计算密集型或追求硬件无关的业务，传统 TCP 依然是合适的选择。