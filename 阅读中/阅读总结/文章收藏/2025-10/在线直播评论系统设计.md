---
source: "[[面试官：如何设计一个百万用户同时在线的直播评论系统？]]"
create: 2025-10-05
---

## 1. 第一部分：需求定义与基础设计 (The Foundation)

任何复杂的系统都始于清晰的边界界定。

### 1.1. 需求分析 (Requirement Analysis)

- **功能需求**:
    - **核心**: 发布评论、实时查看、历史追溯。
    - **扩展**: 楼中楼回复、点赞、表情回应等（初期可暂不考虑，但架构应有扩展性）。
- **非功能需求 (决定架构天花板的关键)**:
    - **高并发**: 支持百万级用户同时在线，意味着百万级的长连接。
    - **高吞吐**: 承受每秒数千甚至数万条评论的写入与广播。
    - **低延迟**: 评论从发布到显示，端到端延迟应在毫秒级（目标 < 200ms）。
    - **高可用**: 系统无单点故障，能容忍部分组件失效，核心功能不受影响。
    - **高扩展性**: 系统各组件均可独立水平扩展，以应对业务增长。

### 1.2. 核心实体与接口设计 (Entities & API Design)

- **核心实体**: `User` (用户), `LiveVideo` (直播), `Comment` (评论)。
- **接口设计 (API)**:
    1. **发布评论**: `POST /comments/:liveVideoId`
        - **安全细节**: `userId` **必须**从请求头中的 `Authorization` (如 JWT) 解析，而非由客户端在请求体中传递，以防身份伪造。
        - **请求体**: `{ "message": "This is a comment!" }`
    2. **获取历史评论**: `GET /comments/:liveVideoId?cursor={last_comment_id}&pageSize=10`
        - **分页方案**:
            - **偏移量分页 (Offset Pagination)**: `LIMIT/OFFSET`。**坚决不使用**。在高速写入场景下，会导致数据重复或遗漏，且深度分页性能极差（数据库需扫描 `offset` 条记录）。
            - **游标分页 (Cursor Pagination)**: **推荐使用**。`cursor` 指向上一页最后一条记录的唯一标识（如 `comment_id` 或 `timestamp`）。查询条件为 `WHERE id < cursor`，性能稳定，数据准确。
            - **示例 (SQL-like)**: `SELECT * FROM comments WHERE liveVideoId = ? AND commentId < ? ORDER BY commentId DESC LIMIT 10;`

---

## 2. 第二部分：架构演进的核心路径 (The Evolution Path)

> “好的架构不是设计出来的，而是演进出来的。” 我们遵循此原则，从最简单的模型开始，逐步解决瓶颈。

### 2.1. V1: 基础 MVP

- **架构**: 客户端 ↔ 评论服务 ↔ 数据库。
- **解决了**: 评论的持久化。
- **引入的瓶颈**: 如何让百万观众实时看到评论？

### 2.2. V2: 解决实时性——从“拉”到“推”

- **方案 A (拉): 轮询 (Polling)**
    - **致命缺陷**: 资源浪费与实时性矛盾。百万用户每 3 秒轮询一次，将产生约 **33 万 QPS** 的读请求，其中 99.9%为空轮询，足以压垮任何常规系统。
- **方案 B (推): 服务端推送 (Server Push)**

**技术选型对比**:

| 特性       | WebSocket                      | Server-Sent Events (SSE)                            |
| :------- | :----------------------------- | :-------------------------------------------------- |
| **通信模型** | **全双工** (客户端 ↔ 服务端)            | **单工** (服务端 → 客户端)                                  |
| **协议**   | 独立协议 (`ws://`, `wss://`)，需握手升级 | 基于标准 **HTTP/1.1**，`Content-Type: text/event-stream` |
| **资源开销** | 相对较重，为双向通信维护状态                 | 非常轻量，本质是一个不关闭的 HTTP GET 请求                          |
| **断线重连** | 需手动实现                          | **浏览器内置支持**，并自动携带 `Last-Event-ID` 请求头               |
| **适用场景** | 实时聊天、协同编辑、在线游戏 (需要客户端频繁发数据)    | **新闻流、股票行情、直播评论 (读远大于写)**                           |

**结论**: 直播评论是典型的读写极不平衡场景，**SSE 是此场景下的最优解**。


## 3. 第三部分：核心难点——分布式实时推送架构 (深度剖析)

当引入 SSE 服务器集群以承载百万长连接后，核心挑战浮现：**发布到服务器 A 的评论，如何高效、精准地推送给连接在服务器 B、C、D 上的相关观众？**

### 3.1. 方案一：简单 Pub/Sub (广播模式)

- **核心思想**: 利用消息中间件的发布/订阅能力，将评论广播给所有可能需要它的节点。
- **详细工作流**:
    1. **发布**: 评论服务接收到一条对 `live_123` 的评论后，将其发布到 Redis 的一个**通用频道**，例如 `comments-channel`。
    2. **广播**: Redis 将这条评论消息**复制并发送**给所有订阅了 `comments-channel` 的客户端，即**所有** SSE 服务器。
    3. **过滤与推送**:
        - SSE 服务器 1 收到消息，检查其本地内存维护的连接映射表，发现自己有 3 个观众在看 `live_123`，于是将评论推送给这 3 个连接。
        - SSE 服务器 2 收到**同样的消息**，检查本地映射表，发现自己没有任何观众在看 `live_123`，于是**直接丢弃**该消息。
        - 所有其他 SSE 服务器重复此过程。
- **瓶颈分析**:
    - **网络风暴**: 假设有 1000 个直播间，100 台 SSE 服务器。任何一个直播间的一条评论，都会在内网中产生 100 条冗余的网络传输。随着服务器数量增加，内网带宽消耗呈线性增长。
    - **CPU 浪费**: 每台 SSE 服务器都需要接收、反序列化、并判断每一条评论，即使这条评论与它所服务的观众毫无关系。这造成了巨大的 CPU 资源浪费。
    - **结论**: 此方案简单粗暴，但在大规模场景下，其资源利用率极低，无法扩展。它解决了“能不能”的问题，但没有解决“好不好”的问题。

### 3.2. 方案二：分区 Pub/Sub (分组广播)

- **核心思想**: 对消息进行分类，让订阅者只接收自己感兴趣的类别，从而减少无效消息的处理。
- **架构与数据流**:
    - **发布端**: 评论服务计算 `channel = hash(liveVideoId) % N` (N 为预设的分区数，如 64)，然后将评论发布到 `comments-channel-{channel}`。
    - **订阅端**: 每台 SSE 服务器不再订阅通用频道，而是根据其上连接的观众所观看的 `liveVideoId`，动态地订阅其所需要的 `comments-channel-*` 频道。
- **详细工作流**:
    1. **动态订阅**: 当一个观众连接到 SSE 服务器 3，并告知其在看 `live_456` 时，服务器 3 计算出 `channel = hash("live_456") % 64 = 22`。如果服务器 3 尚未订阅 `comments-channel-22`，它会立即发起订阅。
    2. **分区发布**: 一条对 `live_456` 的新评论产生，评论服务计算出频道为 `22`，并将评论发布到 `comments-channel-22`。
    3. **定向接收**: 只有订阅了 `comments-channel-22` 的服务器（包括服务器 3）会收到这条消息。其他服务器完全无感知。
- **瓶颈分析**:
    - **订阅泛滥 (Subscription Flood)**: 此方案的有效性**强依赖于负载均衡策略**。如果使用简单的轮询 LB，观众会被随机分配。
    - **最坏情况**: 假设服务器 X 运气不好，先后连接上了观看 `live_101`, `live_202`, `live_303`... 的观众，而这些 `liveVideoId` 经过哈希后恰好分散在 `channel-1`, `channel-2`, `channel-3`... 等多个不同分区。这将导致服务器 X**被迫订阅大量甚至全部分区**，使其退化回方案一的低效状态。
    - **结论**: 分区思想是正确的，但它只解决了消息广播的问题，并未解决订阅关系混乱的问题。系统的瓶颈从“处理所有消息”转移到了“订阅所有频道”。

### 3.3. 方案三：分区 Pub/Sub + L7 负载均衡 (关联聚合路由)

- **核心思想**: 在流量入口处进行智能路由，将“物以类聚”的思想贯彻到底，确保每个处理节点的工作是高度专注的。
- **架构与数据流**
    ![直播订阅.svg](https://r2.129870.xyz/img/2025/e044b1405814c559fc7ed4d7fd5cb203.svg)
- **详细工作流**:
    1. **控制平面与数据平面**:
        - **控制平面 (Control Plane)**: 一个独立的调度服务，维护 `liveVideoId` 到服务器的映射关系，并监控服务器健康状况。
        - **数据平面 (Data Plane)**: 即 L7 LB (如 Envoy)。它本身是“无脑”的，只负责高效执行由控制平面下发的路由规则。
    2. **智能路由**:
        - 一个观看 `live_789` 的观众发起 SSE 连接请求。
        - L7 LB 解析出 `liveVideoId=live_789`。
        - 它应用**一致性哈希**算法，`server = hash("live_789")`，稳定地计算出应由 SSE 服务器 5 负责。
        - 请求被转发到 SSE 服务器 5。
    3. **专注订阅**: 因此，SSE 服务器 5 上几乎所有观众都在看 `live_789` 或少数几个其他直播。它只需计算出这几个直播对应的分区频道（如 `channel-33`, `channel-44`）并订阅即可。
    4. **高效推送**: 当一条 `live_789` 的评论被发布到 `channel-33` 时，只有 SSE 服务器 5 等极少数服务器会收到，并高效地推送给其上的所有连接。
- **优势分析**:
    - **根本性解决**: 此方案同时解决了“消息风暴”和“订阅泛滥”两个问题。
    - **高内聚**: 每个 SSE 服务器的职责变得高度内聚，只处理少数几个直播间的业务，极大地提升了资源利用率和系统可预测性。
    - **优秀的水平扩展性**: 当流量增加时，只需增加 SSE 服务器，控制平面会自动感知并更新路由规则，新服务器会平滑地接管一部分 `liveVideoId` 的流量。
    - **结论**: 这是工业界构建此类系统的**黄金标准**，兼顾了性能、扩展性和解耦。

### 3.4. 方案四：调度器模式 (中央集权路由)

- **核心思想**: 放弃 Pub/Sub 的间接通信，改为通过一个全知全能的中央调度器进行直接的、点对点的命令式路由。
- **架构与数据流**:
    ![服务调度.svg](https://r2.129870.xyz/img/2025/851df90fbe349755bb06fa802810fedf.svg)
- **详细工作流**:
    1. **维护全局状态**: 调度器通过内存中的一个巨大 `Map` 维护 `liveVideoId -> [Server List]` 的映射。这个状态通过 SSE 服务器的注册和周期性心跳来保持最新。
    2. **评论路由**:
        - 评论服务收到一条对 `live_123` 的评论。
        - 它向调度器发起一次 RPC 查询：`get_server_for_live("live_123")`。
        - 调度器查询其 `Map`，返回 `["server_ip_1", "server_ip_2"]`。
        - 评论服务再分别向 `server_ip_1` 和 `server_ip_2` 发起 RPC，将评论数据直接发送过去。
- **深度权衡**:

| 对比维度 | 分区 Pub/Sub + L7 LB (方案三) | 调度器模式 (方案四) |
| :--- | :--- | :--- |
| **架构模型** | **声明式** (我订阅我关心的) + **去中心化** (依赖 MQ) | **命令式** (我告诉你该发给谁) + **中心化** (依赖调度器) |
| **状态管理** | 相对简单。订阅关系由 MQ 管理，路由关系由控制平面管理。 | **极其复杂**。调度器自身是有状态的，需处理一致性、高可用、脑裂等分布式难题。 |
| **系统韧性** | **更高**。MQ 通常是高可用的集群，部分 broker 故障不影响全局。 | **更脆弱**。调度器是关键单点，一旦它故障或数据不一致，整个实时推送链路瘫痪。 |
| **延迟** | 略高，有消息中间件的 hop。 | 理论上最低，是直接的 RPC 调用。 |
| **运维复杂度** | 中等。依赖成熟的 MQ 和 LB 组件。 | **极高**。需要一个专门的团队来维护这个自研的核心调度系统。 |

- **结论**: 调度器模式在性能上可能达到极致，但它以极高的系统复杂度和运维成本为代价。它更适合对延迟要求极端苛刻、且有强大基础架构团队支撑的场景。对于绝大多数公司，**方案三是更务实、更健壮的选择**。

---

## 4. 第四部分：工业级实践的深度考量 (The Devil is in the Details)

这是将理论模型转化为健壮生产系统的关键。

### 4.1. 热点直播间处理 (Hot Spot Mitigation)

- **问题**: 单个超级热门直播间（如春晚）的流量会压垮被分配到的单台服务器。
- **解决方案**: **动态分片与流量迁移**。
    1. **检测 (Detect)**: 控制平面通过监控发现服务器 A 的负载（CPU、连接数）超阈值。
    2. **决策 (Decide)**: 决定将服务器 A 上的热门直播 `live_123` 迁移一部分流量到空闲服务器 D。
    3. **准备 (Prepare)**: 控制平面通知服务器 D 准备接收流量（如订阅相关频道）。
    4. **更新路由 (Route)**: 控制平面更新 L7 LB 的配置，将 `live_123` 的映射从 `server_A` 变为 `[server_A, server_D]` 的集群。新连接会开始流向服务器 D。
    5. **优雅排水 (Drain & Reconnect)**:
        - 服务器 A 主动向其上 `live_123` 的部分客户端连接发送一个自定义的 SSE 事件 `event: reconnect`，或直接关闭连接。
        - 客户端 `EventSource` 监听到事件或 `onclose` 事件后，**立即发起重连**。
        - 重连请求中，浏览器会自动携带 `Last-Event-ID` HTTP 头。
        - LB 将此请求路由到新服务器 D。服务器 D 收到后，根据 `Last-Event-ID` 从数据库或缓存中**补发**该用户错过的评论，然后无缝切换到实时流。

### 4.2. 内容安全与审核 (Content Security)

- **原则**: **先审后发**，杜绝风险内容瞬间广播。
- **异步审核流程**:
    ![评论审核.svg](https://r2.129870.xyz/img/2025/97caed68815d35619060a3c831e0c760.svg)
- **数据库状态**: `Comment` 表需增加 `status` 字段 (`pending`, `approved`, `rejected`)。

### 4.3. 极致性能优化：服务器端评论聚合

- **问题**: “666”、“加油”等弹幕风暴会消耗巨量带宽和客户端性能。
- **解决方案**: 在服务器端进行**流式聚合 (Stream Aggregation)**。
    1. **聚合层**: 引入一个有状态的流处理服务（可用 Flink/Spark Streaming 或自研）。
    2. **时间窗口**: 服务在极短的时间窗口内（如 1s）对评论内容进行计数。
    3. **聚合消息协议**:

        ```json
        // 普通评论
        { "type": "comment", "id": "...", "content": "主播好厉害" }
        // 聚合后的新评论
        { "type": "aggregated_comment", "key": "agg_key_666", "content": "666", "count": 150 }
        // 对已存在的聚合评论进行增量更新
        { "type": "aggregation_update", "key": "agg_key_666", "increment": 50, "total_count": 200 }
        ```

    4. **发送者体验补偿**: 用户发送评论后，其**本地客户端立即渲染**该条评论（客户端预测），以获得即时反馈。当收到服务器的聚合消息时，再将本地预测的评论与聚合信息对齐或替换。

### 4.4. 系统可观测性 (Observability)

- **Metrics (监控)**:
    - **API 网关**: QPS, P99/P95 延迟, 错误率。
    - **评论服务**: 各接口性能, 数据库连接池状态。
    - **Kafka**: **Consumer Lag (消费延迟)** 是核心指标，消息生产/消费速率。
    - **SSE 服务器**: **在线连接总数**, 消息推送队列积压深度, CPU/内存使用率。
- **Logging (日志)**: 所有日志必须为 **JSON 格式**，并包含统一的 `trace_id`，方便聚合查询。
- **Tracing (追踪)**: 使用 OpenTelemetry 标准，通过 Jaeger/SkyWalking 等工具，可视化一个请求从入口到所有微服务的完整调用链。

### 4.5. 工程细节与韧性设计

- **SSE 工程问题**:
    - **代理缓冲**: NGINX 必须为 SSE 接口关闭缓冲。

        ```nginx
        location /sse-endpoint {
            proxy_pass http://sse_backend;
            proxy_set_header Connection '';
            proxy_http_version 1.1;
            proxy_buffering off; # 核心配置
            proxy_cache off;
        }
        ```

    - **连接数限制**:
        - **短期**: 使用多个子域名（`s1.domain.com`, `s2.domain.com`）进行**域名分片**。
        - **长期**: 全站启用 **HTTP/2 或 HTTP/3**，利用其多路复用特性，在单个 TCP/QUIC 连接上承载多个 SSE 流。
    - **空闲超时**: SSE 服务器必须实现**应用层心跳**，例如每 20 秒发送一个 SSE 注释行 `:heartbeat\n\n`，以保持网络路径上的连接活跃。
- **缓存策略**:
    - **Redis ZSET**: 缓存每个直播间最新的 200 条评论。`key` 为 `comments:{liveVideoId}`，`score` 为时间戳，`member` 为评论内容或 ID。新用户加载历史评论时，首先命中此缓存。
- **降级与熔断 (Resilience)**:
    - **优雅降级**: 当客户端 `EventSource` 连接多次失败后，可自动降级到 **HTTP 轮询**模式，保障基本可用性。
    - **服务熔断**: 服务间调用（如查询调度器）必须有熔断器（如 Sentinel）。当依赖的服务故障时，快速失败或返回兜底数据，防止雪崩。

## 5. 总结

设计一个百万级直播评论系统，是一个典型的从理论走向实践的复杂工程。其核心在于：

1. **迭代演进**: 从最简单的模型出发，识别瓶颈，并引入恰当的技术（SSE, Pub/Sub, L7 LB）逐步优化。
2. **权衡艺术 (Trade-offs)**: 在延迟、成本、复杂度和用户体验之间做出明智的决策。例如，为安全引入审核延迟，为性能引入聚合延迟。
3. **细节是魔鬼**: 工业级系统不仅需要宏伟的架构蓝图，更需要对内容安全、可观测性、热点处理、降级预案等“脏活累活”进行细致入微的设计。