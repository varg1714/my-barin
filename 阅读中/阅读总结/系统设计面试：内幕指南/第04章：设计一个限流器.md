---
source: "[[阅读中/文章列表/系统设计面试：内幕指南/第04章：设计一个限流器|第04章：设计一个限流器]]"
create: 2025-09-17
---

## 1. 复习笔记：设计一个限流器 (结合原文与 Redis 实现)

### 1.1. 第一部分：限流器的目标与设计要求 (The Why)

根据原文，设计限流器的主要目的有：

1.  **防止拒绝服务 (DoS) 攻击**：通过阻止恶意或意外的超量请求，保护系统资源。
2.  **降低成本**：限制对昂贵的第三方 API 的调用次数，或减少自身服务器资源的使用。
3.  **防止服务器过载**：过滤掉由机器人或用户不当行为产生的流量洪峰，保证核心服务稳定。

**核心设计要求：**

*   **准确性**：能精确地限制超过阈值的请求。
*   **低延迟**：限流器本身不能成为系统性能瓶颈。
*   **内存效率**：尽可能少地占用内存。
*   **分布式**：限流策略必须在多个服务器或进程间共享和同步。
*   **高容错**：单个限流器节点的故障不应影响整个系统。

### 1.2. 第二部分：限流器的位置 (The Where)

原文探讨了限流器的部署位置，结论是**服务端限流**是更可靠的选择。具体位置有：

1.  **直接在应用服务器实现**：与业务代码耦合较紧。
2.  **作为独立的中间件**：将限流逻辑解耦，独立于业务服务。
3.  **在 API 网关 (API Gateway) 中实现**：这是微服务架构下的主流方案，网关作为所有流量的入口，统一执行认证、监控、限流等策略。

**高层架构**：无论部署在哪里，核心思想都是在请求到达最终的 API 服务之前，先经过限流器的处理。限流器会与一个高速的中心化存储（如 **Redis**）交互，以获取和更新计数状态。

![High-level architecture diagram from the article](https://github.com/Admol/SystemDesign/raw/main/images/chapter4/figure4-12.jpg)

### 1.3. 第三部分：核心算法及其 Redis 实现 (The How)

这是设计的核心。下面我们将原文的五种算法理论与具体的 Redis 实现方案相结合。

#### 1.3.1. 令牌桶算法 (Token Bucket)

*   **原文原理**：系统以固定速率向桶里添加令牌，桶有容量上限。每个请求消耗一个令牌，若桶中无令牌则拒绝请求。此算法允许突发流量（消耗积攒的令牌）。
*   **Redis 实现方案**：
    *   **数据结构**：`Hash`
    *   **Key 设计**：`token_bucket:{user_id}`
    *   **Hash 字段**：
        *   `tokens`: 当前剩余令牌数。
        *   `last_refill_ts`: 上次刷新令牌的时间戳。
    *   **核心逻辑 (Lua 脚本)**：
        1.  读取 `tokens` 和 `last_refill_ts`。
        2.  计算自上次刷新以来应补充的令牌数，并更新 `tokens`（不能超过桶容量）。
        3.  检查 `tokens` 是否足够，若足够则减 1 并更新 `last_refill_ts`，允许请求；否则拒绝。

#### 1.3.2. 漏桶算法 (Leaking Bucket)

*   **原文原理**：请求进入一个固定容量的队列（桶）。系统以恒定的速率从队列中取出请求进行处理（漏出）。若队列已满，则新请求被丢弃。此算法强制平滑流量。
*   **Redis 实现方案**：
    *   **数据结构**：`List` (作为队列) + `Hash` (存储状态)
    *   **Key 设计**：队列 `leaky_bucket:{user_id}`，状态 `leaky_bucket_state:{user_id}`。
    *   **核心逻辑 (Lua 脚本)**：
        1.  **模拟漏水**：根据流出速率和上次漏水时间，用 `LPOP` 从队列头部移除已处理的请求。
        2.  **尝试入队**：用 `LLEN` 检查队列长度是否小于容量。
        3.  若未满，用 `RPUSH` 将新请求加入队尾，允许请求；否则拒绝。

#### 1.3.3. 固定窗口计数器 (Fixed Window Counter)

*   **原文原理**：将时间划分为固定窗口（如每分钟），在窗口内对请求计数。窗口结束时计数器清零。主要缺点是窗口边缘的突刺问题。
*   **Redis 实现方案**：
    *   **数据结构**：`String`
    *   **核心命令**：`INCR`, `EXPIRE`
    *   **Key 设计**：`rate_limit:{user_id}:{window_start_ts}`
    *   **核心逻辑**：
        1.  对当前窗口的 Key 执行 `INCR`。
        2.  若是窗口内第一个请求（`INCR` 返回 1），则用 `EXPIRE` 设置过期时间。
        3.  判断计数值是否超限。

#### 1.3.4. 滑动窗口日志 (Sliding Window Log)

*   **原文原理**：记录每个请求的精确时间戳。每次检查时，移除窗口外的旧时间戳，然后统计窗口内剩余时间戳的数量。精度最高，但内存消耗大。
*   **Redis 实现方案**：
    *   **数据结构**：`Sorted Set (ZSET)`
    *   **核心命令**：`ZADD`, `ZREMRANGEBYSCORE`, `ZCARD`
    *   **Key 设计**：`rate_limit_log:{user_id}`
    *   **核心逻辑 (Lua 脚本)**：
        1.  用 `ZREMRANGEBYSCORE` 移除窗口外的旧记录。
        2.  用 `ZCARD` 获取当前窗口内的请求数。
        3.  若未超限，则用 `ZADD` 将当前请求的时间戳加入集合，允许请求；否则拒绝。

#### 1.3.5. 滑动窗口计数器 (Sliding Window Counter)

*   **原文原理**：结合了固定窗口和滑动窗口。通过加权计算上一个窗口和当前窗口的请求数，来近似估算滑动窗口内的总数，以平滑边缘突刺。
*   **Redis 实现方案**：
    *   **数据结构**：`Hash`
    *   **Key 设计**：`sw_counter:{user_id}`
    *   **Hash 字段**：`window_start_ts`, `current_window_count`, `prev_window_count`。
    *   **核心逻辑 (Lua 脚本)**：
        1.  检查并更新窗口状态（将旧的当前窗口计数移到上一个窗口）。
        2.  根据当前时间在窗口内的百分比，加权计算 `(prev_count * (1-percentage)) + current_count`。
        3.  若估算值未超限，则用 `HINCRBY` 增加 `current_window_count`，允许请求；否则拒绝。

### 1.4. 第四部分：分布式环境下的挑战与解决方案

原文重点指出了分布式环境下的两个核心挑战，而 Redis 及其原子操作正是解决这些问题的关键。

1.  **竞争条件 (Race Condition)**
    *   **问题描述**：多个请求并发地执行“读取-计算-写入”操作，可能导致计数不准。
    *   **解决方案**：使用 **Lua 脚本**。将整个逻辑（如令牌桶的计算和消耗）封装在一个 Lua 脚本中发送给 Redis 执行。Redis 会保证脚本的执行是**原子性**的，执行期间不会被其他命令打断，从而根除竞争条件。

2.  **同步问题 (Synchronization)**
    *   **问题描述**：在分布式集群中，每个限流器节点需要访问到全局一致的限流状态。
    *   **解决方案**：使用**中心化的 Redis 集群**。所有限流器实例都连接到同一个 Redis，所有状态的读写都发生在这里，天然地保证了数据的一致性和同步。

### 1.5. 第五部分：其他重要设计点

*   **限流规则**：规则应可配置，通常存储在配置文件或数据库中，由限流器动态加载。
*   **超出限制的处理**：
    *   **返回 HTTP 429**：明确告知客户端请求过多。
    *   **设置响应头**：通过 `X-Ratelimit-Limit`, `X-Ratelimit-Remaining`, `X-Ratelimit-Retry-After` 等头部向客户端提供更多上下文信息。
    *   **请求排队**：对于某些重要业务，可以将受限的请求放入消息队列，稍后重试。
*   **监控**：收集和分析限流数据，判断限流规则是否合理、算法是否有效，并根据业务变化（如秒杀活动）进行调整。

这份整合了原文理论和 Redis 实现细节的笔记，希望能为你提供一个全面且深入的复习视角。