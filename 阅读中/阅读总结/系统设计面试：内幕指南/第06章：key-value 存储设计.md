---
source: "[[阅读中/文章列表/系统设计面试：内幕指南/第06章：key-value 存储设计]]"
create: 2025-09-30
---

## 1. 分布式键值存储核心设计笔记

这份笔记旨在全面总结设计一个高可用、高可扩展的分布式键值（Key-Value）存储系统所需的核心概念与技术。

### 1.1. 一、核心目标与设计范围

设计一个支持 `put(key, value)` 和 `get(key)` 操作的分布式键值存储，需满足以下特性：

- **大数据量存储**：能存储远超单机容量的数据。
- **高可用性**：系统在部分节点故障时仍能提供服务。
- **高可扩展性**：可以方便地通过增删节点来动态调整系统容量（自动扩缩容）。
- **可调节的一致性**：允许根据业务需求在强一致性和最终一致性之间做选择。
- **低延迟**：读写操作响应迅速。

### 1.2. 二、理论基础：CAP 定理

CAP 定理是分布式系统设计的基石，它指出任何分布式系统最多只能同时满足以下三个特性中的两个：

- **一致性 (Consistency)**：所有客户端在同一时间看到相同的数据。
- **可用性 (Availability)**- 任何请求总能收到响应（即使是部分节点故障）。
- **分区容错性 (Partition Tolerance)**：系统在网络分区（节点间通信中断）的情况下仍能继续运行。

由于网络分区在分布式系统中不可避免，因此设计通常在 **CP** 和 **AP** 之间做权衡：

- **CP (一致性 + 分区容错)**：牺牲可用性。发生网络分区时，为保证数据一致，系统可能会拒绝部分操作，导致不可用（例如：银行系统）。
- **AP (可用性 + 分区容错)**：牺牲一致性。系统始终接受读写，即使可能返回旧数据。数据最终会同步，达到**最终一致性**（例如：社交媒体点赞数）。

本笔记的设计倾向于一个 **AP** 系统，因为它更符合高可用的目标。

### 1.3. 三、核心组件与技术详解

#### 1.3.1. 数据分区 (Data Partitioning)

- **问题**：如何将海量数据均匀分布到多个服务器上，并在增删节点时最小化数据迁移？
- **技术**：**一致性哈希 (Consistent Hashing)**
- **原理**：
    1.  将所有服务器节点（物理或虚拟）的哈希值映射到一个环形空间上。
    2.  将数据 `key` 的哈希值也映射到同个环上。
    3.  `key` 存储在从其位置顺时针寻找到的第一个服务器节点上。
- **优点**：
    - **自动扩缩容**：新增或删除节点时，仅影响环上相邻的节点，数据迁移量最小。
    - **异构性**：可以为性能更强的服务器分配更多的虚拟节点，使其承担更多负载。

#### 1.3.2. 数据复制 (Data Replication)

- **问题**：如何保证系统的高可用性和数据可靠性，防止单点故障？
- **技术**：**N 副本策略**
- **原理**：
    1.  将一份数据复制到 N 个不同的节点上（N 是可配置的副本数）。
    2.  这些节点通常通过一致性哈希环顺时针选取，并确保它们位于不同的物理机架或数据中心，以抵御更大范围的故障。

#### 1.3.3. 一致性模型 (Consistency Model)

- **问题**：数据在多个副本间如何同步，如何定义读写的成功？
- **技术**：**Quorum 共识 (法定人数)**
- **定义**：
    - `N`：总副本数。
    - `W`：写入操作需要成功确认的副本数。
    - `R`：读取操作需要成功响应的副本数。
- **规则与权衡**：
    - **强一致性保证**：当 `W + R > N` 时，读写操作的副本集合至少有一个重叠节点，确保能读到最新的写入数据。常用配置为 `N=3, W=2, R=2`。
    - **优化读取**：设置 `R=1, W=N`，读取速度快，但写入慢。
    - **优化写入**：设置 `W=1, R=N`，写入速度快，但读取慢。
    - **弱/最终一致性**：当 `W + R <= N` 时，读写可能没有交集，无法保证读到最新数据。

#### 1.3.4. 不一致性解决：版本控制

- **冲突的根源**：
    - 冲突并非由路由错误导致。**相同键永远被路由到固定的 N 个副本节点**。
    - 冲突的根源在于**并发写入**。当两个客户端几乎同时修改同一个 `key` 时，由于网络延迟等因素，不同的副本节点可能以不同的顺序处理这两个写请求，导致副本间数据不一致。
- **技术**：**向量时钟 (Vector Clocks)**
- **原理**：
    1.  向量时钟是一个 `[服务器ID, 版本号]` 列表，附加在每一份数据上。
    2.  当节点 `Sx` 写入数据时，它会递增自己的版本号 `(Sx, v)`。
    3.  **检测冲突**：
        - 如果一个数据版本 `D2` 的向量时钟中，所有服务器的版本号都 **大于等于** `D1` 的对应版本号，那么 `D2` 是 `D1` 的后代，无冲突。
        - 如果两个版本的向量时钟互不为对方的后代（例如 `D([s1, 2], [s2, 1])` 和 `D([s1, 1], [s2, 2])`），则说明发生了并发修改，存在冲突。
    4.  **解决冲突**：当系统检测到冲突时，会将所有冲突版本的数据返回给客户端，由客户端的业务逻辑决定如何合并（例如，保留最新的，或合并两者），然后将合并后的结果重新写入系统。

#### 1.3.5. 故障处理 (Failure Handling)

- **故障检测**：
    - **技术**：**Gossip 协议**
    - **原理**：节点间随机、周期性地交换彼此的健康状态信息（心跳计数器）。如果一个节点的心跳长时间未更新，它将被集群标记为下线。这是一种去中心化的、容错性强的检测方式。
- **处理临时性故障**：
    - **技术**：**草率仲裁 (Sloppy Quorum)** 和 **暗示切换 (Hinted Handoff)**
    - **原理**：当一个 `key` 的某个副本节点临时故障时，系统不会让写操作失败。
        1.  **Sloppy Quorum**：协调器会选择环上健康的、顺位的下一个节点作为临时副本，保证 `W` 个写入仍然成功。
        2.  **Hinted Handoff**：临时节点会存储一份数据，并附带一个“暗示”（Hint），指明这份数据最终应属于哪个故障节点。当故障节点恢复后，临时节点会将这份数据交还给它。

- **处理永久性故障与数据同步**：
    - **技术**：**反熵协议 (Anti-entropy Protocol)** 与 **Merkle 树**
    - **原理**：为了确保副本间的最终一致性，节点需要定期同步数据。
        1.  **Merkle 树**：将 `key` 空间分块，对每块数据计算哈希，然后逐层向上构建哈希树，直到根哈希。
        2.  **高效比较**：只需比较两个副本的 Merkle 树的根哈希，即可快速判断数据是否一致。如果不一致，则递归比较子树，能快速定位到不一致的数据块，只需同步差异部分，极大减少了网络传输。

### 1.4. 四、整体系统架构

- **架构特点**：
    - **去中心化**：所有节点角色对等，无单点故障。
    - **协调器 (Coordinator)**：客户端可与任意节点通信，该节点即成为本次请求的协调器，负责将请求路由到正确的副本节点，并等待 `W` 或 `R` 个响应。
- **节点变动与数据路由**：
    - 当新增或删除节点导致一致性哈希环变化时，系统会自动处理。
    - **路由更新**：所有节点通过 Gossip 协议等方式获知环的变化，更新本地路由信息。后续请求将按新的环结构进行路由。
    - **数据迁移**：受影响的节点（通常是变动节点的邻居）之间会启动后台数据迁移任务。
    - **过渡期处理**：在迁移完成前，系统会使用**暗示切换**等机制来处理发往新节点的请求，保证服务的连续性。迁移完成后，通过**反熵协议**（Merkle 树）校验数据，确保最终一致。

### 1.5. 五、读写路径

- **写入路径 (Write Path)**：
    1.  请求持久化到**提交日志 (Commit Log)** 文件中，防止节点掉电数据丢失。
    2.  数据写入**内存缓存 (Mem-cache)**，用于快速读取。
    3.  当内存缓存满或达到阈值时，数据被批量刷到磁盘上的 **SSTable** (Sorted String Table) 文件中。SSTable 是有序且不可变的。

- **读取路径 (Read Path)**：
    1.  首先检查数据是否在**内存缓存**中，若命中则直接返回。
    2.  若未命中，使用**布隆过滤器 (Bloom Filter)** 快速判断数据**可能**存在于哪些 SSTable 文件中。布隆过滤器可以有效避免查询不存在的 `key` 时扫描大量磁盘文件。
    3.  根据布隆过滤器的指引，到对应的 SSTable 文件中查找数据。
    4.  将查找到的数据返回给客户端。

### 1.6. 六、总结表

| 目标/问题 | 采用技术 |
| :--- | :--- |
| 存储大数据能力 | 一致性哈希 |
| 高可用性（读/写） | 数据复制、多数据中心、向量时钟 |
| 数据分区与增量扩展 | 一致性哈希 |
| 处理临时性故障 | 草率仲裁 (Sloppy Quorum) 和暗示切换 (Hinted Handoff) |
| 处理永久性故障 | Merkle 树 (反熵协议) |
| 处理数据中心中断 | 跨数据中心复制 |