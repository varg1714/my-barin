#知识大纲 #Redis

# 1. Redis 大纲

![](https://r2.129870.xyz/img/20220517160132.png)

![](https://r2.129870.xyz/img/20220517160149.png)

# 2. 使用篇

## 2.1. Redis 的使用场景

- 缓存
- 计数相关：计数器/排行榜/浏览量/播放量等。
- 交并集操作：共同好友，朋友圈点赞。
- 简单消息队列：发布订阅。
- Session 服务器。
- 基于 RedisTimeSeries 模块操作时序数据库。
- 处理秒杀业务，进行库存查询和预减。

## 2.2. Redis 支持的数据结构

![image.png](https://r2.129870.xyz/img/202308040028860.png)

- String
- List
- Set
- Sorted Set
- Hash
- Bitmaps
- Hyperloglogs
- Geospatial

![](https://r2.129870.xyz/img/20220518231920.png)

## 2.3. Redis 实现消息队列

消息队列设计需要满足以下几个需求：

- 消息顺序：支持按消息生产顺序消费。
- 消息持久化：支持消息的持久化，防止丢失。
- 消息重复消费：支持消息重复消费。

### 2.3.1. 基于 list 实现

1. `rpush` 生产消息，`lpop` 消费消息，没有消息时 sleep 或者使用 `blpop`。
2. 为了支持消息重复消费，可以利用 List 类型提供的 `BRPOPLPUSH` 命令。这个命令的作用是让消费者程序从一个 List 中读取消息，同时，Redis 会把这个消息再插入到另一个 List（可以叫作备份 List）留存。这样一来，如果消费者程序读了消息但没能正常处理，等它重启后，就可以从备份 List 中重新读取消息并进行处理了。同时需要注意消费成功后移除消息。
3. 消息支持多次消费，使用 pub / sub 模式可以达到 `1:N` 的消息队列
4. pub / sub 模式的缺点：消费者下线的情况下，生产的消息会丢失
5. 延时队列的实现：基于 SortedSet，以时间戳为 score，消息内容作为 key 来生产消息。调用 zrangebyscore 来获取 N 秒前的数据。
    该方案存在以下潜在问题：
    - 消息重复消费
        问题：消费者可能读取到消息后，因崩溃或其他原因未及时删除消息，导致下次轮询时重复读取。
        解决：需保证读取+删除的原子性（例如通过 Lua 脚本或 Redis 事务）。
    - 高并发竞争
        问题：多消费者同时读取同一批消息，可能导致重复处理。
        解决：使用分布式锁或 Redis 的 ZPOPMIN/ZPOPMAX 命令（需 Redis 5.0+）。
    - 消息唯一性冲突
        问题：若 member（消息内容）重复，后插入的消息会覆盖前者（因 SortedSet 的 member 唯一）。
        解决：将 member 设计为唯一标识（如 UUID），消息内容存储在其他结构（如 Hash）。
    - 轮询性能开销
        问题：频繁调用 ZRANGEBYSCORE 可能对 Redis 造成压力。
        解决：使用阻塞式轮询或结合 Redis 的 Pub/Sub 通知机制（需额外设计）。
    - 大规模数据性能
        问题：SortedSet 的 ZRANGEBYSCORE 时间复杂度为 O(log(N)+M)（N 为总元素数，M 为返回元素数），当数据量极大时可能影响性能。
        解决：分片存储或改用其他更适合的 Redis 结构（如 Streams）。

### 2.3.2. 基于 Streams 实现

Redis 5.0 推出了 Stream 数据结构，它借鉴了 Kafka 的设计思想，弥补了 List 和 PubSub 的不足。Stream 类型数据可以持久化、支持 ack 机制、支持多个消费者、支持回溯消费，基本上实现了队列中间件大部分功能，比 List 和 PubSub 更可靠。

Streams 是 Redis 专门为消息队列设计的数据类型，它提供了丰富的消息队列操作命令。

- XADD：插入消息，保证有序，可以自动生成全局唯一 ID。
- XREAD：用于读取消息，可以按 ID 读取数据。同时支持阻塞式读取。
- XREADGROUP：按消费组形式读取消息。
- XPENDING 和 XACK：XPENDING 命令可以用来查询每个消费组内所有消费者已读取但尚未确认的消息，而 XACK 命令用于向消息队列确认消息处理已完成。

![](https://r2.129870.xyz/img/20220519121100.png)

## 2.4. Redis 实现分布式锁

### 2.4.1. 基于单节点的锁

- 使用 `setnx` / `del` 添加和释放锁，可带生效时间设定。
- 删除锁时判断线程 ID 是否是自己，防止误删除。
- 重入性使用 state 值来判断。
- 判断锁释放可以通过轮询或者基于 Redis 的发布订阅机制实现。

`Redisson` 使用的部分 Redis 命令如下：
```lua
# 加锁：keys[1] = 原始锁名称，argv[1] = 锁持有时间，argv[2] = 带线程ID的key
if ((redis.call('exists', KEYS[1]) == 0) or (redis.call('hexists', KEYS[1], ARGV[2]) == 1))
    then
	    redis.call('hincrby', KEYS[1], ARGV[2], 1);
	    redis.call('pexpire', KEYS[1], ARGV[1]);
	    return nil;
   end;
return redis.call('pttl', KEYS[1]);
```

### 2.4.2. 基于多节点的锁-RedLock

为了避免 Redis 实例故障而导致的锁无法工作的问题，Redis 的开发者 Antirez 提出了分布式锁算法 Redlock。

Redlock 算法的基本思路是让客户端和多个独立的 Redis 实例依次请求加锁，**如果客户端能够和半数以上的实例成功地完成加锁操作**，那么我们就认为，客户端成功地获得分布式锁了；否则加锁失败。这样一来，即使有单个 Redis 实例发生故障，因为锁变量在其它实例上也有保存，所以，客户端仍然可以正常地进行锁操作，锁变量并不会丢失。

RedLock 的加锁步骤如下：

1. 客户端获取当前时间。

2. 客户端按顺序依次向 N 个 Redis 实例执行加锁操作。
    这里的加锁操作和在单实例上执行的加锁操作一样，使用 `SET` 命令，带上 `NX`，`EX` / `PX` 选项，以及带上客户端的唯一标识。
    
    当然，如果某个 Redis 实例发生故障了，为了保证在这种情况下 Redlock 算法能够继续运行，我们需要给加锁操作设置一个超时时间。如果客户端在和一个 Redis 实例请求加锁时，一直到超时都没有成功，那么此时，客户端会和下一个 Redis 实例继续请求加锁。加锁操作的超时时间需要远远地小于锁的有效时间，一般也就是设置为几十毫秒。
1. 一旦客户端完成了和所有 Redis 实例的加锁操作，客户端就要计算整个加锁过程的总耗时。

客户端只有在满足下面的这两个条件时，才能认为是加锁成功：

1. 客户端从超过半数（ `>= N/2+1`）的 Redis 实例上成功获取到了锁。

2. 客户端获取锁的总耗时没有超过锁的有效时间。

在满足了这两个条件后，需要重新计算这把锁的有效时间，计算的结果是锁的最初有效时间减去客户端为获取锁的总耗时。如果锁的有效时间已经来不及完成共享数据的操作了，我们可以释放锁，以免出现还没完成数据操作锁就过期了的情况。当然，如果客户端在和所有实例执行完加锁操作后没能同时满足这两个条件，那么客户端向所有 Redis 节点发起释放锁的操作。

在 Redlock 算法中，释放锁的操作和在单实例上释放锁的操作一样，只要执行释放锁的 Lua 脚本就可以了。释放锁时，要对所有节点释放（即使某个节点加锁失败了），因为加锁时可能发生服务端加锁成功，由于网络问题，给客户端回复网络包失败的情况，所以需要把所有节点可能存的锁都释放掉。这样一来，只要 N 个 Redis 实例中的半数以上实例能正常工作，就能保证分布式锁的正常工作了。

### 2.4.3. RedLock 的思考

#### 2.4.3.1. RedLock 失效场景之节点崩溃

假设一共有 5 个 Redis 节点：A, B, C, D, E。设想发生了如下的事件序列：

1. 客户端 1 成功锁住了 A, B, C，获取锁成功（但 D 和 E 没有锁住）。

2. 节点 C 崩溃重启了，但客户端 1 在 C 上加的锁没有持久化下来，丢失了。

3. 节点 C 重启后，客户端 2 锁住了 C, D, E，获取锁成功。

这样，客户端 1 和客户端 2 同时获得了锁（针对同一资源）。

针对这一情况，可以采取**延迟重启**（delayed restarts）的概念。也就是说，一个节点崩溃后，先不立即重启它，而是等待一段时间再重启，这段时间应该大于锁的有效时间 (lock validity time)。这样的话，这个节点在重启前所参与的锁都会过期，它在重启后就不会对现有的锁造成影响。

但是这个延迟时间需要根据锁的时间来控制，这个通常来说不可控，且延迟重启对集群也会有影响。

#### 2.4.3.2. RedLock 失效场景之时钟跳跃

Martin Kleppmann 在 2016-02-08 这一天发表了一篇 blog，名字叫” [How to do distributed locking ](https://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html) “，Martin 在这篇文章中谈及了分布式系统的很多基础性的问题（特别是分布式计算的异步模型），对分布式系统的从业者来说非常值得一读。其中部分篇幅是对 Redlock 本身的批评。Martin 指出，由于 Redlock 本质上是建立在一个同步模型之上，对系统的记时假设 (timing assumption) 有很强的要求，因此本身的安全性是不够的。Redis 的作者对此进行了[回复](http://antirez.com/news/101)，双方对此进行了许多探讨，许多人也参与到这个探讨中。

在失效场景之节点崩溃中，我们设想了节点发生崩溃未持久化数据而导致的锁失效问题。这是由于节点数据丢失导致的，还有另一种情况会产生类似的效果：时钟跳跃导致锁提前失效。

如果节点 C 上的时钟发生了向前跳跃，导致它上面维护的锁快速过期，那么另一个客户端就能获取到锁，从而导致互斥资源保护锁失效。

针对这一情况，就需要我们确保[[数据密集型系统设计4：分布式的挑战#3 不可靠的时钟|时钟混乱]] 的场景不会发生。

#### 2.4.3.3. RedLock 失效场景之客户端延迟

**即使我们拥有一个完美实现的分布式锁（带自动过期功能），在没有共享资源参与进来提供某种 fencing 机制的前提下，我们仍然不可能获得足够的安全性。**

![](https://r2.129870.xyz/img/20220521192441.png)

在上面的时序图中，假设锁服务本身是没有问题的，它总是能保证任一时刻最多只有一个客户端获得锁。上图中出现的 lease 这个词可以暂且认为就等同于一个带有自动过期功能的锁。

假设这样一个场景：客户端 1 在获得锁之后发生了很长时间的 GC pause，在此期间，它获得的锁过期了，而客户端 2 获得了锁。当客户端 1 从 GC pause 中恢复过来的时候，它不知道自己持有的锁已经过期了，它依然向共享资源（上图中是一个存储服务）发起了写数据请求，而这时锁实际上被客户端 2 持有，因此两个客户端的写请求就有可能冲突（锁的互斥作用失效了）。

对于 GC 来说是一种客户端延迟，**系统环境太复杂，仍然有很多原因导致进程的 pause**。比如虚存造成的缺页故障 (page fault)，再比如 CPU 资源的竞争。即使不考虑进程 pause 的情况，网络延迟也仍然会造成类似的结果。

Martin 给出了一种方法，称为 `fencing token`。`fencing token` 是一个单调递增的数字，当客户端成功获取锁的时候它随同锁一起返回给客户端。而客户端访问共享资源的时候带着这个 `fencing token`，这样提供共享资源的服务就能根据它进行检查，拒绝掉延迟到来的访问请求（避免了冲突）。

![](https://r2.129870.xyz/img/20220521192853.png)

在上图中，客户端 1 先获取到的锁，因此有一个较小的 fencing token，等于 33，而客户端 2 后获取到的锁，有一个较大的 fencing token，等于 34。客户端 1 从 GC pause 中恢复过来之后，依然是向存储服务发送访问请求，但是带了 fencing token = 33。存储服务发现它之前已经处理过 34 的请求，所以会拒绝掉这次 33 的请求。这样就避免了冲突。

#### 2.4.3.4. 基于 Zookeeper 的锁安全吗？

很多人（也包括 Martin 在内）都认为，如果你想构建一个更安全的分布式锁，那么应该使用 ZooKeeper，而不是 Redis。基于 ZooKeeper 的分布式锁能提供绝对的安全吗？它需要 `fencing token` 机制的保护吗？我们不得不提一下分布式专家 [Flavio Junqueira](https://fpj.me/) 所写的一篇 blog，题目叫 [“Note on fencing and distributed locks”](https://fpj.me/2016/02/10/note-on-fencing-and-distributed-locks/)。

Flavio Junqueira 是 ZooKeeper 的作者之一，他的这篇 blog 就写在 Martin 和 antirez 发生争论的那几天。他在文中给出了一个基于 ZooKeeper 构建分布式锁的描述（当然这不是唯一的方式），需要注意下面的例子并非 Zookeeper 实现分布式锁的最佳示例，这会出现[羊群效应](https://developer.aliyun.com/article/427024) ：

1. 客户端尝试创建一个 `znode` 节点，比如 `/lock`。那么第一个客户端就创建成功了，相当于拿到了锁；而其它的客户端会创建失败（`znode` 已存在），获取锁失败。

2. 持有锁的客户端访问共享资源完成后，将 `znode` 删掉，这样其它客户端接下来就能来获取锁了。

3. `znode` 应该被创建成 `ephemeral` 的。这是 `znode` 的一个特性，它保证如果创建 `znode` 的那个客户端崩溃了，那么相应的 `znode` 会被自动删除。这保证了锁一定会被释放。

看起来这个锁相当完美，没有 Redlock 过期时间的问题，而且能在需要的时候让锁自动释放。但仔细考察的话，并不尽然。

ZooKeeper 是怎么检测出某个客户端已经崩溃了呢？实际上，每个客户端都与 ZooKeeper 的某台服务器维护着一个 Session，这个 Session 依赖定期的心跳 (heartbeat) 来维持。如果 ZooKeeper 长时间收不到客户端的心跳（这个时间称为 Sesion 的过期时间），那么它就认为 Session 过期了，通过这个 Session 所创建的所有的 ephemeral 类型的 znode 节点都会被自动删除。

设想如下的执行序列：

1. 客户端 1 创建了 znode 节点 `/lock`，获得了锁。
2. 客户端 1 进入了长时间的 GC pause。
3. 客户端 1 连接到 ZooKeeper 的 Session 过期了。znode 节点 `/lock` 被自动删除。
4. 客户端 2 创建了 znode 节点 `/lock`，从而获得了锁。
5. 客户端 1 从 GC pause 中恢复过来，它仍然认为自己持有锁。

最后，客户端 1 和客户端 2 都认为自己持有了锁，冲突了。

这与之前 Martin 在文章中描述的由于 GC pause 导致的分布式锁失效的情况类似。虽然用 ZooKeeper 实现的分布式锁也不一定就是安全的。该有的问题它还是有。但是，ZooKeeper 作为一个专门为分布式应用提供方案的框架，它提供了一些非常好的特性，是 Redis 之类的方案所没有的。像前面提到的 ephemeral 类型的 znode 自动删除的功能与 watch 机制等。

所以我们再次强调一遍，**即使我们拥有一个完美实现的分布式锁（带自动过期功能），在没有共享资源参与进来提供某种 fencing 机制的前提下，我们仍然不可能获得足够的安全性。**

## 2.5. BigKey 有什么影响

- 网络阻塞，传输耗时长
- 超时阻塞，操作耗时长
- 内存占用高，空间分配不平衡

## 2.6. 如何查询固定前缀 key

- `Keys` 命令。会阻塞线程，查询时间复杂度是 O (n)，且查询结果是全量，不支持分页。KEYS 命令需要遍历存储的键值对，所以操作延时高。
- Scan 命令。不会阻塞线程，但有**可能查出重复数据，不保证能得到查询期间被修改的元素**。
    客户端通过执行 `SCAN $cursor COUNT $count` 可以得到一批 key 以及下一个游标 `$cursor`，然后把这个 `$cursor` 当作 `SCAN` 的参数，再次执行，以此往复，直到返回的 `$cursor` 为 0 时，就把整个实例中的所有 key 遍历出来了。
    
    使用 `SCAN` 命令时，不会漏 key，但可能会得到重复的 key，这和 Redis 的 `rehash` 机制有关：
    
    - 为什么不会漏 key？
        Redis 在 `SCAN` 遍历全局哈希表时，采用**高位进位**的方式遍历哈希桶，当哈希表扩容后，通过这种算法遍历，旧哈希表中的数据映射到新哈希表，依旧会保留原来的先后顺序，这样就可以保证遍历时不会遗漏也不会重复。
    - 为什么 SCAN 会得到重复的 key？
        这个情况主要发生在哈希表缩容。已经遍历过的哈希桶在缩容时，会映射到新哈希表没有遍历到的位置，所以继续遍历就会对同一个 key 返回多次。
    
    `SCAN` 是遍历整个实例的所有 key，另外 Redis 针对 `Hash` / `Set` / `Sorted Set` 也提供了 `HSCAN` / `SSCAN` / `ZSCAN` 命令，用于遍历一个 key 中的所有元素，建议在获取一个 bigkey 的所有数据时使用，避免发生阻塞风险。

    使用 `HSCAN` / `SSCAN` / `ZSCAN` 命令，返回的元素数量与执行 `SCAN` 逻辑可能不同。执行 `SCAN $cursor COUNT $count` 时一次最多返回 count 个数的 key，数量不会超过 count。但 `Hash` / `Set` / `Sorted Set` 元素数量比较少时，底层会采用 `intset` / `ziplist` 方式存储，如果以这种方式存储，在执行 `HSCAN` / `SSCAN` / `ZSCAN` 命令时，会无视 count 参数，直接把所有元素一次性返回，也就是说，得到的元素数量是会大于 count 参数的。当底层转为哈希表或跳表存储时，才会真正使用发 count 参数，最多返回 count 个元素。

## 2.7. Redis 变慢的可能原因

![](https://r2.129870.xyz/img/20220519164941.png)

`redis-cli` 命令提供了 `–intrinsic-latency` 选项，可以用来监测和统计测试期间内的最大延迟。

### 2.7.1. 客户端相关的原因

- 客户端与 Redis 本身网络故障。

- API 使用不合理，如大 key 查询或 keys 扫描，集合统计聚合计算等。

- 设置的过期时间太集中，Redis 清理过期数据工作耗时长。

### 2.7.2. 服务端相关原因

#### 2.7.2.1. 与服务端机器性能相关

- 服务器 CPU 负载过高。

- 内存资源不足造成的内存淘汰与磁盘读取。

- 网络带宽过载。

- 频繁短连接，导致时间耗费在连接建立与释放过程上。应当使用长连接/连接池替代。

- 其它应用程序的资源争夺，更严重的产生内存与磁盘的 swap，导致性能下降。

#### 2.7.2.2. 与 Redis 应用相关

- 正在持久化
    - AOF 持久化
        若刷盘策略设置为 `always`，则每次更新操作都是 `fsync` 刷盘。若策略是 `EVERYSEC` ，主线程会创建子线程去完成 `fsync` 操作。
        
        在持久化过程中，此时若有 AOF 重写线程，这两个线程还会抢占 IO 资源，造成性能下降。如果需要高性能，同时也允许数据丢失，可以将配置项 `no-appendfsync-on-rewrite ` 设置为 yes，避免 AOF 重写和 `fsync` 竞争磁盘 IO 资源，导致 Redis 延迟增加。
    - RDB 文件生成
        在 RDB 文件生成过程中，fork 时会拷贝页表。如果实例很大，那么拷贝过程耗时长。主从同步时，也会生成 RDB 文件。
    - 内存大页
        Linux 内核从 2.6.38 开始支持内存大页机制，该机制支持 2MB 大小的内存页分配，而常规的内存页分配是按 4KB 的粒度来执行的。在 Redis 持久化过程中，采用写时复制技术。内存大页会导致拷贝的页数据变大。
- 正在进行过期数据删除操作
    默认情况下，Redis 每 100 毫秒会删除一些过期 key。`ACTIVE_EXPIRE_CYCLE_LOOKUPS_PER_LOOP` 是 Redis 的一个参数，默认是 20，即一秒内基本有 200 个过期 key 会被删除。
	
	如果超过 25% 的 key 过期了，则重复删除的过程，直到过期 key 的比例降至 25% 以下或者用时超过了定期删除循环流程的时间上限（该时间默认为 25ms）。
	
	删除操作是阻塞的（Redis 4.0 后可以用异步线程机制来减少阻塞影响）。所以一旦该条件触发，Redis 的线程就会一直执行删除。所以需要避免将过期时间设置在同一时刻。
	

- 多核 CPU 运行时，若 Redis 与 CPU 核心绑定不合理，也会造成性能下降。
- 正在进行碎片整理
    Redis 数据频繁操作，导致内存碎片的产生。碎片整理 Redis 提供了许多可配置的选项，**碎片整理是在主线程进行的**，所以需要评估影响进行合适的配置。
    
    可以通过 `info memory` 查看碎片情况，其中包括但不限于以下几个指标：
	 -  `mem_fragmentation_ratio` ：表示系统分配给 Redis 的内存和实际使用的内存的比值。正常情况下这个比值大于 1，当**小于 1 的时候就意味着部分数据被 swap 到磁盘去了**，这个时候如果读取这些数据就要从磁盘加载到内存了。
	- `active-defrag-cycle-min 25` ： 表示自动清理过程所用 CPU 时间的比例不低于 25%，保证清理能正常开展。
	- `active-defrag-cycle-max 75` ：表示自动清理过程所用 CPU 时间的比例不高于 75%，一旦超过，就停止清理，从而避免在清理时，大量的内存拷贝阻塞 Redis，导致响应延迟升高。
	- `active-defrag-ignore-bytes 100mb` ：表示内存碎片的字节数达到 100MB 时，开始清理。
	- `active-defrag-threshold-lower 10` ：表示内存碎片空间占操作系统分配给 Redis 的总空间比例达到 10% 时，开始清理。

### 2.7.3. 慢日志分析

可以使用 Redis 日志（慢查询日志）和 `latency monitor` 来排查执行较慢的命令操作。在使用慢查询日志前，我们需要设置两个参数：

- slowlog-log-slower-than：慢查询日志对执行时间大于多少微秒的命令进行记录。

- slowlog-max-len：慢查询日志最多能记录多少条命令记录。

我们可以使用 `SLOWLOG GET` 命令，来查看慢查询日志中记录的命令操作。Redis 从 `2.8.13` 版本开始，还提供了 `latency monitor` 监控工具，这个工具可以用来监控 Redis 运行过程中的峰值延迟情况。这个工具是通过扫描数据库来查找 bigkey 的，所以，在执行的过程中，会对 Redis 实例的性能产生影响。

### 2.7.4. bigkey 排查

- Redis 可以在执行 `redis-cli` 命令时带上 `–bigkeys` 选项，进而对整个数据库中的键值对大小情况进行统计分析。

- 使用 scan 命令扫描 key，然后手动统计。可以使用 `MEMORY USAGE` 命令查询某个 key 占用的内存空间。

- 利用第三方工具，如 `rdb-tools`。这是基于 rdb 文件统计的一个工具。

### 2.7.5. 数据倾斜问题

数据倾斜有两类：

1. 数据量倾斜：在某些情况下，实例上的数据分布不均衡，某个实例上的数据特别多。
	1. 存在 bigkey。对于 bigkey，尽量将数据打散，将集合类型数据拆分成小集合。业务层避免创建 bigkey 数据。
	2. `Slot` 分配不均匀。查看每个节点的 `Slot` 分配与使用情况，必要时重新分配 `Slot`。
	3. 使用 `Hash Tag` 导致数据集中到同一个 `Slot`。`Hash Tag` 是命令操作的 key 包含 `{}` 符号，此时 crc16 计算时只会计算 `{}` 符号内的内容从而导致数据落到一个 `Slot` 上。`Hash Tag` 可用于事务命令执行，因为大多事务要求在同一个节点执行。
2. 数据访问倾斜：虽然每个集群实例上的数据量相差不大，但是某个实例上的数据是热点数据，被访问得非常频繁。
	存在热点数据：对于热点数据加随机前缀或后缀，将其打散到多个节点。这种方式适合读多写少的数据，否则数据同步更新是一个问题。

## 2.8. 缓存问题

### 2.8.1. 缓存雪崩

原因：由于大批量的缓存突然失效导致请求都打到了数据库上。特别要注意一致性哈希环的场景，某个节点挂点数据分散到其他集群导致其他集群也挂掉。

措施：

- 缓存失效时间分开，设置随机失效时间。
- 不再设置生效时间，由后台线程进行数据更新。
- 控制数据库写入操作，只允许一个线程写入。
- 服务降级，熔断，限流机制。

### 2.8.2. 缓存穿透

原因：查询缓存中没有的数据，请求打到了数据库。更有甚者，数据库也没有这条数据，导致无法将数据更新到缓存从而导致请求一直到达数据库。

措施：

- 使用布隆过滤器拦截。
	布隆过滤器会有误判：由于采用固定 bit 的数组，使用多个哈希函数映射到多个 bit 上，有可能会导致两个不同的值都映射到相同的一组 bit 上。即布隆过滤器说不存在的一定不存在，说存在的不一定存在。

	布隆过滤器误判率和空间使用的计算：误判本质是因为哈希冲突，降低误判的方法是增加哈希函数 + 扩大整个 bit 数组的长度，但增加哈希函数意味着影响性能，扩大数组长度意味着空间占用变大，所以使用布隆过滤器，需要在误判率和性能、空间作一个平衡。
	
	布隆过滤器可以放在缓存和数据库的最前面。但需要注意**布隆过滤器的 bigkey 问题**：Redis 布隆过滤器是使用 String 类型实现的，存储的方式是一个 bigkey。
- 缓存空的数据，并设置过期时间。

- 对于热点数据不设置失效时间。

### 2.8.3. 缓存预热

原理：自动将热点数据加载到缓存中

### 2.8.4. 缓存更新

#### 2.8.4.1. Cache-Aside 旁路缓存模式

做法：
- 读请求
	1. 先查询缓存
	2. 缓存中有直接返回
	3. 缓存中没有查询数据库更新缓存
- 写请求
	1. 先更新数据库
	2. 然后删除缓存

选择原因：
- 为何不先删除缓存后更新数据库？
	若先删除缓存，在缓存删除期间产生读请求，可能会将未更新的数据查询到缓存中导致缓存脏数据。
- 若选择先删除缓存后更新数据库，如何解决一致性问题？
	采用延时双删，更新数据库后延时一段时间再次删除缓存，总共删除两次。这种做法不但需要两次删除而且有延迟，所以不推荐使用。
- 为何是删除缓存而不是更新缓存？
	若产生两个并发的写请求，因为各种原因导致先来请求缓存更新操作晚于后来的请求，同样会导致缓存脏数据。
- Cache-Aside 存在数据不一致的可能吗？
	存在，若缓存失效期间同时产生写请求与读请求，且读请求的缓存更新操作晚于写请求的缓存删除操作，这个时候也会出现数据不一致问题。这种情况要求缓存失效且读写同时触发，条件比较复杂。
	
	另一种情况是读请求先读到了缓存，写请求更新了数据。这个时候缓存与数据库数据不一致，也是一种一致性问题。若是业务对此要求严格一致，可采取加锁方式解决。

Cache-Aside 的异常补偿机制，删除缓存时存在缓存删除失败的问题，需要对此作出补偿策略：
- 删除重试机制，同步删除重试影响性能，因此可以使用异步重试删除，如使用 MQ。但这样引入了 MQ 中间件，以及删除失败后的逻辑可能需要业务作一定补偿。
- 监听 binlog 日志，解析 binlog 日志来处理缓存删除失败的问题。注意这里优先选择监听从数据库的 binlog 日志，防止主节点事务还未完成就过早的删除了缓存。
- 采用云服务商提供的 DTS (数据传输) 服务，DTS 服务适配了常见的数据源与数据操作场景，解决了如 binlog 日志回收，主备切换场景下的高可用问题。

适用点：
- 缓存数据计算逻辑复杂
- 数据一致性要求高
- 不存在 bigkey 或热点数据

#### 2.8.4.2. Read-Through 读穿透模式

流程和 Cache-Aside 模式相似，不同点在于 Read-Through 多了访问控制层，读请求只和访问控制层交换，访问控制层与数据库进行数据交互与同步。缓存能否命中与读请求无关。

#### 2.8.4.3. Write-Through 直写模式

同样提供了访问控制层来进行更高程度的封装，由访问控制层与数据库进行数据交互与同步。不同于 Cache-Aside 模式的是 Write-Through 不是删除缓存而是更新缓存。该模式适合写操作多且对一致性要求高的场景。

#### 2.8.4.4. Write-Behind 异步回写模式

写请求只更新缓存不更新数据库，数据库在合适的时机异步批量更新。

这种模式写延迟低，吞吐性好，但一致性弱，需要缓存做好高可用，适合于大量的写请求场景。如秒杀，MQ 的消息存储机制等。

#### 2.8.4.5. Write-Around 绕写模式

写请求不更新缓存，缓存设定失效时间，由失效时间自动更新。这种模式适用于一致性要求不高的业务场景。

### 2.8.5. 缓存降级

缓存失效或缓存服务器挂掉的情况下不去访问数据库直接返回内存数据或默认数据，以此减少降级对业务对影响操作，这是饮鸩止渴的操作，为了保证服务正常运行而不得已做出的措施。

在发生缓存雪崩时，为了防止引发连锁的数据库雪崩甚至是整个系统的崩溃，我们暂停业务应用对缓存系统的接口访问。再具体点说，就是业务应用调用缓存接口时缓存客户端请求，并不把请求发给 Redis 缓存实例而是直接返回，等到 Redis 缓存实例重新恢复服务后再允许应用请求发送到缓存系统。

服务熔断虽然可以保证数据库的正常运行，但是暂停了整个缓存系统的访问，对业务应用的影响范围大。为了尽可能减少这种影响，我们也可以进行请求限流。这里说的请求限流，就是指，我们在业务系统的请求入口前端控制每秒进入系统的请求数，避免过多的请求被发送到数据库。

### 2.8.6. 客户端缓存

`Redis 6.0` 新增了一个重要的特性，就是实现了**服务端协助的客户端缓存功能**，也称为跟踪（`Tracking`）功能。有了这个功能，业务应用中的 Redis 客户端就可以把读取的数据缓存在业务应用本地了，应用就可以直接在本地快速读取数据了。

`6.0` 实现的 `Tracking` 功能实现了两种模式：

1. 普通模式
	实例会在服务端记录客户端读取过的 key，并监测 key 是否有修改。一旦 key 的值发生变化，服务端会给客户端发送 `invalidate` 消息，通知客户端缓存失效了。
	
	在使用普通模式时，服务端对于记录的 key **只会报告一次 invalidate 消息**，也就是说，服务端在给客户端发送过一次 `invalidate` 消息后，如果 key 再被修改，此时服务端就不会再次给客户端发送 `invalidate` 消息。只有当客户端再次执行读命令时，服务端才会再次监测被读取的 key，并在 key 修改时发送 invalidate 消息。
	
	这样设计的考虑是节省有限的内存空间。毕竟如果客户端不再访问这个 key 了，而服务端仍然记录 key 的修改情况，就会浪费内存资源。
2. 广播模式
	服务端会给客户端广播所有 key 的失效情况。广播模式下，如果 key 被频繁修改，服务端会发送大量的失效广播消息，这就会消耗大量的网络带宽资源。
	
	在实际应用时，我们会让客户端注册希望跟踪的 key 的前缀，当带有注册前缀的 key 被修改时，服务端会把失效消息广播给所有注册的客户端。
	
	和普通模式不同，在**广播模式下，即使客户端还没有读取过 key，但只要它注册了要跟踪的 key，服务端都会把 key 失效消息通知给这个客户端**。

# 3. 高性能

## 3.1. Redis 效率高的原因

- C 语言实现，效率高。
- 纯内存操作。
- 基于非阻塞式的 IO 复用模型。
    基于 `select` / `epoll` 的事件回调机制，使 Redis 可以在网络 IO 阻塞时处理其他事情。由操作系统内核与客户端建立连接，Redis 监听这些连接的事件进行处理。
- 单线程避免上下文切换，各种锁操作。
    - Redis 的**瓶颈往往在于内存和网络带宽**，不在 CPU。
    - 单线程避免调度切换开销。	
- 丰富的数据结构，对数据存储做了优化，如压缩表、跳表。

## 3.2. Redis 为什么是单线程的

Redis 中的多线程：

- 在 `Redis 4.0` 中引入了多线程处理异步任务
- 在 `Redis 6.0` 中对网络模型实现了多线程 IO
	多线程用于处理网络数据的读写和协议解析：主线程将可读 Socket 分发给 IO 线程组进行并行请求解析，解析完毕的命令执行还是单线程的，执行结果交给 IO 线程组写回 Socket 是并行的。
	
	即将与 Socket 相关的读写操作变为并行执行的，以此减轻网络 IO 的负担。但是命令执行还是由主线程执行的，仍是单线程，且是线程安全的。

## 3.3. Redis 的数据类型

![image.png](https://r2.129870.xyz/img/202308040028860.png)

### 3.3.1. String

![image.png](https://r2.129870.xyz/img/202308050037475.png)

String 是最基本的 key-value 结构，key 是唯一标识，value 是具体的值，value 其实不仅是字符串，也可以是数字（整数或浮点数），value 最多可以容纳的数据长度是 512M。

String 类型的底层的数据结构实现主要是 int 和 SDS（简单动态字符串）。SDS 和我们认识的 C 字符串不太一样，之所以没有使用 C 语言的字符串表示，因为 SDS 相比于 C 的原生字符串有以下优势：
- **SDS 不仅可以保存文本数据，还可以保存二进制数据**。
    因为 `SDS` 使用 `len` 属性的值而不是空字符来判断字符串是否结束，并且 SDS 的所有 API 都会以处理二进制的方式来处理 SDS 存放在 `buf[]` 数组里的数据。所以 SDS 不光能存放文本数据，而且能保存图片、音频、视频、压缩文件这样的二进制数据。
- **SDS 获取字符串长度的时间复杂度是 O (1)**。
    因为 C 语言的字符串并不记录自身长度，所以获取长度的复杂度为 O (n)；而 SDS 结构里用 `len` 属性记录了字符串长度，所以复杂度为 `O(1)`。
- **Redis 的 SDS API 是安全的，拼接字符串不会造成缓冲区溢出**。
    因为 SDS 在拼接字符串之前会检查 SDS 空间是否满足要求，如果空间不够会自动扩容，所以不会导致缓冲区溢出的问题。

字符串对象的内部编码（encoding）有 3 种 ：**int、raw 和 embstr**。

![image.png](https://r2.129870.xyz/img/202308050040746.png)

- int
    如果一个字符串对象保存的是整数值，并且这个整数值可以用 `long` 类型来表示，那么字符串对象会将整数值保存在字符串对象结构的 `ptr` 属性里面（将 `void*` 转换成 long），并将字符串对象的编码设置为 `int`。
    
    ![image.png](https://r2.129870.xyz/img/202308050040178.png)
- raw
    如果字符串对象保存的是一个字符串，并且这个字符串的长度大于 44 字节（redis 5.0 版本），那么字符串对象将使用一个简单动态字符串（SDS）来保存这个字符串，并将对象的编码设置为 `raw`。
    
    ![image.png](https://r2.129870.xyz/img/202308050059181.png)
- embstr
    如果字符串对象保存的是一个字符串，并且这个字符申的长度小于等于 44 字节（redis 5.0 版本），那么字符串对象将使用一个简单动态字符串（SDS）来保存这个字符串，并将对象的编码设置为 `embstr`， `embstr` 编码是专门用于保存短字符串的一种优化编码方式。
    
    ![image.png](https://r2.129870.xyz/img/202308050058207.png)

可以看到 `embstr` 和 `raw` 编码都会使用 `SDS` 来保存值，但不同之处在于 **`embstr` 会通过一次内存分配函数来分配一块连续的内存空间来保存 `redisObject` 和 `SDS`**，而 **`raw` 编码会通过调用两次内存分配函数来分别分配两块空间来保存 `redisObject` 和 `SDS`**。Redis 这样做会有很多好处：

- `embstr` 编码将创建字符串对象所需的内存分配次数从 `raw` 编码的两次降低为一次。
- 释放 `embstr` 编码的字符串对象同样只需要调用一次内存释放函数。
- 因为 `embstr` 编码的字符串对象的所有数据都保存在一块连续的内存里面可以更好的利用 CPU 缓存提升性能。

但是 embstr 也有缺点的：如果字符串的长度增加需要重新分配内存时，整个 redisObject 和 sds 都需要重新分配空间，所以**embstr 编码的字符串对象实际上是只读的**，redis 没有为 embstr 编码的字符串对象编写任何相应的修改程序。当我们对 embstr 编码的字符串对象执行任何修改命令（例如 append）时，程序会先将对象的编码从 embstr 转换成 raw，然后再执行修改命令。

### 3.3.2. List

![image.png](https://r2.129870.xyz/img/202308050045916.png)

List 列表是简单的字符串列表，**按照插入顺序排序**，可以从头部或尾部向 List 列表添加元素。列表的最大长度为 `2^32 - 1`，也即每个列表支持超过 `40 亿` 个元素。

List 类型的底层数据结构是由**双向链表或压缩列表**实现的：
- 如果列表的元素个数小于 `512` 个（默认值，可由 `list-max-ziplist-entries` 配置），列表每个元素的值都小于 `64` 字节（默认值，可由 `list-max-ziplist-value` 配置），Redis 会使用**压缩列表**作为 List 类型的底层数据结构；
- 如果列表的元素不满足上面的条件，Redis 会使用**双向链表**作为 List 类型的底层数据结构；

但是**在 Redis 3.2 版本之后，List 数据类型底层数据结构就只由 quicklist 实现了，替代了双向链表和压缩列表**。

### 3.3.3. Hash

![image.png](https://r2.129870.xyz/img/202308050046281.png)

Hash 是一个键值对（key - value）集合，其中 value 的形式如： `value=[{field1，value1}，...{fieldN，valueN}]`。Hash 特别适合用于存储对象。

Hash 类型的底层数据结构是由**压缩列表或哈希表**实现的：

- 如果哈希类型元素个数小于 `512` 个（默认值，可由 `hash-max-ziplist-entries` 配置），所有值小于 `64` 字节（默认值，可由 `hash-max-ziplist-value` 配置）的话，Redis 会使用**压缩列表**作为 Hash 类型的底层数据结构；
- 如果哈希类型元素不满足上面条件，Redis 会使用**哈希表**作为 Hash 类型的底层数据结构。

**在 Redis 7.0 中，压缩列表数据结构已经废弃了，交由 listpack 数据结构来实现了**。

### 3.3.4. Set

![image.png](https://r2.129870.xyz/img/202308050047921.png)

Set 类型是一个无序并唯一的键值集合，它的存储顺序不会按照插入的先后顺序进行存储。一个集合最多可以存储 `2^32-1` 个元素。概念和数学中个的集合基本类似，可以交集，并集，差集等等，所以 Set 类型除了支持集合内的增删改查，同时还支持多个集合取交集、并集、差集。

Set 类型的底层数据结构是由**哈希表或整数集合**实现的：

- 如果集合中的元素都是整数且元素个数小于 `512` （默认值，`set-maxintset-entries` 配置）个，Redis 会使用**整数集合**作为 Set 类型的底层数据结构；
- 如果集合中的元素不满足上面条件，则 Redis 使用**哈希表**作为 Set 类型的底层数据结构。

### 3.3.5. Zset

![image.png](https://r2.129870.xyz/img/202308050049665.png)

Zset 类型（有序集合类型）相比于 Set 类型多了一个排序属性 score（分值），对于有序集合 ZSet 来说，每个存储元素相当于有两个值组成的，一个是有序集合的元素值，一个是排序值。有序集合保留了集合不能有重复成员的特性（分值可以重复），但不同的是，有序集合中的元素可以排序。

Zset 类型的底层数据结构是由**压缩列表或跳表**实现的：

- 如果有序集合的元素个数小于 `128` 个，并且每个元素的值小于 `64` 字节时，Redis 会使用**压缩列表**作为 Zset 类型的底层数据结构；
- 如果有序集合的元素不满足上面的条件，Redis 会使用**跳表**作为 Zset 类型的底层数据结构；

对于由跳表实现的 zset，zset 结构体里有两个数据结构：一个是跳表，一个是哈希表。这样的好处是既能进行高效的范围查询，也能进行高效单点查询。

```c
typedef struct zset {
    dict *dict;
    zskiplist *zsl;
} zset;
```

Zset 对象在执行数据插入或是数据更新的过程中，会依次在跳表和哈希表中插入或更新相应的数据，从而保证了跳表和哈希表中记录的信息一致。Zset 对象能支持范围查询（如 ZRANGEBYSCORE 操作），这是因为它的数据结构设计采用了跳表，而又能以常数复杂度获取元素权重（如 ZSCORE 操作），这是因为它同时采用了哈希表进行索引。

**在 Redis 7.0 中，压缩列表数据结构已经废弃了，交由 listpack 数据结构来实现了。**

### 3.3.6. BitMap

![image.png](https://r2.129870.xyz/img/202308050050075.png)

Bitmap，即位图，是一串连续的二进制数组（0 和 1），可以通过偏移量（offset）定位元素。BitMap 通过最小的单位 bit 来进行 `0|1` 的设置，表示某个元素的值或者状态，时间复杂度为 O (1)。

由于 bit 是计算机中最小的单位，使用它进行储存将非常节省空间，特别适合一些数据量大且使用**二值统计的场景**。

Bitmap 本身是用 String 类型作为底层数据结构实现的一种统计二值状态的数据类型。String 类型是会保存为二进制的字节数组，所以，Redis 就把字节数组的每个 bit 位利用起来，用来表示一个元素的二值状态，你可以把 Bitmap 看作是一个 bit 数组。

### 3.3.7. HyperLogLog

Redis HyperLogLog 是 Redis 2.8.9 版本新增的数据类型，是一种用于「统计基数」的数据集合类型，基数统计就是指统计一个集合中不重复的元素个数。但要注意，HyperLogLog 是统计规则是基于概率完成的，不是非常准确，标准误算率是 0.81%。所以，简单来说 HyperLogLog **提供不精确的去重计数**。

HyperLogLog 的优点是，在输入元素的数量或者体积非常非常大时，计算基数所需的内存空间总是固定的、并且是很小的。在 Redis 里面，**每个 HyperLogLog 键只需要花费 12 KB 内存，就可以计算接近 `2^64` 个不同元素的基数**，和元素越多就越耗费内存的 Set 和 Hash 类型相比，HyperLogLog 就非常节省空间。

### 3.3.8. Stream

Redis Stream 是 Redis 5.0 版本新增加的数据类型，Redis 专门为消息队列设计的数据类型。在 Redis 5.0 Stream 没出来之前，消息队列的实现方式都有着各自的缺陷，例如：

- 发布订阅模式，不能持久化也就无法可靠的保存消息，并且对于离线重连的客户端不能读取历史消息的缺陷。
- List 实现消息队列的方式不能重复消费，一个消息消费完就会被删除，而且生产者需要自行实现全局唯一 ID。

Stream 支持消息的持久化、支持自动生成全局唯一 ID、支持 ack 确认消息的模式、支持消费组模式等，让消息队列更加的稳定和可靠。

Stream 消息队列操作命令：

- XADD：插入消息，保证有序，可以自动生成全局唯一 ID。
- XLEN ：查询消息长度。
- XREAD：用于读取消息，可以按 ID 读取数据。
- XDEL ： 根据消息 ID 删除消息。
- DEL ：删除整个 Stream。
- XRANGE ：读取区间消息。
- XREADGROUP：按消费组形式读取消息。
- XPENDING 和 XACK。
    - XPENDING 命令可以用来查询每个消费组内所有消费者已读取、但尚未确认的消息。
    - XACK 命令用于向消息队列确认消息处理已完成。

## 3.4. Redis 的数据结构

### 3.4.1. 数据存储结构

#### 3.4.1.1. RedisObject

对于 Redis 的数据，不同数据类型都有些相同的元数据要记录（比如最后一次访问的时间、被引用的次数等），所以 Redis 会用一个 `RedisObject` 结构体来统一记录这些元数据，同时存储一个指针指向实际数据。因此对于存储在 Redis 中的对象，其结构如下：

![image.png](https://r2.129870.xyz/img/202308050103546.png)

对象结构里包含的成员变量：
- type，标识该对象是什么类型的对象（String 对象、 List 对象、Hash 对象、Set 对象和 Zset 对象）。
- encoding，标识该对象使用了哪种底层的数据结构；
- ptr，指向底层数据结构的指针。
- refcount，被引用次数。
- lru，最近一次访问时间。

#### 3.4.1.2. Hash dictEntry

Redis 是使用了一个哈希表保存所有键值对，哈希表的最大好处就是让我们可以用 O (1) 的时间复杂度来快速查找到键值对。哈希表其实就是一个数组，数组中的元素叫做哈希桶。

哈希桶存放的是指向键值对数据的指针（dictEntry*），这样通过指针就能找到键值对数据，然后因为键值对的值可以保存字符串对象和集合数据类型的对象，所以键值对的数据结构中并不是直接保存值本身，而是保存了 key 和 value 指针，分别指向了实际的键对象和值对象，这样一来，即使值是集合数据，也可以通过 value 指针找到。

![image.png](https://r2.129870.xyz/img/202308050107112.png)


- RedisDb 结构：表示 Redis 数据库的结构，结构体里存放了指向了 dict 结构的指针。
- dict 结构
    结构体里存放了 2 个哈希表正常情况下都是用哈希表 1，哈希表 2 只有在 rehash 的时候才用。
- ditctht 结构：表示哈希表的结构
    结构里存放了哈希表数组，数组中的每个元素都是指向一个哈希表节点结构（dictEntry）的指针。
- dictEntry 结构：表示哈希表节点
    结构里存放了 key 和  value 指针， key 指向的是 String 对象，而 value 则可以指向 String 对象，也可以指向集合类型的对象，比如 List 对象、Hash 对象、Set 对象和 Zset 对象。

#### 3.4.1.3. rehash

dict 结构体里定义了两个哈希表，之所以定义了 2 个哈希表，是因为进行 rehash 的时候需要用到。在正常服务请求阶段，插入的数据，都会写入到哈希表 1，此时的哈希表 2  并没有被分配空间。

随着数据逐步增多，触发了 rehash 操作，这个过程分为三步：

1. 给哈希表 2 分配空间，一般会比哈希表 1 大 2 倍。
2. 将哈希表 1 的数据迁移到哈希表 2 中。
3. 迁移完成后，哈希表 1 的空间会被释放，并把哈希表 2 设置为哈希表 1，然后在哈希表 2 新创建一个空白的哈希表，为下次 rehash 做准备。

为了避免 rehash 在数据迁移过程中，因拷贝数据的耗时，影响 Redis 性能的情况，所以 Redis 采用了**渐进式 rehash**，也就是将数据的迁移的工作不再是一次性迁移完成，而是分多次迁移。

扩容实现：从第一个索引开始一点一点的转移数据，具体的实现过程如下：

1. 给哈希表 2 分配更大的空间，例如是当前哈希表 1 大小的两倍。
2. 把哈希表 1 中的数据重新映射并拷贝到哈希表 2 中。
     `渐进式 rehash`：在**拷贝数据时正常处理请求，每处理一个请求就将这个索引上的所有数据拷贝到 hash 表 2 中**。等处理下一个请求时，再顺带拷贝哈希表 1 中的下一个索引位置的 entries。
    
    当 rehash 开始后，即使没有请求写入，也会**定时的迁移数据到新的 hash 桶中**。
3. 释放哈希表 1 的空间。

对于**扩容期间的数据写入与查询，哈希表元素的删除、查找、更新等操作都会在这两个哈希表进行**。

扩容时机：Redis 会使用装载因子（load factor）来判断是否需要做 rehash。装载因子的计算方式是：$哈希表中所有 entry 的个数 \div 哈希表的哈希桶个数$。Redis 会根据装载因子的两种情况，来触发 rehash 操作：

- $装载因子 \geq 1$，同时，哈希表被允许进行 rehash。
    **在进行 RDB 生成和 AOF 重写时，哈希表的 rehash 是被禁止的，这是为了避免对 RDB 和 AOF 重写造成影响**。
- $装载因子 \geq 5$，哈希桶里会有大量的链式哈希存在，性能会受到严重影响，此时，就立马开始做 rehash。

### 3.4.2. SDS 简单动态字符串

Redis 自己设计了一个字符串存储结构的原因是由于 C 语言的字符串存在以下问题：
- 字符串的结尾是以 “\0” 字符标识，字符串里面不能包含有 “\0” 字符，因此不能保存二进制数据。
    ![image.png](https://r2.129870.xyz/img/202308051532570.png)
- 操作字符串时间复杂度为 O（N）。
- 字符串操作函数不高效且不安全，比如有缓冲区溢出的风险，有可能会造成程序运行终止。

字符串类型在 Redis 中的实现为 `简单动态字符串 (Simple Dynamic String，SDS)` 格式，其存储结构如下（Redis5.0 版本）：

![image.png](https://r2.129870.xyz/img/202308051533706.png)

- len：记录了字符串长度
    因为 SDS 不需要用 “\0” 字符来标识字符串结尾了，而是**有个专门的 len 成员变量来记录长度，所以可存储包含 “\0” 的数据**。但是 SDS 为了兼容部分 C 语言标准库的函数， SDS 字符串结尾还是会加上 “\0” 字符。
- alloc：分配给字符数组的空间长度
    在修改字符串的时候，可以通过 $alloc - len$ 计算出剩余的空间大小，可以用来判断空间是否满足修改需求，如果不满足的话，就会自动将 SDS 的空间扩展至执行修改所需的大小。
    
    - 如果所需的 sds 长度**小于 1 MB**，按照**翻倍扩容**来执行的，即 $2 \times len$。
    - 如果所需的 sds 长度**超过 1 MB**，扩容长度是 $len+1MB$。
      
    在扩容 SDS 空间之前，SDS API 会优先检查未使用空间是否足够，如果不够的话，API 不仅会为 SDS 分配修改所必须要的空间，还会给 SDS 分配额外的未使用空间。这样的好处是，下次在操作 SDS 时，如果 SDS 空间够的话，API 就会直接使用剩余空间，而无须执行内存分配，**有效的减少内存分配次数**。
- flags：用来表示不同类型的 SDS
    一共设计了 5 种类型，分别是 sdshdr5、sdshdr8、sdshdr16、sdshdr32 和 sdshdr64。这 5 种类型的主要区别就在于，它们**数据结构中的 len 和 alloc 成员变量的数据类型不同**。
    
    **之所以 SDS 设计不同类型的结构体，是为了能灵活保存不同大小的字符串，从而有效节省内存空间**。比如，sdshdr16 类型的 len 和 alloc 的数据类型都是 uint16_t，表示字符数组长度和分配空间大小不能超过 2 的 16 次方。在保存小字符串时，结构头占用空间也比较少。
    
    除了设计不同类型的结构体，Redis 在编程上还**使用了专门的编译优化来节省内存空间**，即在 struct 声明了 `__attribute__ ((packed))` ，它的作用是：**告诉编译器取消结构体在编译过程中的优化对齐，按照实际占用字节数进行对齐**。
    
    ![image.png](https://r2.129870.xyz/img/202308051541865.png)

- buf\[]：字节数组，用来保存实际数据。
    不仅可以保存字符串，也可以保存二进制数据。

总的来说，Redis 的 SDS 结构在原本字符数组之上，增加了三个元数据：len、alloc、flags，用来解决 C 语言字符串的缺陷。

### 3.4.3. 链表

![image.png](https://r2.129870.xyz/img/202308051545038.png)

list 结构为链表提供了链表头指针 head、链表尾节点 tail、链表节点数量 len、以及可以自定义实现的 dup、free、match 函数。

Redis 的链表实现优点如下：
- listNode 链表节点的结构里带有 prev 和 next 指针，**获取某个节点的前置节点或后置节点的时间复杂度只需 O (1)，而且这两个指针都可以指向 NULL，所以链表是无环链表**。
- list 结构因为提供了表头指针 head 和表尾节点 tail，所以**获取链表的表头节点和表尾节点的时间复杂度只需 O (1)**。
- list 结构因为提供了链表节点数量 len，所以**获取链表中的节点数量的时间复杂度只需 O (1)**。
- listNode 链表节使用指针保存节点值，并且可以通过 list 结构的 dup、free、match 函数指针为节点设置该节点类型特定的函数，因此**链表节点可以保存各种不同类型的值**。

链表的缺陷也是有的：

- 链表每个节点之间的内存都是不连续的，意味着**无法很好利用 CPU 缓存**。
- 每个链表节点的值都需要一个链表节点结构头的分配，**内存开销较大**。

因此，Redis 3.0 的 List 对象在数据量比较少的情况下，会采用「压缩列表」作为底层数据结构的实现，它的优势是节省内存空间，并且是内存紧凑型的数据结构。然后在 Redis 5.0 设计了新的数据结构 listpack，沿用了压缩列表紧凑型的内存布局，最终在最新的 Redis 版本，将 Hash 对象和 Zset 对象的底层数据结构实现之一的压缩列表，替换成由 listpack 实现。

### 3.4.4. 压缩列表

#### 3.4.4.1. 压缩列表的定义

压缩列表是 Redis 为了节约内存而开发的，它是**由连续内存块组成的顺序型数据结构**，有点类似于数组。

![image.png](https://r2.129870.xyz/img/202308051549499.png)

压缩列表在表头有三个字段：
- zlbytes：记录整个压缩列表占用对内存字节数。
- zltail：记录压缩列表尾部节点的地址，通过该指针可以直接找到最后一个节点。
- zllen：记录压缩列表包含的节点数量。
- zlend：标记压缩列表的结束点，固定值 0xFF（十进制 255）。

在压缩列表中，如果我们要查找定位第一个元素和最后一个元素，可以通过表头三个字段（zllen）的长度直接定位，复杂度是 O (1)。而**查找其他元素时，就没有这么高效了，只能逐个查找，此时的复杂度就是 O (N) 了，因此压缩列表不适合保存过多的元素**。

压缩列表中**每个数据可以具有不同的长度**，因此具有**压缩**的特性。

#### 3.4.4.2. 压缩列表的存储

![image.png](https://r2.129870.xyz/img/202308051552740.png)

压缩列表之所以能节省内存，在于它是用一系列连续的 `entry` 保存数据。每个 `entry` 的元数据包括下面几部分：
- `prev_len`，表示前一个 entry 的长度，主要用于反向遍历
    `prev_len` 有两种取值情况：
    -  `1 字节` ：当上一个 `entry` 的长度**小于 254 字节**时，取值 1 字节。
        - 当上一个 entry 长度小于 `254` 字节时，`prev_len` 取值为 `1 字节`。
        - 对于 `254`，用于表示 `5 字节` 长度开始的标识。
        - 对于 `255`，为压缩列表中 `zlend` 默认取值。
    -  `5 字节` ：上一个 `entry` 的长度**大于等于 254 字节**
        - 第一部分为 `1 字节`，值为 `254`，表示这是一段 `5 字节` 表示的数据。
        - 第二部分为 `4 字节`，实际的长度。
- `encoding` ：表示编码方式。
    主要用于记录当前节点实际数据的类型和长度，类型主要有两种：字符串和整数。
    
    当我们往压缩列表中插入数据时，压缩列表就会根据数据类型是字符串还是整数，以及数据的大小，会使用不同空间大小的 prevlen 和 encoding 这两个元素里保存的信息，**这种根据数据大小和类型进行不同的空间大小分配的设计思想，正是 Redis 为了节省内存而采用的**。

    ![image.png](https://r2.129870.xyz/img/202308051557263.png)
    - 如果**当前节点的数据是整数**，则 encoding 会使用 **1 字节的空间**进行编码，也就是 encoding 长度为 1 字节。通过 encoding 确认了整数类型，就可以确认整数数据的实际大小了，比如如果 encoding 编码确认了数据是 int16 整数，那么 data 的长度就是 int16 的大小。
    - 如果**当前节点的数据是字符串，根据字符串的长度大小**，encoding 会使用 **1 字节/2 字节/5 字节的空间**进行编码，encoding 编码的前两个 bit 表示数据的类型，后续的其他 bit 标识字符串数据的实际长度，即 data 的长度。
- `data` ：保存实际数据。

这些 `entry` 连续在内存中，不需要再用额外的指针进行连接，这样就可以节省指针所占用的空间。

#### 3.4.4.3. 压缩列表的使用事项

当使用压缩列表存储时，我们尽量存储 `int` 数据，压缩列表在设计时每个 `entry` 都进行了优化，针对要存储的数据，会尽量选择占用内存小的方式存储（整数比字符串在存储时占用内存更小），这也有利于我们节省 Redis 的内存。

压缩列表是每个元素紧凑排列，而且每个元素存储了上一个元素的长度，所以**当修改其中一个元素超过一定大小时，会引发多个元素的级联调整**（前面一个元素发生大的变动，后面的元素都要重新排列位置，重新分配内存），这也会引发性能问题，需要注意。

![image.png](https://r2.129870.xyz/img/202308051601572.png)

采用压缩列表方式存储时，虽然可以节省内存空间，但是在查询指定元素时，都要遍历整个列表才能找到指定的元素。所以使用压缩列表方式存储时，虽然可以利用 CPU 高速缓存，但也不适合存储过多的数据（`hash-max-ziplist-entries` 和 `zset-max-ziplist-entries` 不宜设置过大），否则查询性能就会下降比较厉害。

### 3.4.5. 哈希表

Redis 的哈希表结构如下：

```c
typedef struct dictht {
    //哈希表数组
    dictEntry **table;
    //哈希表大小
    unsigned long size;  
    //哈希表大小掩码，用于计算索引值
    unsigned long sizemask;
    //该哈希表已有的节点数量
    unsigned long used;
} dictht;
```

哈希表是一个 dictEntry 数组，数组的每个元素是一个指向哈希表节点 dictEntry 的指针：

![image.png](https://r2.129870.xyz/img/202308051604922.png)

哈希表节点的结构如下：

```c
typedef struct dictEntry {
    //键值对中的键
    void *key;
  
    //键值对中的值
    union {
        void *val;
        uint64_t u64;
        int64_t s64;
        double d;
    } v;
    //指向下一个哈希表节点，形成链表
    struct dictEntry *next;
} dictEntry;
```

dictEntry 结构里不仅包含指向键和值的指针，还包含了指向下一个哈希表节点的指针，这个指针可以将多个哈希值相同的键值对链接起来，以此来解决哈希冲突的问题，这就是链式哈希。

dictEntry 结构里键值对中的值是一个联合体定义的，因此，键值对中的值可以是一个指向实际值的指针，或者是一个无符号的 64 位整数或有符号的 64 位整数或 double 类的值。这么做的好处是可以节省内存空间，因为当值是整数或浮点数时，就可以将值的数据内嵌在 dictEntry 结构里，无需再用一个指针指向实际的值，从而节省了内存空间。

### 3.4.6. 整数集合

整数集合本质上是一块连续内存空间，它的结构定义如下：

```c
typedef struct intset {
    //编码方式
    uint32_t encoding;
    //集合包含的元素数量
    uint32_t length;
    //保存元素的数组
    int8_t contents[];
} intset;
```

contents 数组的真正类型取决于 encoding 属性的值，例如：

- INTSET_ENC_INT16
- INTSET_ENC_INT32
- INTSET_ENC_INT64

**不同类型的 contents 数组，意味着数组的大小也会不同**。

整数集合会有一个升级规则，当将一个新元素加入到整数集合里面，如果**新元素的类型（int32_t）比整数集合现有所有元素的类型（int16_t）都要长时，整数集合需要先进行升级**，也就是按新元素的类型（int32_t）扩展 contents 数组的空间大小，然后才能将新元素加入到整数集合里，当然升级的过程中，也要维持整数集合的有序性。

整数集合升级的过程不会重新分配一个新类型的数组，而是在原本的数组上扩展空间，然后在将每个元素按间隔类型大小分割，例如若 encoding 属性值为 INTSET_ENC_INT16，则每个元素的间隔就是 16 位。

![image.png](https://r2.129870.xyz/img/202308051618161.png)

整数集合按照数据类型的最大值占用长度来存储数据。因此，整数集合升级的好处是**节省内存资源**。但需要注意的是，整数集合并**不支持降级操作**，一旦对数组进行了升级，就会一直保持升级后的状态。

### 3.4.7. 跳表

#### 3.4.7.1. 跳表的基本原理

链表在查找元素的时候，因为需要逐一查找，所以查询效率非常低，时间复杂度是 O (N)，于是就出现了跳表。**跳表是在链表基础上改进过来的，实现了一种「多层」的有序链表**，跳表的查找复杂度是 O(logN)。

下图展示了一个层级为 3 的跳表：

![image.png](https://r2.129870.xyz/img/202308051626408.png)

#### 3.4.7.2. Redis 跳表的实现

跳表节点的数据结构如下：

```c
typedef struct zskiplistNode {
    //Zset 对象的元素值
    sds ele;
    //元素权重值
    double score;
    //后向指针
    struct zskiplistNode *backward;
  
    //节点的level数组，保存每层上的前向指针和跨度
    struct zskiplistLevel {
        struct zskiplistNode *forward;
        unsigned long span;
    } level[];
} zskiplistNode;
```

Zset 对象要同时保存元素和元素的权重，对应到跳表节点结构里就是 sds 类型的 ele 变量和 double 类型的 score 变量。每个跳表节点都有一个指针，指向前一个节点，目的是为了方便从跳表的尾节点开始访问节点，这样倒序查找时很方便。

跳表是一个带有层级关系的链表，而且每一层级可以包含多个节点，每一个节点通过指针连接起来，实现这一特性就是靠跳表节点结构体中的**zskiplistLevel 结构体类型的 level 数组**。

level 数组中的每一个元素代表跳表的一层，也就是由 zskiplistLevel 结构体表示，比如 leve\[0] 就表示第一层，leve\[1] 就表示第二层。zskiplistLevel 结构体里定义了指向下一个跳表节点的指针和跨度两个属性，跨度时用来记录两个节点之间的距离。

![image.png](https://r2.129870.xyz/img/202308051630117.png)

跳表结构体如下：

```c
typedef struct zskiplist {
    struct zskiplistNode *header, *tail;
    unsigned long length;
    int level;
} zskiplist;
```

- 跳表的头尾节点，便于在 O (1)时间复杂度内访问跳表的头节点和尾节点。
- 跳表的长度，便于在 O (1)时间复杂度获取跳表节点的数量。
- 跳表的最大层数，便于在 O (1)时间复杂度获取跳表中层高最大的那个节点的层数量。

#### 3.4.7.3. 跳表的查询过程

查找一个跳表节点的过程时，跳表会从头节点的最高层开始，逐一遍历每一层。在遍历某一层的跳表节点时，会用跳表节点中的 SDS 类型的元素和元素的权重来进行判断，共有三个判断条件：
- 如果当前节点的权重小于要查找的权重时，跳表就会访问该层上的下一个节点。
- 如果当前节点的权重等于要查找的权重时，并且当前节点的 SDS 类型数据小于要查找的数据时，跳表就会访问该层上的下一个节点。
- 如果上面两个条件都不满足，或者下一个节点为空时，跳表就会使用目前遍历到的节点的 level 数组里的下一层指针，然后沿着下一层指针继续查找，这就相当于跳到了下一层接着查找。

![image.png](https://r2.129870.xyz/img/202308051633026.png)

如果要查找{元素：abcd，权重：4}的节点，查找的过程是这样的：

- 先从头节点的最高层开始，L2 指向了{元素：abc，权重：3}节点，这个节点的权重比要查找节点的小，所以要访问该层上的下一个节点；
- 但是该层的下一个节点是空节点（ leve\[2]指向的是空节点），于是就会跳到{元素：abc，权重：3}节点的下一层去找，也就是 leve\[1];
- {元素：abc，权重：3}节点的 leve\[1] 的下一个指针指向了{元素：abcde，权重：4}的节点，然后将其和要查找的节点比较。虽然{元素：abcde，权重：4}的节点的权重和要查找的权重相同，但是当前节点的 SDS 类型数据大于要查找的数据，所以会继续跳到{元素：abc，权重：3}节点的下一层去找，也就是 leve\[0]；
- {元素：abc，权重：3}节点的 leve\[0] 的下一个指针指向了{元素：abcd，权重：4}的节点，该节点正是要查找的节点，查询结束。

#### 3.4.7.4. 跳表节点的生成

**跳表的相邻两层的节点数量最理想的比例是 2:1，查找复杂度可以降低到 O (logN)**。

![image.png](https://r2.129870.xyz/img/202308051637984.png)

那怎样才能维持相邻两层的节点数量的比例为 2 : 1 呢？如果采用新增节点或者删除节点时，来调整跳表节点以维持比例的方法的话，会带来额外的开销。

Redis 则采用一种巧妙的方法是，**跳表在创建节点的时候，随机生成每个节点的层数**，并没有严格维持相邻两层的节点数量比例为 2 : 1 的情况。**跳表在创建节点时候，会生成范围为\[0-1]的一个随机数，如果这个随机数小于 0.25，那么层数就增加 1 层，然后继续生成下一个随机数，直到随机数的结果大于 0.25 结束，最终确定该节点的层数**。

这样的做法，相当于每增加一层的概率不超过 25%，层数越高，概率越低，层高最大限制是 64。**如果层高最大限制是 64，那么在创建跳表头节点的时候，就会直接创建 64 层高的头节点**，节点值指向都为空。

#### 3.4.7.5. 跳表与平衡树的抉择

为什么 Zset 的实现用跳表而不用平衡树呢？Redis 作者出于以下几点考虑而使用了跳表，主要是从内存占用、对范围查找的支持、实现难易程度这三方面总结的原因：

- **从内存占用上来比较，跳表比平衡树更灵活一些**
    跳表基本不是非常内存密集型的。改变关于节点具有给定级别数的概率的参数将使其比 btree 占用更少的内存。
-  **在做范围查找的时候，跳表比平衡树操作要简单**
    Zset 经常需要执行 ZRANGE 或 ZREVRANGE 的命令，即作为链表遍历跳表。通过此操作，跳表的缓存局部性至少与其他类型的平衡树一样好。
    
    在平衡树上，我们找到指定范围的小值之后，还需要以中序遍历的顺序继续寻找其它不超过大值的节点。如果不对平衡树进行一定的改造，这里的中序遍历并不容易实现。而在跳表上进行范围查找就非常简单，只需要在找到小值之后，对第 1 层链表进行若干步的遍历就可以实现。
- 它们更易于实现、调试等
    **从算法实现难度上来比较，跳表比平衡树要简单得多**。平衡树的插入和删除操作可能引发子树的调整，逻辑复杂，而跳表的插入和删除只需要修改相邻节点的指针，操作简单又快速。

### 3.4.8. quicklist

压缩列表是通过紧凑型的内存布局节省了内存开销，但是因为它的结构设计，如果保存的元素数量增加，或者元素变大了，压缩列表会有「连锁更新」的风险，一旦发生，会造成性能下降。

quicklist 解决办法：**通过控制每个链表节点中的压缩列表的大小或者元素个数，来规避连锁更新的问题。因为压缩列表元素越少或越小，连锁更新带来的影响就越小，从而提供了更好的访问性能。**

![image.png](https://r2.129870.xyz/img/202308051647183.png)

quicklistNode 结构体里包含了前一个节点和下一个节点指针，这样每个 quicklistNode 形成了一个双向链表。但是**链表节点的元素不再是单纯保存元素值，而是保存了一个压缩列表**，所以 quicklistNode 结构体里有个指向压缩列表的指针。

在向 quicklist 添加一个元素的时候，不会像普通的链表那样，直接新建一个链表节点。而是会**检查插入位置的压缩列表是否能容纳该元素**，如果能容纳就直接保存到 quicklistNode 结构里的压缩列表，如果不能容纳，才会新建一个新的 quicklistNode 结构。

quicklist 会控制 quicklistNode 结构里的压缩列表的大小或者元素个数，来规避潜在的连锁更新的风险，但是这并没有完全解决连锁更新的问题。

### 3.4.9. listpack

quicklist 虽然通过控制 quicklistNode 结构里的压缩列表的大小或者元素个数，来减少连锁更新带来的性能影响，但是并没有完全解决连锁更新的问题，因为 quicklistNode 还是用了压缩列表来保存元素。

Redis 在 5.0 新设计一个数据结构叫 listpack，目的是替代压缩列表，它最大特点是 listpack 中每个节点不再包含前一个节点的长度了，压缩列表每个节点正**因为需要保存前一个节点的长度字段，就会有连锁更新的隐患**。

listpack 采用了压缩列表的很多优秀的设计，比如还是用一块连续的内存空间来紧凑地保存数据，并且为了节省内存的开销，listpack 节点会采用不同的编码方式保存不同大小的数据。

![image.png](https://r2.129870.xyz/img/202308051650864.png)

listpack 头包含两个属性，分别记录了 listpack 总字节数和元素数量，然后 listpack 末尾也有个结尾标识。图中的 listpack entry 就是 listpack 的节点。

每个 listpack 节点结构如下：

![image.png](https://r2.129870.xyz/img/202308051650228.png)

主要包含三个方面内容：
- encoding：定义该元素的编码类型，会对不同长度的整数和字符串进行编码。
- data：实际存放的数据。
- len：$encoding + data$ 的总长度。

可以看到，**listpack 没有压缩列表中记录前一个节点长度的字段了，listpack 只记录当前节点的长度，当我们向 listpack 加入一个新元素的时候，不会影响其他节点的长度字段的变化，从而避免了压缩列表的连锁更新问题**。

那么在没有存储前一个节点的长度下，listpack 如何[实现向前查找](https://juejin.cn/post/7093530299866284045)的呢？len 的特殊编码方式：len 每个字节的最高位，是用来表示当前字节是否为 len 的最后一个字节，这里存在两种情况，分别是：
- 最高位为 1，表示 len 还没有结束，当前字节的左边字节仍然表示 len 的内容；
- 最高位为 0，表示当前字节已经是 len 最后一个字节了。而 len 每个字节的低 7 位，则记录了实际的长度信息。

正是因为有了 len 的特别编码方式，redis 就可以从当前列表项起始位置的指针开始，向左逐个字节解析，得到前一项的 entry  的 len 值，也就可以得到对应 entry 的总长度，从而得出 entry 的总长度；减去 entry 的总长度，就得到了前一个 entry 的地址。

### 3.4.10. BitMap

`Bitmap` 本身是用 `String` 类型作为底层数据结构实现的一种**统计二值状态的数据类型**。`String` 类型是会保存为二进制的字节数组，所以，Redis 就把字节数组的每个 bit 位利用起来，用来表示一个元素的二值状态。

可以把 `Bitmap` 看作是一个 bit 数组。Bitmap 提供了 `GETBIT` / `SETBIT` 操作，使用一个偏移值 `offset` 对 bit 数组的某一个 bit 位进行读和写。不过，需要注意的是，`Bitmap` 的偏移量是从 0 开始算的，也就是说 `offset` 的最小值是 `0`。当使用 `SETBIT` 对一个 bit 位进行写操作时，这个 bit 位会被设置为 `1`。`Bitmap` 还提供了 `BITCOUNT` 操作，用来统计这个 bit 数组中所有 `1` 的个数。

### 3.4.11. HyperLogLog

`HyperLogLog` 是一种用于统计基数的数据集合类型，它的最大优势就在于，**当集合元素数量非常多时，它计算基数所需的空间总是固定的，而且还很小**。

在 Redis 中，每个 `HyperLogLog` 只需要花费 `12 KB` 内存，就可以计算接近 $2^{64}$ 个元素的基数。

`HyperLogLog` 的统计规则是基于概率完成的，所以它给出的统计结果是有一定误差的，标准误算率是 `0.81%`。

### 3.4.12. Geospatial

GEO 类型的底层数据结构就是用 `Sorted Set` 来实现的，其中 `Set` 的 key 为数据的 key，而 value 为经纬度的编码值。

为了能高效地对经纬度进行比较，Redis 采用了业界广泛使用的 `GeoHash` 编码方法，这个方法的基本原理就是 `二分区间，区间编码`。

在进行第一次二分区时，经度范围\[-180, 180]会被分成两个子区间：\[-180, 0) 和\[0, 180]。此时，我们可以查看一下要编码的经度值落在了左分区还是右分区。如果是落在左分区，我们就用 0 表示；如果落在右分区，就用 1 表示。

这样一来，每做完一次二分区，我们就可以得到 1 位编码值。然后，我们再对经度值所属的分区再做一次二分区，同时再次查看经度值落在了二分区后的左分区还是右分区，按照刚才的规则再做 1 位编码。当做完 N 次的二分区后，经度值就可以用一个 N bit 的数来表示了。

![](https://r2.129870.xyz/img/20220518234740.png)
![](https://r2.129870.xyz/img/20220518234746.png)
![](https://r2.129870.xyz/img/20220518234819.png)

当一组经纬度值都编完码后，我们再把它们的各自编码值组合在一起，组合的规则是：最终编码值的偶数位上依次是经度的编码值，奇数位上依次是纬度的编码值，即偶数位从 0 开始，奇数位从 1 开始。

用了 `GeoHash` 编码后，原来无法用一个权重分数表示的一组经纬度（116.37，39.86）就可以用 1110011101 这一个值来表示，就可以保存为 `Sorted Set` 的权重分数了。

使用 `GeoHash` 编码后，我们相当于把整个地理空间划分成了一个个方格，每个方格对应了 `GeoHash` 中的一个分区。所以，我们使用 `Sorted Set` 范围查询得到的相近编码值，在实际的地理空间上，也是相邻的方格，这就可以实现 LBS 应用“搜索附近的人或物”的功能了。

有的编码值虽然在大小上接近，但实际对应的方格却距离比较远。例如，我们用 4 位来做 GeoHash 编码，把经度区间\[-180, 180]和纬度区间\[-90, 90]各分成了 4 个分区，一共 16 个分区，对应了 16 个方格。编码值为 0111 和 1000 的两个方格就离得比较远，如下图所示：所以，为了避免查询不准确问题，我们可以同时查询给定经纬度所在的方格周围的 4 个或 8 个方格。

![](https://r2.129870.xyz/img/20220518235015.png)

## 3.5. Pipline

`pipline` 将多次 IO 压缩成一次，但是要求管道中的指令没有因果关系。使用 `pipline` 可以实现请求/响应的功能：客户端未读取服务端响应时服务端可处理新的请求，客户端发送多个命令时只需等待服务端最终结果。

## 3.6. 引起 Redis 阻塞的几种操作

- 集合全量查询和聚合操作。
- bigkey 删除，可以利用子线程进行，称之为 `lazy-free`。
    `lazy-free` 是 `Redis 4.0` 新增的功能，但是默认是关闭的，需要手动开启。手动开启 `lazy-free` 时，有 4 个选项可以控制，分别对应不同场景下，是否开启异步释放内存机制： 
    -  `lazyfree-lazy-expire` ：key 在过期删除时尝试异步释放内存。
    -  `lazyfree-lazy-eviction` ：内存达到 `max memory` 并设置了淘汰策略时尝试异步释放内存。
    -  `lazyfree-lazy-server-del` ：执行 `RENAME` / `MOVE` 等命令或需要覆盖一个 key 时，删除旧 key 尝试异步释放内存。
    -  `replica-lazy-flush` ：主从全量同步，从库清空数据库时异步释放内存
- 清空数据库。
- AOF 日志同步写，可以利用子线程进行。
- 从库加载 RDB 文件。

### 3.6.1. Lazy-free

即使开启了 `lazy-free`，如果直接使用 `DEL` 命令还是会同步删除 key，只有使用 `UNLINK` 命令才会可能异步删除 key。

上面提到开启 `lazy-free` 的场景，除了 `replica-lazy-flush` 之外，其他情况都只是可能去异步释放 key 的内存，并不是每次必定异步释放内存的。 

开启 `lazy-free` 后，**Redis 在释放一个 key 的内存时首先会评估代价**，如果释放内存的代价很小，那么就直接在主线程中操作了，没必要放到异步线程中执行（不同线程传递数据也会有性能消耗）。什么情况才会真正异步释放内存？这和 key 的类型、编码方式、元素数量都有关系（详细可参考源码中的 `lazyfreeGetFreeEffort` 函数）：

- 当 `Hash` / `Set` 底层采用哈希表存储（非 `ziplist` / `int` 编码存储）时，并且元素数量超过 64 个。
- 当 `ZSet` 底层采用跳表存储（非 `ziplist` 编码存储）时，并且元素数量超过 64 个。
- 当 `List` 链表节点数量超过 64 个（注意，不是元素数量，而是链表节点的数量，List 的实现是在每个节点包含了若干个元素的数据，这些元素采用 `ziplist` 存储）。

只有以上这些情况，在删除 key 释放内存时，才会真正放到异步线程中执行，其他情况一律还是在主线程操作。也就是说 `String`（不管内存占用多大）、`List`（少量元素）、`Set`（`int` 编码存储）、`Hash` / `ZSet`（`ziplist` 编码存储）这些情况下的 key 在释放内存时，依旧在主线程中操作。 

可见，即使开启了 `lazy-free`，`String` 类型的 bigkey 在删除时依旧有阻塞主线程的风险。所以，即便 Redis 提供了 `lazy-free`，还是尽量不要在 Redis 中存储 bigkey。 

**Redis 在设计评估释放内存的代价时，不是看 key 的内存占用有多少，而是关注释放内存时的工作量有多大**。从上面分析基本能看出，如果需要释放的内存是连续的，Redis 作者认为释放内存的代价比较低，就放在主线程做。如果释放的内存不连续（大量指针类型的数据），这个代价就比较高，所以才会放在异步线程中去执行。

除了主动调用 unlink 命令实现异步删除之外，我们还可以通过配置参数，**达到某些条件的时候自动进行异步删除**。主要有 4 种场景，默认都是关闭的：

```txt
lazyfree-lazy-eviction no
lazyfree-lazy-expire no
lazyfree-lazy-server-del
noslave-lazy-flush no
```

它们代表的含义如下：
- `lazyfree-lazy-eviction`：表示当 Redis 运行内存超过 maxmeory 时，是否开启 lazy free 机制删除。
- `lazyfree-lazy-expire`：表示设置了过期时间的键值，当过期之后是否开启 lazy free 机制删除。
- `lazyfree-lazy-server-del`：有些指令在处理已存在的键时，会带有一个隐式的 del 键的操作，比如 rename 命令，当目标键已存在，Redis 会先删除目标键，如果这些目标键是一个 big key，就会造成阻塞删除的问题，此配置表示在这种场景中是否开启 lazy free 机制删除。
- `slave-lazy-flush`：针对 slave (从节点) 进行全量数据同步，slave 在加载 master 的 RDB 文件前，会运行 flushall 来清理自己的数据，它表示此时是否开启 lazy free 机制删除。

建议开启其中的 `lazyfree-lazy-eviction`、`lazyfree-lazy-expire`、`lazyfree-lazy-server-del` 等配置，这样就可以有效的提高主线程的执行效率。

## 3.7. Redis 高效利用 CPU 的方式

### 3.7.1. CPU 架构之 NUMA 架构

![](https://r2.129870.xyz/img/20220519125629.png)

多核 CPU 通过共享总线对内存进行访问，那么对总线对争用将会称为性能对阻碍。

![](https://r2.129870.xyz/img/20220519125740.png)

`Non-Uniform Memory Access`，`NUMA` 架构通过把 CPU 和临近的 RAM 当做一个 `node`，CPU 会优先访问距离近的 RAM。同时，CPU 之间有一个快速通道连接，所以每个 CPU 还是访问到所有的 RAM 位置（只是速度会有差异）。

采用 `NUMA` 架构虽然减少了总线的争用，但是**若程序发生了 CPU 的切换，那么 CPU 中的缓存数据就要重新加载**，这是 `NUMA` 架构存在的问题：

![](https://r2.129870.xyz/img/20220519130036.png)

### 3.7.2. Redis 进行 CPU 层面的优化

#### 3.7.2.1. 绑定 CPU 与 Redis 程序

如果在 CPU 多核场景下，Redis 实例被频繁调度到不同 CPU 核上运行的话，那么，对 Redis 实例的请求处理时间影响就更大了。每调度一次，**一些请求就会受到运行时信息、指令和数据重新加载过程的影响**，这就会导致某些请求的延迟明显高于其他请求。因此可以把 Redis 与运行的 CPU 进行绑定，避免 CPU 的切换带来的数据重载问题：

```bash
taskset -c 0 ./redis-server
```

 Redis 实例和网络中断程序的数据交互：网络中断处理程序从网卡硬件中读取数据，并把数据写入到操作系统内核维护的一块内存缓冲区。内核会通过 epoll 机制触发事件，通知 Redis 实例，Redis 实例再把数据从内核的内存缓冲区拷贝到自己的内存空间。那么，在 CPU 的 `NUMA` 架构下，当网络中断处理程序、Redis 实例分别和 CPU 核绑定后，就会有一个潜在的风险：**如果网络中断处理程序和 Redis 实例各自所绑的 CPU 核不在同一个 CPU Socket 上，那么，Redis 实例读取网络数据时，就需要跨 CPU Socket 访问内存**，这个过程会花费较多时间。
 
![](https://r2.129870.xyz/img/20220519130422.png)
![](https://r2.129870.xyz/img/20220519130432.png)

所以，为了避免 Redis 跨 CPU Socket 访问网络数据，我们最好把网络中断程序和 Redis 实例绑在同一个 CPU Socket 上，这样一来，Redis 实例就可以直接从本地内存读取网络数据了，如下图所示：

![](https://r2.129870.xyz/img/20220519130503.png)

#### 3.7.2.2. 绑核带来的问题

当我们把 Redis 实例绑到一个 CPU 逻辑核上时，就会导致子进程、后台线程和 Redis 主线程竞争 CPU 资源，一旦子进程或后台线程占用 CPU 时，主线程就会被阻塞，导致 Redis 请求延迟增加。

对于上述情况，我们可以采取以下策略优化：

- 一个 Redis 实例对应绑一个物理核：在给 Redis 实例绑核时，不要把一个实例和一个逻辑核绑定，而要和一个物理核绑定。也就是说，把一个物理核的 2 个逻辑核都用上。
- 优化 Redis 源码：通过修改 Redis 源码，把子进程和后台线程绑到不同的 CPU 核上。Redis 6.0 后，可以支持 CPU 核绑定的配置操作了。

虽然与 CPU 进行关系绑定可以在一定程度上可以带来收益，但是我们仍需要关注 CPU 层面的使用情况。若绑定的 CPU 处于忙碌状态，上面已经绑定了其他的程序，那么 CPU 缓存的公用就会带来淘汰问题。

# 4. 高可用

## 4.1. Redis 的主从同步

### 4.1.1. 主从同步的过程

![](https://r2.129870.xyz/img/20220517231216.png)

主从同步用于备份主库的数据，从库通过与主库建立连接而不断获取到主库到数据。建立连接的步骤如下：

1. 从库使用 `replicaof` / `slaveof` (5.0 之前) 来开始主库建立连接。
2. 从库发送 `psync` 命令传递同步信息
    首次建立连接 `runId` 为 -1，`offset` 为 -1。`runId` 为机器 ID，`offset` 为复制进度，首次 -1 表示从头开始。
3. 主库响应从库信息
    主库通过 `FULLRESYNC` 响应命令带上两个参数：主库 `runID` 和主库目前的复制进度 `offset`，返回给从库。
4. 主库生成备份文件
    主库执行 `bgsave` 命令得到 RDB 文件给从库。从库接收到文件后清空当前数据，然后解析 RDB 恢复数据。使用 RDB 文件有以下几点原因：
    - RDB 文件是压缩过的二进制文件，文件内容小。对于网络传输快。
    - RDB 文件直接使用二进制协议解析还原数据，避免 AOF 文件的命令重复执行（一条数据的多次修命令）。
5. 缓冲写入命令
    对于生成 RDB 文件期间和从库解析 RDB 文件期间产生的数据操作命令，记录到了主库的 `replication buffer` 中，待后续从库解析完后再根据这些命令补充数据。
6. 实时传播
    首次备份完毕后主从之间通过长连接维护通信，主库将每次写入命令都推送到从库。
    
    进入长连接以后，主从之间会按一定的频率通信，确认双方存活状态：
    - Redis 主节点默认每隔 10 秒对从节点发送 ping 命令，判断从节点的存活性和连接状态，可通过参数 repl-ping-slave-period 控制发送频率。
    - Redis 从节点每隔 1 秒发送 replconf ack{offset} 命令，给主节点上报自身当前的复制偏移量。
        上报自身复制偏移量，检查复制数据是否丢失，如果从节点数据丢失，再从主节点的复制缓冲区中拉取丢失数据。

在 `Redis Cluster` 模式下，若发生了主从切换，切换时间由于太长心跳检测超时会被集群标记为异常。当过多节点出现这种情况时就会导致集群异常。所以需要注意心跳检测时间 `cluster-node-timeout` 的配置。

### 4.1.2. 主从同步的重连

![](https://r2.129870.xyz/img/20220517234150.png)

主从连接建立完毕以后就可以正常同步数据了，但是当主从之间因为某些原因断开重新建立连接后，双方数据怎么继续同步呢？

对于主库，在每次收到写入操作后，会将命令存入到 `repl_backlog_buffer` 环形缓冲区。主从之间的通信，会有一个 `replication buffer`。`repl_backlog_buffer` 用于记录主库的写入命令，对于主库来说只有一个。而 `replication buffer` 是对每一个从库都有的，这用于主从之间发送缓冲数据使用。这两者之间更为详细的对比如下：

- `repl_backlog_buffer` 缓存待同步命令
    `repl_backlog_buffer` 是为了从库断开之后，如何找到主从差异数据而设计的环形缓冲区，从而避免全量同步带来的性能开销。**各个从节点的 offset 偏移量都是相对该缓冲区而言的**，只要基于每个从节点的 offset 和当前 `repl_backlog_buffer` 的写入位置就能计找到还未被同步的命令。若一个从库都没有，那么这个缓冲区存在就没有意义了，此时会被释放。
- `replication buffer` 是为了主从之间数据通信缓冲所使用的
    这个缓冲区是一个数据发送缓冲区，就像 Mysql 中的 [[Mysql大纲#4 3 数据流转过程|net_buffer]]。这个 buffer 是针对每一个从库都会有一个的。不管是全量同步还是增量同步，都会使用到这个缓冲区。

如果主从在传播命令时，因为某些原因从库处理得非常慢，那么主库上的这个 `replication buffer` 就会持续增长，消耗大量的内存资源，甚至 OOM。所以 Redis 提供了 `client-output-buffer-limit` 参数限制这个 buffer 的大小，如果**超过限制，主库会强制断开这个 client 的连接**，也就是说从库处理慢导致主库内存 buffer 的积压达到限制后，主库会强制断开从库的连接，此时**主从复制会中断，中断后如果从库再次发起复制请求，那么此时可能会导致恶性循环，引发复制风暴**，这种情况需要格外注意。

对于缓冲区 `repl_backlog_buffer`，主库使用 `master_repl_offset` 来记录写入的位置。从库使用 `slave_repl_offset` 记录自己从主库读取数据的位置。这样在主从之间数据通信的时候，只需要将两个 offset 之间的数据进行同步即可。

对于主从重新建立连接后，有两种情况：
1. 主库的 `master_repl_offset` 还未追赶上从库的 `slave_repl_offset`，这时只要发送 offset 之间的数据即可。
2. 而如果主库的 `master_repl_offset` 已经追赶上从库的 `slave_repl_offset`，那么将会进行数据的覆盖。数据覆盖后从库就不能通过 offset 进行数据同步了，此时只能进行全量数据同步。

因此需要特别留意 `repl_backlog_size` 这个配置参数。如果它配置得过小，在增量复制阶段，可能会导致从库的复制进度赶不上主库，进而导致从库重新进行全量复制。

对于 `repl_backlog_buffer` 的大小，可以通过 `repl_backlog_size` 参数进行控制。这个参数和所需的缓冲空间大小有关。缓冲空间的计算公式是：

 $$
 缓冲空间大小 = 主库写入命令速度 \times 操作大小 - 主从库间网络传输命令速度 \times 操作大小
 $$
 在实际应用中，考虑到可能存在一些突发的请求压力，我们通常需要把这个缓冲空间扩大一倍，即 $repl\_backlog\_size = 缓冲空间大小 \times 2$，这也就是 `repl_backlog_size` 的最终值。

### 4.1.3. 主从数据不一致问题

主从数据可能出现不一致的场景如下：

- 主库数据更改命令未及时同步到从库，或者已经同步到从库但从库在执行其他命令如集合操作而阻塞。所以主从数据不一致。
- 主库数据已过期，但未删除。当触发删除逻辑时会给从库同步删除命令。在 3.2 版本以前，从库若读到过期数据是不会触发删除逻辑的，惰性删除策略只在主库有。在 3.2 版本后若从库读到过期数据会返回空。
- 主库数据接收到了设置过期命令的命令，该命令传到从库会有延迟。如 `expire key 60` 设定 60s 后过期，可是从库接收到时开始执行时间和主库已经不一致了，会有这段延迟。使用 `expire at` 没有这个问题。

可以使用 `slave-serve-stale-data` 来配置从库读取命令的一些策略。当一个 slave 与 master 失去联系时，或者复制正在进行的时候，slave 应对请求的行为：
- yes：slave 仍然会应答客户端请求，但返回的数据可能是过时，或者数据可能是空的（在第一次同步的时候）。
- no ：在执行除了 `info` 和 `salveof` 之外的其他命令时，slave 都将返回一个 `SYNC with master in progress` 的错误。

### 4.1.4. 脑裂问题

当由于主库出现网络阻塞或者资源被抢占导致哨兵监听无法正常响应时，主库会被标记为客观下线并选举出新的主库。而若是主库只是短暂的不可用在哨兵选举期间恢复正常，原主库与新主库共存，就出现了脑裂问题。等到哨兵让原主库执行 `slave of` 后，才会只保留新主库。

在新主库完全切换完之前，客户端请求仍能正常写入主库。等到哨兵选举出新的主库而原主库这段期间接收到的请求未同步给从库，从而导致新主库无这些数据。造成数据不一致。

![](https://r2.129870.xyz/img/20220521220627.png)

这个问题是出在原主库发生假故障后仍然能接收请求上，可以尝试设置以下参数限制主库接收请求：

- `min-slaves-to-write` ：这个配置项设置了主库能进行数据同步的最少从库数量。
- `min-slaves-max-lag` ：这个配置项设置了主从库间进行数据复制时，从库给主库发送 ACK 消息的最大延迟（以秒为单位）。通常配置  $min\mbox{-}slaves\mbox{-}max\mbox{-}lag  \leq  down\mbox{-}after\mbox{-}milliseconds$ 

我们可以把 `min-slaves-to-write` 和 `min-slaves-max-lag` 这两个配置项搭配起来使用，分别给它们设置一定的阈值，假设为 N 和 T。这两个配置项组合后的要求是，**主库连接的从库中至少有 N 个从库，和主库进行数据复制时的 ACK 消息延迟不能超过 T 秒**，否则，主库就不会再接收客户端的请求了。

即使原主库是假故障，它在假故障期间也无法响应哨兵心跳，也不能和从库进行同步，自然也就无法和从库进行 ACK 确认了。这样一来，`min-slaves-to-write` 和 `min-slaves-max-lag` 的组合要求就无法得到满足，原主库就会被限制接收客户端请求，客户端也就不能在原主库中写入新数据了。等到新主库上线时，就只有新主库能接收和处理客户端请求，此时，新写的数据会被直接写到新主库中。而原主库会被哨兵降为从库，即使它的数据被清空了，也不会有新数据丢失。

脑裂产生问题的本质原因是，**Redis 主从集群内部没有通过共识算法，来维护多个节点数据的强一致性**。它不像 Zookeeper 那样，每次写请求必须大多数节点写成功后才认为成功。当脑裂发生时，Zookeeper 主节点被孤立，此时无法写入大多数节点，写请求会直接返回失败，因此它可以保证集群数据的一致性。Redis 是出于高可用的目的放弃一致性的，[[分布式大纲#2 2 分布式事务缺陷之 CAP 定理|CAP]] 三者放弃了 C（一致性）。

## 4.2. 哨兵机制

基于主从模式，我们可以将数据分散到多个节点。主节点可以接收读写请求，从节点接收读请求。但是当主从之间出问题时该如何处理呢？从节点挂了影响尚小，可要是主节点挂了呢？我们根据什么判断主节点挂了，主节点挂了以后将哪个从库推选为主节点，如何将新的主节点通知给其他从库呢？

### 4.2.1. 哨兵机制的概念

哨兵是一个处于特殊模式的 Redis 进程，它负责以下三个任务：

1. 监控：监控主库运行状态，并判断主库是否客观下线
    哨兵会周期性的给所有主从库发送 ping 命令以检测服务是否在线。如果检测失败，就会被标记下线。若主库被标记下线，启动自动选举主库流程。
    
    1. 对于从库，哨兵对从库的 ping 超时未回复就可以将其标记为下线。
    3. 对于主库，若某个哨兵 ping 超时则将其标记为主观下线。需要多个哨兵都认为下线了才可将其标记成客观下线，此时主库真的会下线，开始执行选主流程。
    
    “客观下线”的标准就是，当**有 N 个哨兵实例时，最好要有 $\frac{N}{2} + 1$ 个实例判断主库为“主观下线”，才能最终判定主库为“客观下线”**。这样一来，就可以减少误判的概率，也能避免误判带来的无谓的主从库切换。
    
2. 选主：在主库客观下线后，选取新主库
    主节点选择顺序如下：
    1. 筛选出不满足条件的节点
        如除了检查从库的当前在线状态，还要判断它之前的网络连接状态。若一个节点过去频繁超时掉线，这个节点被认为不可靠。例如配置 $down\mbox{-}after\mbox{-}milliseconds \times 10$ 控制超时时间与次数。
    2. 从剩余的从节点中挑选出一个新的主节点
        选主时会综合判断从节点的状态，哨兵将会从多个维度对从节点进行打分操作，选择一个分高的从节点作为新的主节点。
    
    打分操作如下：
    
    1. 第一轮会从优先级判断，若手动给从库设置了高优先级，则高优先级优先。
    2. 优先级相同对按照与旧主库的数据同步进度，同步进度快的优先。哨兵监视期间，从库会使用 `info` 命令将自身信息同步给哨兵，其中就包括 `offset` 信息。
    3. 进度相同的情况下按照从库 ID 号优先。
3. 通知：选出新主库后，通知从库和客户端
    **哨兵将新的主节点通知给其他从节点，让从节点与新的主节点开始数据同步**。
    
    哨兵会把新主库的地址写入自己实例的 `pubsub（switch-master）` 中。客户端需要订阅这个 `pubsub`，当这个 `pubsub` 有数据时，客户端就能感知到主库发生变更，同时可以拿到最新的主库地址，然后把写请求写到这个新主库即可，这种机制属于哨兵主动通知客户端。
    
    客户端需要访问主从库时，不能直接写死主从库的地址，需要从哨兵集群中获取最新的地址（`sentinel get-master-addr-by-name` 命令）。这样当实例异常时，哨兵切换后或者客户端断开重连，都可以从哨兵集群中拿到最新的实例地址。

### 4.2.2. 哨兵集群

我们可以通过哨兵来进行主从数据库的监控选举与通知，但是如果哨兵节点挂机了要怎么办呢？因此我们需要部署多个哨兵构建哨兵集群来保证哨兵的高可用。

![](https://r2.129870.xyz/img/20220518115624.png)
![](https://r2.129870.xyz/img/20220518115632.png)

哨兵集群的原理：

1. 哨兵通过 Redis 的发布订阅机制来获取 Redis 主库信息以及和其他哨兵建立联系。

	1. 哨兵首先与主库建立连接，监控主库信息。
	
	2. 除了自身以外，哨兵还会通过发布订阅机制从主库获取到其他哨兵节点构建哨兵集群。
	
	3. 所有哨兵通过主库的 `__sentinel__:hello` 频道来互相发现与通信。
	
2. 哨兵连接到主库后，就可以获取到主库信息和其他哨兵信息了。对于从库，哨兵会通过 `info` 命令从主库获取到所有的从库，从而再次和从库建立连接进行监听。

3. 除了从库外，哨兵还需要和客户端建立连接，以通知客户端主库切换信息，以及客户端关心的其他信息。如通过 `+switch-master` 命令得知主库地址切换，`+sdown` 实例进入主观下线状态等。

	![](https://r2.129870.xyz/img/20220518120328.png)

### 4.2.3. 哨兵选主

当主库被标记为客观下线后，就会开始进行主库的切换。那么由哪个哨兵来进行这个切换的过程呢？这就需要哨兵选举出一个 leader，由 leader 来进行切换操作：

1. 首先需要将主库标记为客观下线状态，这需要哨兵集群中大多数哨兵达成共识。
    任何一个哨兵只要自身判断主库“主观下线”后，就会给其他实例发送 `is-master-down-by-addr` 命令。接着，其他实例会根据自己和主库的连接情况，做出 Y 或 N 的响应，Y 相当于赞成票，N 相当于反对票。一个哨兵获得了仲裁所需的赞成票数 `quorum` 后，就可以标记主库为“客观下线”。
    
    这里就要求所有的哨兵将主库的 `down-after-milliseconds` 参数设置为一致，否则就会出现下线行为认为不一致的现象。
1. 主库被标记为下线状态后，哨兵集群需要选举出一个 leader 来进行切换操作。称为 leader 的如下：
    1. 拿到半数以上的赞成票
        **这意味着若有超过半数以上哨兵节点宕机，则选举操作无法进行。**所以我们要求哨兵节点数量至少为 3，否则若一台哨兵节点宕机，则无法进行选举。当然哨兵节点数量也不要过多，否则会造成节点选举耗时时间长。
    2. 拿到的票数同时还需要大于等于哨兵配置文件中的 quorum 值
1. 选举出 leader 之后，由 leader 完成主库的切换操作。
    哨兵 leader 的选举过程：
    1. 当哨兵节点判断主库下线后，会向其他哨兵节点发送投票通知让其他节点投票给自己。同时该哨兵也会给自己投一票。
    2. 若一个哨兵节点接收到投票通知后，自身还未投过票，则同意该通知，返回 Y。否则若已经给自己和其他节点投过票，返回 N。
    3. 投票完成后若有节点获得超过半数以上赞成票且票数大于 quorum 值，则其称为 leader，否则将会再次选举。选举周期为哨兵故障转移超时时间。

Redis 哨兵通过以下机制确保选举的**有穷性**，避免活锁：

1. **任期递增**：
    每次选举失败后任期号递增，旧任期的投票无效，避免历史投票干扰新选举。
2. **随机化超时**：
    随机化减少了多个哨兵同时发起选举的可能性，使某一候选者大概率能优先获得多数票。
3. **多数派约束**：
    在 N 个哨兵中，多数派为 `N/2 + 1`。即使多次选举失败，最终总会有某一轮次中单个候选者赢得多数票。

## 4.3. Redis 的持久化方式

### 4.3.1. AOF (Append-only-file)

#### 4.3.1.1. AOF 原理

原理：将所有命令以 Redis 命令请求协议存储，保存为 AOF 文件。命令保存时机由以下参数配置：
- `Always` ：同步写回。每个写命令执行完，立马同步地将日志写回磁盘。崩溃时会丢失一次事件循环的命令。
- `Everysec` ：每秒写回。每个写命令执行完，只是先把日志写到 AOF 文件的内存缓冲区，每隔一秒把缓冲区中的内容写入磁盘。崩溃时会丢失一秒内的命令。
- `No` ：操作系统控制的写回。每个写命令执行完，只是先把日志写到 AOF 文件的内存缓冲区，由操作系统决定何时将缓冲区内容写回磁盘。崩溃时会丢失不可控数据的命令。

优点：
- 数据安全，一次操作就可以备份一次
- 通过 append 模式，即使中途宕机也可以回复数据
- 有 rewrite 模式，当 aof 文件过大时可以进行命令合并

缺点：
- AOF 文件偏大，恢复慢
- 数据集大时，比 RDB 启动效率低

当 Redis 以 AOF 模式持久化时，**如果数据库某个过期键还没被删除，那么 AOF 文件会保留此过期键，当此过期键被删除后，Redis 会向 AOF 文件追加一条 DEL 命令来显式地删除该键值**。

#### 4.3.1.2. AOF 重写

![](https://r2.129870.xyz/img/20220517181251.png)

当操作命令积累时，AOF 文件会变大。**AOF 重写支持以当前数据库的所有未过期数据，对其生成每一条 set 命令，从而产生一个全新的 AOF 文件**。重写机制具有“多变一”功能，该 AOF 文件理论上会比原始文件小许多，这个操作称为 AOF 重写。

每次 AOF 重写时，Redis 会先执行一个内存拷贝，用于重写。然后，使用两个日志保证在重写过程中，新写入的数据不会丢失。而且 Redis 采用额外的线程 bgrewriteaof 进行数据重写，所以，这个过程并不会阻塞主线程。

但是 AOF 重写依然在某些情况下存在阻塞主线程的风险：
1. fork 这个瞬间一定是会阻塞主线程的。
    fork 采用操作系统提供的写时复制 (`Copy On Write`) 机制，就是为了避免一次性拷贝大量内存数据给子进程造成的长时间阻塞问题。但 fork 子进程需要拷贝进程必要的数据结构，其中有一项就是拷贝内存页表（虚拟内存和物理内存的映射索引表），这个拷贝过程会消耗大量 CPU 资源，拷贝完成之前整个进程是会阻塞的，阻塞时间取决于整个实例的内存大小，实例越大，内存页表越大，fork 阻塞时间越久。
    
    > [!info] 写时复制的实现
    > fork 会复制父进程的页表给子进程，并把所有当前正常状态的数据段、堆和栈空间的虚拟内存页，设置为不可写，然后把已经映射的物理页面的引用计数加 1。这一步只需要复制页表和修改页表项中的写权限位可以了，并不会真的为子进程的所有内存空间分配物理页面，修改映射，所以它的效率是非常高的。至此，fork 调用结束。
    > 
    > 不管是父进程还是子进程，它们接下来都有可能发生写操作，但我们知道在 fork 过程中，已经将所有原来可写的地方都变成不可写了，所以这时必然会发生写保护中断。
    > 
    > 接着，内核会调用写保护中断的处理函数，系统会首先**判断发生中断的虚拟地址所对应的物理地址的引用计数，如果大于 1，就说明现在存在多个进程共享这一块物理页面**，那么它就需要为发生中断的进程再分配一个物理页面，把老的页面内容拷贝进这个新的物理页，最后把发生中断的虚拟地址映射到新的物理页，这就完成了一次写时复制。
    
1. 拷贝内存页表完成后，父进程也可能会产生阻塞的风险。
    子进程与父进程指向相同的内存地址空间，也就是说此时虽然产生了子进程，但是并没有申请与父进程相同的内存大小。“写时复制”是在写发生时，才真正拷贝内存真正的数据，这个过程中存在以下问题：
    
    - 写入数据的问题
        fork 出的子进程指向与父进程相同的内存地址空间，此时子进程就可以执行 AOF 重写，把内存中的所有数据写入到 AOF 文件中。但是此时父进程依旧是会有流量写入的，如果**父进程操作的是一个已经存在的 key，那么这个时候父进程就会真正拷贝这个 key 对应的内存数据，申请新的内存空间**。这样逐渐地父子进程内存数据开始分离，父子进程逐渐拥有各自独立的内存空间。
    - bigkey 的问题
        因为内存分配是以页为单位进行分配的，默认 4k。如果父进程此时操作的是一个 bigkey，重新申请大块内存耗时会变长，可能会产阻塞风险。另外，如果操作系统开启了内存大页机制 (`Huge Page`，页面大小 2M)，那么父进程申请内存时阻塞的概率将会大大提高，所以在**Redis 机器上需要关闭 Huge Page 机制**。Redis 每次 fork 生成 RDB 或 AOF 重写完成后，都可以在 Redis log 中看到父进程重新申请了多大的内存空间。

AOF 重写的配置：
- `auto-aof-rewrite-min-size 64mb` 配置 AOF 文件运行时最大容量。
- `auto-aof-rewrite-percentage 100` 配置 AOF 文件运行时本次 AOF 文件与上一次 AOF 文件体积增量比。

AOF 重写触发时机：
- 在 aof 文件体量超过 `auto-aof-rewrite-min-size`，且比上次重写后的体量增加了 `auto-aof-rewrite-percentage` 时自动触发重写。
- 手动发送 `bgrewriteaof` 指令触发一次重写。

AOF 重写命令的生成：
- 过期的数据不再写入。
- 无效的命令不再写入：如先新增再删除的命令可以不保留。
- 多条命令可以合并成一个
    不过为了防止单条命令过大造成客户端缓冲区溢出，对于 list、set、hash、zset 类型的 key，并不一定只使用一条命令，而是以某个常量为界将命令拆分为多条。这个常量由 AOF_REWRITE_ITEMS_PER_CMD 定义，不可更改，[该值目前是 64](https://github.com/redis/redis/blob/unstable/src/server.h#L125)。

`Redis4.0` 以后，Redis 的 AOF 重写的时候就直接把 RDB 的内容写到 AOF 文件开头，将增量改动的以指令的方式 Append 到 AOF，这样做的好处是可以结合 RDB 和 AOF 的优点，快速加载同时避免丢失过多的数据。缺点是 AOF 里面的 RDB 部分就是压缩格式不再是 AOF 格式，可读性较差。

重写 AOF 日志过程中，如果主进程修改了已经存在 key-value，此时这个 key-value 数据在子进程的内存数据就跟主进程的内存数据不一致了，这时要怎么办呢？

为了解决这种数据不一致问题，Redis 设置了一个 **AOF 重写缓冲区**，这个缓冲区在创建 bgrewriteaof 子进程之后开始使用。在重写 AOF 期间，当 Redis 执行完一个写命令之后，它会**同时将这个写命令写入到 AOF 缓冲区和 AOF 重写缓冲区**。

也就是说，在 bgrewriteaof 子进程执行 AOF 重写期间，主进程需要执行以下三个工作:
1. 执行客户端发来的命令。
2. 将执行后的写命令追加到 AOF 缓冲区。
3. 将执行后的写命令追加到 AOF 重写缓冲区。

当子进程完成 AOF 重写工作（扫描数据库中所有数据，逐一把内存数据的键值对转换成一条命令，再将命令记录到重写日志）后，会向主进程发送一条信号，信号是进程间通讯的一种方式，且是异步的。

主进程收到该信号后，会调用一个信号处理函数，该函数主要做以下工作：
1. 将 AOF 重写缓冲区中的所有内容追加到新的 AOF 的文件中，使得新旧两个 AOF 文件所保存的数据库状态一致。
2. 新的 AOF 的文件进行改名，覆盖现有的 AOF 文件。

信号函数执行完后，主进程就可以继续像往常一样处理命令了。在整个 AOF 后台重写过程中，除了发生写时复制会对主进程造成阻塞，还有信号处理函数执行时也会对主进程造成阻塞，在其他时候，AOF 后台重写都不会阻塞主进程。

![image.png](https://r2.129870.xyz/img/202308061622220.png)

#### 4.3.1.3. AOF 与 WAL 的抉择

Redis 为什么考虑使用 AOF 而不是 WAL 呢？

很多数据库都是采用的 Write Ahead Log（WAL）写前日志，其特点就是先把修改的数据记录到日志中，再进行写数据的提交，可以方便通过日志进行数据恢复。但是 Redis 采用的却是 AOF（Append Only File）写后日志，特点就是先执行写命令，把数据写入内存中，再记录日志。

主要是出于以下几点原因：
- 系统设计方向不同，Redis 考虑可用性和简单性，因此写后日志更简单，而 WAL 考虑一致性，增强复杂性来保证数据的一致性。
- 如果先让系统执行命令，只有命令能执行成功，才会被记录到日志中。因此，Redis 使用写后日志这种形式，可以避免出现记录错误命令的情况。
- AOF 是在命令执行后才记录日志，所以不会阻塞当前的写操作。

### 4.3.2. RDB (Redis DataBase Back File)

![](https://static001.geekbang.org/resource/image/a2/58/a2e5a3571e200cb771ed8a1cd14d5558.jpg)

原理：定期生成所有数据的快照，依赖快照恢复。采用写时复制技术实现，不会阻塞 Redis 读写操作。

优点：
- 只有一个 `dump.rdb` 文件，方便持久化。
- 单个文件方便存储。
- 性能最大化，fork 子线程备份，主线程不阻塞，IO 最大化.
- 数据集大时，比 AOF 启动效率高。

缺点：
- 间隔时间太长，容易丢失数据。间隔事件太短，对磁盘和 CPU 的抢占率高，容易阻塞主线程。
- 在写请求高于读请求，且写请求分散在大量数据的情况下，RDB 进行的时候会发生以下风险：
    - 内存资源风险
        由于采用写时复制技术，大量写请求会导致主线程进行内存复制与分离，这时内存使用率会上升。当到达机器容量时，若未配置 swap 策略则会出现 OOM 现象，若配置了会导致内存热点数据与磁盘的交换，导致性能下降。
    - CPU 资源风险
        在 CPU 核心少时，主线程已占用了一个线程。fork 的子线程会再占用一个且消耗资源较高。同时还存在刷盘，异步关闭文件符这些操作的线程，以及系统内其他的线程。这些线程的争用导致性能下降。

Redis 提供了两个命令来生成 RDB 文件，分别是 `save` 和 `bgsave`，他们的区别就在于是否在主线程里执行：

- 执行了 save 命令，就会在主线程生成 RDB 文件，由于和执行操作命令在同一个线程，所以如果写入 RDB 文件的时间太长，**会阻塞主线程**。当生成文件时，对于过期的键不会被保存到新的 RDB 文件中。
- 执行了 bgsave 命令，会创建一个子进程来生成 RDB 文件，这样可以**避免主线程的阻塞**。

RDB 文件的加载工作是在服务器启动时自动执行的，Redis 并没有提供专门用于加载 RDB 文件的命令。**当加载 RBD 文件时，如果是主服务器运行模式，则会加载 RDB 文件中所有的键，不会检查过期时间；反之若是从服务器运行模式，则只会加载未过期的键**。

Redis 还可以通过配置文件的选项来实现每隔一段时间自动执行一次 bgsave 命令，默认会提供以下配置：

```txt
save 900 1
save 300 10
save 60 10000
```

只要满足上面条件的任意一个，就会执行 bgsave，它们的意思分别是：
- 900 秒之内，对数据库进行了至少 1 次修改。
- 300 秒之内，对数据库进行了至少 10 次修改。
- 60 秒之内，对数据库进行了至少 10000 次修改。

### 4.3.3. AOF 与 RDB 混合模式

`Redis4.0` 引入了混合模式，支持 RDB 与 AOF 同时使用。新的 AOF 文件前半段是 RDB，后半段是增量的 AOF。采用混合模式既可以利用 RDB 模式备份文件小和全的特性，又可以利用 AOF 一致性高的特性。

![](https://r2.129870.xyz/img/20220517194638.png)

如果想要开启混合持久化功能，可以在 Redis 配置文件将下面这个配置项设置成 yes：

```txt
aof-use-rdb-preamble yes
```

当开启了混合持久化时，在 AOF 重写日志时，`fork` 出来的重写子进程会先将与主线程共享的内存数据以 RDB 方式写入到 AOF 文件，然后主线程处理的操作命令会被记录在重写缓冲区里，重写缓冲区里的增量命令会以 AOF 方式写入到 AOF 文件，写入完成后通知主进程将新的含有 RDB 格式和 AOF 格式的 AOF 文件替换旧的的 AOF 文件。

# 5. 高扩展

## 5.1. Redis Cluster 集群

基于主从，我们可以将节点划分为读写节点以此承担压力，并且利用主从维持了节点之间数据的一致性。之后可以利用哨兵，对主从进行监控，从而可以使节点发生故障时可以切换。但是无论是主从还是哨兵，这里所针对的情况都是数据在一台节点完全存储的，如果单台节点容量不够了呢？

对于容量负载，我们有两种方式，纵向扩展与横向扩展。纵向扩展即升配机器，横向扩展即增加机器。这二者对比如下：

- 纵向扩展

	- 实施起来简单、直接。
	
	- 数据量增加，需要的内存也会增加，主线程 fork 子进程时就可能会阻塞。
	
	- 会受到硬件和成本的限制
	
	- 数据存在哪儿，客户端访问哪儿，对外访问方式简单。
	
- 横向扩展

	- 增减机器比升配更容易做到，可以利用多台一般的机器构建一个高可靠的集群。
	
	- 单台机器容量小，fork 线程压力小。
	
	- 需要管理多台机器的数据，对于请求分发与数据存储需要做合理划分。
	
	- 客户端可能需要和多台机器打交道，多机器集群管理问题。

### 5.1.1. Redis Cluster 集群的构建

#### 5.1.1.1. 初始哈希槽的划分

![](https://r2.129870.xyz/img/20220518160601.png)

在 Redis Cluster 方案中，一个切片集群共有 [[数据密集型系统设计2：复制与分区#2.3.1.1. 固定数量分区| 16384 个哈希槽]]，这些哈希槽类似于数据分区，每个键值对都会根据它的 key，被映射到一个哈希槽中。映射的规则如下：

1. 首先根据键值对的 key，按照 `CRC16` 算法计算一个 16 bit 的值。
2. 用这个 16 bit 值对 16384 取模，得到 0~16383 范围内的模数，每个模数代表一个相应编号的哈希槽。

将数据划分到哈希槽以后，接下来就是需要将槽分配给 Redis 实例了。有以下几种方式：
- 部署 Redis Cluster 方案时，可以使用 `cluster create` 命令创建集群，此时，Redis 会自动把这些槽平均分布在集群实例上。
- 使用 `cluster meet ` 命令手动建立实例间的连接，形成集群，再使用 `cluster addslots` 命令，指定每个实例上的哈希槽个数，按照机器配置或者优先级来手动分配。在手动分配哈希槽时，需要把 16384 个槽都分配完，否则 Redis 集群无法正常工作。

当哈希槽分配完毕后，每个实例会负责一部分哈希槽。这些集群节点之间还会互相通知自己所负责的哈希槽，这样集群中所有节点都会知道槽的映射关系。

#### 5.1.1.2. 哈希槽的重建

随着数据的的分布不均或者 Redis 实例的增减，出于负载均衡策略就需要将哈希槽需要重新划分，将原属于一个实例的哈希槽分配给其他实例。

数据迁移过程是同步的，迁移一个 key 会同时阻塞源节点和目标节点。

#### 5.1.1.3. 利用哈希槽的原因

1. Redis Cluster 集群采取去中心化的思想
    客户端与服务端直连，如果这个访问的 key 不在这个节点上，需要**服务端有纠错的能力。即节点保存了完整的映射关系，发现错误请求进行 `MOVED` 响应处理**。对于其他集群的实现，如 Codis 的中心化模式，有 proxy 层进行请求分发与节点管理，这要求 proxy 层也要做到高可用。
2. 增加一层哈希槽，可以把**数据和节点解耦**
    key 通过 Hash 计算，只需要关心映射到了哪个哈希槽，然后再通过哈希槽和节点的映射表找到节点。相当于消耗了很少的 CPU 资源，不但让数据分布更均匀，还可以让这个映射表变得很小，利于客户端和服务端保存，节点之间交换信息时也变得轻量。
3. 节点运维简单
    当集群在扩容、缩容、数据均衡时，节点之间的操作例如数据迁移，都以哈希槽为基本单位进行操作，简化了节点扩容、缩容的难度，便于集群的维护和管理。

Redis 哈希槽的概念和 `一致性 hash` 很像，但它们的侧重点不同：
1. `一致性 hash` 通常用来解决当集群节点数变化时，大量缓存无法命中的情况。因为 hash 计算方式为 $hashcode\div节点数$，节点数的变化使得大部分计算结果都发生了变化。一致性 hash 可以保证同一个 key 在节点数变化的时候，大部分 $hashcode\div节点数$ 仍然能计算出相同结果。
2. 哈希槽的设计也可以保证这一点：只要把那个节点对应的槽的数据做迁移即可，其余数据不用移动。甚至可以做细粒度的控制，比如机器性能不一样，有的机器可以分配多几个槽。
3. 使用哈希槽避免了从 hash 环上寻找实例节点，中间可能还有从虚拟节点到实际节点的过渡。
4. 使用哈希槽灵活性更强，如将某个实例上的热点槽迁移到其他机器上去，其他槽数据保持不动。

### 5.1.2. Redis Cluster 集群的使用

![](https://r2.129870.xyz/img/20220518162735.png)

集群划分好以后就涉及到客户端的使用了，那么客户端如何定位到 Redis 实例呢？这其实是一个[[数据密集系统设计/数据密集型系统设计2：复制与分区#2 4 请求路由|请求路由]]的问题，客户端通过与 Redis 集群建立连接后，可以知道 Redis 集群中所有哈希槽的映射关系，在操作数据时，按以下步骤进行：

1. 根据操作 key 计算对应的哈希槽。
2. 根据映射信息请求哈希槽所在的实例。对请求到的实例可能出现以下几种情况：
    - 当前实例正常，直接执行命令。
    - 当前 key 所属哈希槽已经不归属于此实例，返回 `MOVED` 命令响应结果，结果中就包含新实例的访问地址。客户端更新哈希槽映射关系并请求新的实例信息。
    - 当前哈希槽正在迁移，返回 `ASK` 命令。客户端询问新实例该 key 的情况决定下一步操作。`ASK` 命令表示两层含义：
        1. 第一，表明 `Slot` 数据还在迁移中。
        2. 第二，`ASK` 命令把客户端所请求数据的最新实例地址返回给客户端，此时，客户端需要给实例 3 发送 `ASKING` 命令，然后再发送操作命令。
    
        和 `MOVED` 命令不同，`ASK` 命令并不会更新客户端缓存的哈希槽分配信息。这也就是说，`ASK` 命令的作用只是**让客户端能给新实例发送一次请求**，而不像 `MOVED` 命令那样，会更改本地缓存，让后续所有命令都发往新实例。 

### 5.1.3. 集群的通信

Redis Cluster 在运行时，每个实例上都会保存 `Slot` 和实例的对应关系（也就是 `Slot 映射表`），以及自身的状态信息。为了让集群中的每个实例都知道其它所有实例的状态信息，实例之间会按照一定的规则进行通信。这个规则就是 [[数据密集型系统设计2：复制与分区#2.4. 请求路由|Gossip 协议]]。

`Gossip 协议` 的工作原理可以概括成两点：

1. 每个实例之间会按照一定的频率，从集群中随机挑选一些实例，把 PING 消息发送给挑选出来的实例，用来检测这些实例是否在线，并交换彼此的状态信息。PING 消息中封装了发送消息的实例自身的状态信息、部分其它实例的状态信息，以及 `Slot 映射表`。

2. 一个实例在接收到 PING 消息后，会给发送 PING 消息的实例，发送一个 PONG 消息。PONG 消息包含的内容和 PING 消息一样。

![](https://r2.129870.xyz/img/20220521235556.png)

`Gossip 协议` 可以保证在一段时间后，集群中的每一个实例都能获得其它所有实例的状态信息。这样一来，即使有新节点加入、节点故障、Slot 变更等事件发生，实例间也可以通过 PING、PONG 消息的传递，完成集群状态在每个实例上的同步。

#### 5.1.3.1. 集群通信的频率

Redis Cluster 的实例启动后，默认会每秒从本地的实例列表中随机选出 5 个实例，再从这 5 个实例中找出一个最久没有通信的实例，把 PING 消息发送给该实例。这是实例周期性发送 PING 消息的基本做法。

但是，这里有一个问题：实例选出来的这个最久没有通信的实例，毕竟是从随机选出的 5 个实例中挑选的，这并不能保证这个实例就一定是整个集群中最久没有通信的实例。所以，这有可能会出现，有些实例一直没有被发送 PING 消息，导致它们维护的集群状态已经过期了。为了避免这种情况，Redis Cluster 的实例会按照每 100ms 一次的频率，扫描本地的实例列表，**如果发现有实例最近一次接收 PONG 消息的时间，已经大于配置项 `cluster-node-timeout` （故障的心跳超时时间）的一半了，就会立刻给该实例发送 PING 消息**，更新这个实例上的集群状态信息。

## 5.2. Codis 集群

#### 5.2.1. Codis 的架构

![](https://r2.129870.xyz/img/20220521224427.png)
![](https://r2.129870.xyz/img/20220521224738.png)
![](https://r2.129870.xyz/img/20220521224718.png)

Codis 集群中包含了 4 类关键组件：

- codis server
    这是进行了二次开发的 Redis 实例，其中增加了额外的数据结构，支持数据迁移操作，主要负责处理具体的数据读写请求。

    Codis 集群一共有 1024 个 Slot，编号依次是 0 到 1023。可以把这些 Slot 手动分配给 codis server，每个 server 上包含一部分 Slot。也可以让 codis dashboard 进行自动分配，例如，dashboard 把 1024 个 Slot 在所有 server 上均分。
    
    当客户端要读写数据时，会使用 CRC32 算法计算数据 key 的哈希值，并把这个哈希值对 1024 取模。而取模后的值，则对应 Slot 的编号。此时，根据第一步分配的 Slot 和 server 对应关系，就可以知道数据保存在哪个 server 上了。
- codis proxy
    接收客户端请求，实现[[数据密集系统设计/数据密集型系统设计2：复制与分区#2 4 请求路由|请求路由]]把请求转发给 codis server。
- Zookeeper 集群
    保存集群元数据，例如数据位置信息和 codis proxy 信息。
    
    - Slot 和 codis server 的映射关系称为数据路由表（简称路由表）。我们在 codis dashboard 上分配好路由表后，dashboard 会把路由表发送给 codis proxy，同时，dashboard 也会把路由表保存在 Zookeeper 中。
    - codis-proxy 会把路由表缓存在本地，当它接收到客户端请求后，直接查询本地的路由表，就可以完成正确的请求转发了。
- codis dashboard 和 codis fe
    共同组成了集群管理工具，其中，codis dashboard 负责执行集群管理工作，包括增删 codis server、codis proxy 和进行数据迁移；而 codis fe 负责提供 dashboard 的 Web 操作界面，便于我们直接在 Web 界面上进行集群管理。

#### 5.2.2. Coid 的数据迁移

![](https://r2.129870.xyz/img/20220521224840.png)

增加 codis server，这个过程主要涉及到两步操作：

1. 启动新的 codis server，将它加入集群。

2. 把部分数据迁移到新的 server。

Codis 集群按照 Slot 的粒度进行数据迁移：

1. 在源 server 上，Codis 从要迁移的 Slot 中随机选择一个数据，发送给目的 server。

2. 目的 server 确认收到数据后，会给源 server 返回确认消息。这时，源 server 会在本地将刚才迁移的数据删除。

	当源 server 把数据发送给目的 server 后，就可以处理其他请求操作了，不用等到目的 server 的命令执行完。而目的 server 会在收到数据并反序列化保存到本地后，给源 server 发送一个 ACK 消息，表明迁移完成。此时，源 server 在本地把刚才迁移的数据删除。

第一步和第二步就是单个数据的迁移过程。Codis 会不断重复这个迁移过程，直到要迁移的 Slot 中的数据全部迁移完成。

这个过程中，迁移的数据会被设置为只读，所以，源 server 上的数据不会被修改，自然也就不会出现“和目的 server 上的数据不一致”的问题了。

对于 bigkey，异步迁移采用了拆分指令的方式进行迁移。具体来说就是，对 bigkey 中每个元素，用一条指令进行迁移，而不是把整个 bigkey 进行序列化后再整体传输。

#### 5.2.3. Codis 的高可用

codis server 其实就是 Redis 实例，只不过增加了和集群操作相关的命令。Redis 的主从复制机制和哨兵机制在 codis server 上都是可以使用的，所以，Codis 就使用主从集群来保证 codis server 的可靠性。简单来说就是，Codis 给每个 server 配置从库，并使用哨兵机制进行监控，当发生故障时，主从库可以进行切换，从而保证了 server 的可靠性。

Codis 集群中，客户端是和 codis proxy 直接连接的，所以，当客户端增加时，一个 proxy 无法支撑大量的请求操作，此时，我们就需要增加 proxy。增加 proxy 比较容易，我们直接启动 proxy，再通过 codis dashboard 把 proxy 加入集群就行。

此时，codis proxy 的访问连接信息都会保存在 Zookeeper 上。所以，当新增了 proxy 后，Zookeeper 上会有最新的访问列表，客户端也就可以从 Zookeeper 上读取 proxy 访问列表，把请求发送给新增的 proxy。这样一来，客户端的访问压力就可以在多个 proxy 上分担处理了。

proxy 上的信息源头都是来自 Zookeeper（例如路由表）。而 Zookeeper 集群使用多个实例来保存数据，只要有超过半数的 Zookeeper 实例可以正常工作， Zookeeper 集群就可以提供服务，也可以保证这些数据的可靠性。所以，codis proxy 使用 Zookeeper 集群保存路由表，可以充分利用 Zookeeper 的高可靠性保证来确保 codis proxy 的可靠性，不用再做额外的工作了。当 codis proxy 发生故障后，直接重启 proxy 就行。重启后的 proxy，可以通过 codis dashboard 从 Zookeeper 集群上获取路由表，然后，就可以接收客户端请求进行转发了。这样的设计，也降低了 Codis 集群本身的开发复杂度。

![](https://r2.129870.xyz/img/20220521225253.png)
![](https://r2.129870.xyz/img/20220521225301.png)

#### 5.2.4. Redis Cluster 与 Codis 的对比

![](https://r2.129870.xyz/img/20220521225421.png)

- 从稳定性和成熟度来看，Codis 应用得比较早，在业界已经有了成熟的生产部署。虽然 Codis 引入了 proxy 和 Zookeeper，增加了集群复杂度，但是，proxy 的无状态设计和 Zookeeper 自身的稳定性，也给 Codis 的稳定使用提供了保证。而 Redis Cluster 的推出时间晚于 Codis，相对来说，成熟度要弱于 Codis，如果你想选择一个成熟稳定的方案，Codis 更加合适些。
- 从业务应用客户端兼容性来看，连接单实例的客户端可以直接连接 codis proxy，而原本连接单实例的客户端要想连接 Redis Cluster 的话，就需要开发新功能。所以，如果你的业务应用中大量使用了单实例的客户端，而现在想应用切片集群的话，建议你选择 Codis，这样可以避免修改业务应用中的客户端。
- 从使用 Redis 新命令和新特性来看，Codis server 是基于开源的 Redis 3.2.8 开发的，所以，Codis 并不支持 Redis 后续的开源版本中的新增命令和数据类型。另外，Codis 并没有实现开源 Redis 版本的所有命令，比如 `BITOP`、`BLPOP`、`BRPOP`，以及和与事务相关的 `MUTLI`、`EXEC` 等命令。
- 从数据迁移性能维度来看，Codis 能支持异步迁移，异步迁移对集群处理正常请求的性能影响要比使用同步迁移的小。所以，如果你在应用集群时，数据迁移比较频繁的话，Codis 是个更合适的选择。
- `mget` 这种批量命令，codis 的 proxy 会多线程执行多个查询操作，而 redis-cluster 对这种 mget 操作是单线程顺序执行，性能上不如 codis。

## 5.3. 集群优化策略

- Cluster
	- 所有节点互相连接
	- 集群消息通过集群总线通信
	- 节点与节点通过二进制协议通信
	- 客户端与集群节点正常文本协议通信
	- 集群节点不代理查询
	- 数据按 Slot 存储在多个 Redis 实例上。Redis 集群内置 16384 个哈希槽，将 key 按 CRC16 算法计算后对 16484 取余。
	- 集群节点宕机时自动故障转移
	- 可以平滑扩/缩容

- 集群优化策略
	- Master 不做持久化工作
	- 数据交给 Slave 开启持久化备份
	- Master 和 Slave 保持在同一个局域网
	- 避免在压力大的主库上增加从库
	- 主从复制避免图结构，使用单项链表保持节点切换简单

# 6. 运行原理

## 6.1. 文件事件模型

Redis 服务器端只需要单线程可以达到非常高的处理能力，每秒可以达到数万 QPS 的高处理能力。如此高性能的程序其实就是对 Linux 提供的[[计算机网络#2. IO 多路复用|多路复用机制 epoll]] 的一个较为完美的运用。

在 Redis 源码中，核心逻辑其实就是两个，一个是 initServer 启动服务，另外一个就是 aeMain 事件循环：

```c
//file: src/server.c
int main(int argc, char **argv) {
    ......
    // 启动初始化
    initServer();
    // 运行事件处理循环，一直到服务器关闭为止
    aeMain(server.el);
}
```

在 initServer 这个函数内，Redis 做了三件重要的事情：
- 创建一个 epoll 对象
- 对配置的监听端口进行 listen
- 把 listen socket 让 epoll 给管理起来

在 aeMain 函数中，是一个无休止的循环，它是 Redis 中最重要的部分：
- 通过 epoll_wait 发现 listen socket 以及其它连接上的可读、可写事件
    当套接字变得可读时 ( 客户端对套接字执行 write 操作，或者执行 close 操作 )，或者有新的可应答 ( acceptable )套接字出现时 (客户端对服务器的监听套接字执行 connect 操作)，套接字产生 AE_READABLE 事件。

    当套接字变得可写时 ( 客户端对套接字执行 read 操作 )，套接字产生 AE_WRITABLE 事件。
    
    IO 多路复用程序允许服务器同时监听套接字的 AE_READABLE 事件和 AE_WRITABLE 事件，如果一个套接字同时产生了这两种事件，那么文件事件分派器会优先处理 AE_READABLE 事件，等到 AE_READABLE 事件处理完之后，才处理 AE_WRITABLE 事件。
- 若发现 listen socket 上有新连接到达，则接收新连接，并追加到 epoll 中进行管理
- 若发现其它 socket 上有命令请求到达，则读取和处理命令，把命令结果写到缓存中，加入写任务队列
    事件分发器还处理了一个不明显的逻辑，那就是如果 beforesleep 在将结果写回给客户端的时候，如果由于内核 socket 发送缓存区过小而导致不能一次发送完毕的时候，也会注册一个写事件处理器。等到 epoll_wait 发现对应的 socket 可写的时候，再执行 write 写处理。
- 每一次进入 epoll_wait 前都调用 beforesleep 来将写任务队列中的数据实际进行发送

其主要流程总结如下：

![image.png](https://r2.129870.xyz/img/202308102330312.png)

处理器结构：
- 多个 Socket
- IO 多路复用程序
- 文件事件分派器
- 事件处理器
     - 为了对连接服务器的各个客户端进行应答，服务器要为监听套接字关联连接应答处理器
     - 为了接收客户端传来的命令请求，服务器要为客户端套接字关联命令请求处理器
     - 为了向客户端返回命令的执行结果，服务器要为客户端套接字关联命令回复处理器
     - 当主服务器和从服务器进行复制操作时，主从服务器都需要关联特别为复制功能编写的复制处理器
     
     在这些事件处理器里面，服务器最常用的要数与客户端进行通信的连接应答处理器、命令请求处理器和命令回复处理器。

处理器流程：
1. 客户端 Socket 与 Redis 的 Server Socket 请求建立连接
2. Server Socket 产生 `AE_READABLE` 事件，事件压入队列中
3. 文件事件分派器从队列中获取事件，交给连接处理器
4. 连接处理器会对容户端的连接请求进行应答，然后创建客户端套接字，以及客户端状态，并将客户端套接字的 `AE_READABLE` 事件与命令请求处理器进行关联，使得容户端可以向主服务器发送命令请求。
5. 客户端发起操作命令，产生 `AE_READABLE` 事件压入队列。事件分派器将事件交给命令处理器。
6. 命令处理器处理事件，处理器读取客户端的命令内容，然后传给相关程序去执行。执行命令将产生相应的命令回复，为了将这些命令回复传送回客户端，服务器会将端套接字的 `AE_WRITABLE` 事件与命令回复处理器进行关联。
7. 客户端准备好接收结果时，产生 `AE_WRITABLE` 事件压入队列，命令回复器回复结果。
8. 最后操作完成，解除命令回复器与 `AE_WRITABLE` 事件的关联。

![image.png](https://r2.129870.xyz/img/202308102349576.png)

## 6.2. Redis 的内存淘汰策略

每当我们对一个 key 设置了过期时间时，Redis 会把该 key 带上过期时间存储到一个**过期字典**（expires dict）中，也就是说「过期字典」保存了数据库中所有 key 的过期时间：

![image.png](https://r2.129870.xyz/img/202308052348762.png)

字典实际上是哈希表，哈希表的最大好处就是让我们可以用 O (1) 的时间复杂度来快速查找。当我们查询一个 key 时，Redis 首先检查该 key 是否存在于过期字典中：

- 如果不在，则正常读取键值。
- 如果存在，则会获取该 key 的过期时间，然后与当前系统时间进行比对，如果比系统时间大，那就没有过期，否则判定该 key 已过期。

### 6.2.1. Redis 提供的淘汰策略

- `volatile-lru` ：从已设置过期时间的数据集中挑选最近最少使用的数据。这是 3.0 版本以前默认的策略。
- `volatile-lfu` ：从已设置过期时间的数据集中挑选最不经常使用的数据。
- `volatile-ttl` ：从已设置过期时间的数据集中挑选即将过期的数据。
- `volatile-random` ：从已设置过期时间的数据集中任意挑选数据。
- `allkeys-lru` ：从所有键中挑选最近最少使用的数据。
- `allkeys-lfu` ：从所有键中挑选最不经常使用的数据。
- `allkeys-random` ：从所有键中中任意挑选数据。
- `no-enviction` ：禁止淘汰数据。新写入操作会报错。这是 3.00 版本后默认的策略。

基于 volatile 策略时若没有键设置了超时时间，那么表现效果和 allkeys 效果一致。

### 6.2.2. Redis-LRU 算法的实现

LRU 算法在实际实现时，需要用链表管理所有的缓存数据，这会带来额外的空间开销。而且当有数据被访问时，需要在链表上把该数据移动到前面，如果有大量数据被访问，就会带来很多链表移动操作，会很耗时，进而会降低 Redis 缓存性能。

所以，在 Redis 中 LRU 算法被做了简化，以减轻数据淘汰对缓存性能的影响。具体来说，Redis 默认会记录每个数据的最近一次访问的时间戳（由键值对数据结构 RedisObject 中的 `lru 字段` 记录）。

然后，Redis 在决定淘汰的数据时，第一次会随机选出 N 个数据，把它们作为一个候选集合。接下来，Redis 会比较这 N 个数据的 `lru 字段`，把 `lru 字段` 值最小的数据从缓存中淘汰出去。

可以通过 `maxmemory-samples` 设置淘汰的候选数据集个数。当需要再次淘汰数据时，Redis 需要挑选数据进入第一次淘汰时创建的候选集合。这儿的挑选标准是：能进入候选集合的数据的 `lru 字段` 值必须小于候选集合中最小的 `lru 字段` 值。当有新数据进入候选数据集后，如果候选数据集中的数据个数达到了 `maxmemory-samples`，Redis 就把候选数据集中 `lru 字段` 值最小的数据淘汰出去。

### 6.2.3. Redis-LFU 算法的实现

Redis 在实现 LRU 策略时使用了两个近似方法：

- Redis 是用 RedisObject 结构来保存数据的，RedisObject 结构中设置了一个 `lru 字段`，用来记录数据的访问时间戳。
- Redis 并没有为所有的数据维护一个全局的链表，而是通过**随机采样方式，选取一定数量（例如 10 个）的数据放入候选集合**，后续在候选集合中根据 ` lru 字段` 值的大小进行筛选。

在此基础上，Redis 在实现 LFU 策略的时候，只是把原来 24bit 大小的 **`lru 字段` 又进一步拆分成了两部分**：
- `ldt` 值：`lru 字段` 的前 16bit，表示数据的访问时间戳，这意味着精确度只能到分钟级别。
- `counter` 值：`lru 字段` 的后 8bit，表示数据的访问次数。

总结一下：当 LFU 策略筛选数据时，Redis 会在候选集合中，根据数据 lru 字段的后 8bit 选择访问次数最少的数据进行淘汰。当访问次数相同时，再根据 lru 字段的前 16bit 值大小，选择访问时间最久远的数据进行淘汰。

对于访问次数，Redis 并未采用线性递增的方式。counter 字段 8 位最多表示 255，采用线性递增方式很快就用完了。

LFU 策略实现的计数规则是：每当数据被访问一次时：

1. 首先，用计数器当前的值乘以配置项 `lfu_log_factor` 再加 1，再取其倒数，得到一个 p 值，即 $\frac{1}{baseval \times server.lfu\_log\_factor + 1}$。
2. 然后，把这个 p 值和一个取值范围在（0，1）间的随机数 r 值比大小，只有 p 值大于 r 值时，计数器才加 1。
3. 当增长次数越大时，递增概率越低。
4. `lfu_log_factor` 设置越大，递增概率越低。

```c

double r = (double)rand()/RAND_MAX;
...
double p = 1.0/(baseval*server.lfu_log_factor+1);
if (r < p) counter++;   

```

![](https://r2.129870.xyz/img/20220520232722.png)

使用了非线性递增的计数器方法，即使缓存数据的访问次数成千上万，LFU 策略也可以有效地区分不同的访问次数，从而进行合理的数据筛选。

Redis 在实现 LFU 策略时，还设计了一个 `counter` 值的衰减机制：

1. LFU 策略使用衰减因子配置项 `lfu_decay_time` 来控制访问次数的衰减。LFU 策略会计算当前时间和数据最近一次访问时间的差值，并把这个差值换算成以分钟为单位。
2. LFU 策略再把这个差值除以 `lfu_decay_time` 值，所得的结果就是数据 counter 要衰减的值。
3. 因为采用 16 位记录访问时间，这个时间精度只能到分钟级别。所以存在同一分钟内 A 比 B 访问次数多，但是优先因为缓存因子而被淘汰的情况。


### 6.2.4. 淘汰策略选择的方式

- 先根据是否有始终会被频繁访问的数据（例如置顶消息），来选择淘汰数据的候选集，也就是决定是针对所有数据进行淘汰，还是针对设置了过期时间的数据进行淘汰。
- 候选数据集范围选定后，建议优先使用 LRU 算法，也就是，`allkeys-lru` 或 `volatile-lru` 策略。

## 6.3. Redis 的过期删除策略

Redis 提供了以下数据的过期删除策略：

- 定时删除策略：设定定时器，时间到了就删。此种方式资源消耗大。
- 定期删除策略：每隔一定时间扫描删除。不是扫描所有 key，而是随机抽一部分。
- 惰性删除：操作 key 时如果发现过期了就删除。惰性删除策略对 CPU 时间最友好，但是对内存不友好。

在**主从模式下，从库不会自动删除过期的数据**，原因考虑有以下几点：

- 主从同步存在延迟，若从库将数据给删了，那么主库延迟到达的续期命令就不会生效了。
- 如果发生了时钟跳跃，主从之间就会出现删除时机不一致的现象。因此将删除操作放在主库同步给从库是合理的。
- 若从库 `slave-read-only` 设置为 no，在 `Redis4.0` 以前就算在从库给 key 设置了过期时间，那么从库也不会在过期后删除该数据。在 4.0 以后从库才会删除那些直接在自己库上设置生效时间的数据。

## 6.4. Redis 的事务

Redis 事务的特性：
- 事务失败时不支持回滚。
- 一个事务中出现运行错误，其余的命令会继续执行。
- **后执行的命令无法依赖先执行命令的结果**。
- 事务中的每条命令都会与 Redis 服务器进行网络交互。

Redis 事务的使用：
- `MULTI` 开启一个事务：命令入队时就报错，会放弃事务执行，保证原子性。
- `EXEC` 执行一个事务：命令入队时没报错，实际执行时报错，不保证原子性。
- `DISCARD` 取消一个事务：用来主动放弃事务执行，把暂存的命令队列清空，起不到回滚的效果。
- `WATCH` 提供 `Check And Set` 的能力，可以监控一个或多个键。一旦某个键被修改/删除，之后的事务就不会执行。
- 可以使用 `pipline` 批量发送命令来减少事务的冲突，只能尽量减少做不到避免。要想完全避免需要使用 ` lua` 脚本执行。

## 6.5. Redis 分区

方案：

- 代理分区：将请求发送给代理，让代理决定对哪个节点读写。如 Twemporxy
- 客户端分区：由客户端根据一定规则将数据分散到不同节点上
- 查询路由：客户端随机请求一个 Redis 实例，Redis 转发给正确的 Redis 节点。

缺点：

- 涉及多 key 的操作不能很好的支持，如无法直接对分布到不同节点的 key 做交并集计算
- 同时操作多个 key，无法使用事务
- 分区的粒度是 key，无法使用一个非常长的排序 key 存储一个数据集
- 数据处理会变得复杂，如备份时需要收集多个节点的数据
- 需要处理动态缩扩容的问题

## 6.6. Redis 的缓冲区

### 6.6.1. 客户端输入输出缓冲区

为了避免客户端和服务器端的请求发送和处理速度不匹配，服务器端给每个连接的客户端都设置了一个输入缓冲区和输出缓冲区。

输入缓冲区会先把客户端发送过来的命令暂存起来，Redis 主线程再从输入缓冲区中读取命令，进行处理。当 Redis 主线程处理完数据后，会把结果写入到输出缓冲区，再通过输出缓冲区返回给客户端，如下图所示：

![](https://r2.129870.xyz/img/20220519234030.png)

### 6.6.2. 输入输出缓冲区溢出

#### 6.6.2.1. 输入缓冲区溢出

- 写入了 bigkey，比如一下子写入了多个百万级别的集合类型数据。

- 服务器端处理请求的速度过慢，例如，Redis 主线程出现了间歇性阻塞，无法及时处理正常发送的请求，导致客户端发送的请求在缓冲区越积越多。

可以使用 `client list` 命令查看客户端情况，其中有以下几个信息：

- `cmd`，表示客户端最新执行的命令。

- `qbuf`，表示输入缓冲区已经使用的大小。

- `qbuf-free`，表示输入缓冲区尚未使用的大小。

当缓冲区被填满无法接收数据的时候，此时，客户端再写入大量命令的话，就会引起客户端输入缓冲区溢出，Redis 的处理办法就是把客户端连接关闭，结果就是业务程序无法进行数据存取了。

对于客户端的缓冲区，是不支持配置的，为 1GB 的大小。因此我们需要注意客户端写入的速度，防止缓冲区溢出。

#### 6.6.2.2. 输出缓冲区溢出

Redis 为每个客户端设置的输出缓冲区也包括两部分：一部分是一个大小为 16KB 的固定缓冲空间，用来暂存 OK 响应和出错信息；另一部分，是一个可以动态增加的缓冲空间，用来暂存大小可变的响应结果。

那什么情况下会发生输出缓冲区溢出呢？ 可能有以下几种：

- 服务器端返回 bigkey 的大量结果
- 执行了 `MONITOR` 命令
    `MONITOR` 命令是用来监测 Redis 执行的。执行这个命令之后，就会持续输出监测到的各个命令操作。
- 缓冲区大小设置得不合理：我们可以通过 `client-output-buffer-limit` 配置项，来设置缓冲区的大小。具体设置的内容包括两方面：
    - 设置缓冲区大小的上限阈值。
    - 设置输出缓冲区持续写入数据的数量上限阈值，和持续写入数据的时间的上限阈值。

    对于不同类型的客户端，也可以有不同的配置。如普通使用的客户端，监听频道订阅的客户端，从库客户端。
    
    - 普通客户端来说
        它每发送完一个请求，会等到请求结果返回后，再发送下一个请求，这种发送方式称为阻塞式发送。在这种情况下，如果不是读取体量特别大的 bigkey，服务器端的输出缓冲区一般不会被阻塞的。所以，我们通常把普通客户端的缓冲区大小限制，以及持续写入量限制、持续写入时间限制都设置为 0，也就是不做限制。
    - 对于订阅客户端来说
        一旦订阅的 Redis 频道有消息了，服务器端都会通过输出缓冲区把消息发给客户端。所以订阅客户端和服务器间的消息发送方式，不属于阻塞式发送。不过，如果频道消息较多的话，也会占用较多的输出缓冲区空间。因此，我们会给订阅客户端设置缓冲区大小限制、缓冲区持续写入量限制，以及持续写入时间限制。
    
    如果主从同步的 `client-output-buffer-limit` 设置过小，并且 master 数据量很大，主从全量同步时可能会导致 buffer 溢出，溢出后主从全量同步就会失败。如果主从集群配置了哨兵，那么哨兵会让 slave 继续向 master 发起全量同步请求，然后 buffer 又溢出同步失败，如此反复，会形成复制风暴，这会浪费 master 大量的 CPU、内存、带宽资源，也会让 master 产生阻塞的风险。

### 6.6.3. 主从集群缓冲区

#### 6.6.3.1. 全量复制缓冲区

主从集群间的数据复制包括全量复制和增量复制两种。全量复制是同步所有数据，而增量复制只会把主从库网络断连期间主库收到的命令，同步给从库。无论在哪种形式的复制中，为了保证主从节点的数据一致，都会用到缓冲区。

在全量复制过程中，主节点在向从节点传输 RDB 文件的同时，会继续接收客户端发送的写命令请求。这些写命令就会先保存在复制缓冲区中，等 RDB 文件传输完成后，再发送给从节点去执行。主节点上会为每个从节点都维护一个复制缓冲区，来保证主从节点间的数据同步。

![](https://r2.129870.xyz/img/20220519235134.png)

针对主从复制的缓冲区，我们可以做以下几点考虑：

- 控制主节点保存的数据量大小
    按通常的使用经验，我们会把主节点的数据量控制在 2~4GB，这样可以让全量同步执行得更快些，避免复制缓冲区累积过多命令。
- 使用 ` client-output-buffer-limit` 配置项，来设置合理的复制缓冲区大小。
    设置的依据，就是主节点的数据量大小、主节点的写负载压力和主节点本身的内存大小。如估算每秒的请求写入量，每条请求的数据量来预设缓冲区大小。
- 控制节点数量
    主节点上复制缓冲区的内存开销，会是每个从节点客户端输出缓冲区占用内存的总和。如果集群中的从节点数非常多的话，主节点的内存开销就会非常大。所以，我们还必须得控制和主节点连接的从节点个数，不要使用大规模的主从集群。

#### 6.6.3.2. 增量复制缓冲区

主节点在把接收到的写命令同步给从节点时，同时会把这些写命令写入复制积压缓冲区。**一旦从节点发生网络闪断，再次和主节点恢复连接后，从节点就会从复制积压缓冲区中，读取断连期间主节点接收到的写命令，进而进行增量同步**。

这个增量缓冲区就是我们提到的 `repl_backlog_buffer`。复制积压缓冲区是一个大小有限的环形缓冲区。当**主节点把复制积压缓冲区写满后，会覆盖缓冲区中的旧命令数据**。如果从节点还没有同步这些旧命令数据，就会造成主从节点间重新开始执行全量复制。

![](https://r2.129870.xyz/img/20220519235655.png)

从本质上看，缓冲区溢出，有以下几个原因：
- 命令数据发送过快
    对于普通客户端来说可以避免 bigkey，而对于复制缓冲区来说，就是避免过大的 RDB 文件。
- 命令数据处理较慢
    解决方案就是减少 Redis 主线程上的阻塞操作，例如使用异步的删除操作。
- 缓冲区空间过小
    解决方案就是使用 `client-output-buffer-limit` 配置项设置合理的输出缓冲区、复制缓冲区和复制积压缓冲区大小。当然，我们不要忘了，输入缓冲区的大小默认是固定的，我们无法通过配置来修改它，除非直接去修改 Redis 源码。