#知识大纲 #Redis

# 1. Redis 大纲

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220517160132.png)

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220517160149.png)

# 2. 使用篇

## 2.1. Redis 的使用场景

- 缓存

- 计数相关：计数器/排行榜/浏览量/播放量等。

- 交并集操作：共同好友，朋友圈点赞。

- 简单消息队列：发布订阅。

- Session 服务器。

- 基于 RedisTimeSeries 模块操作时序数据库。

- 处理秒杀业务，进行库存查询和预减。

## 2.2. Redis 支持的数据结构

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220518165826.png)

- String

- List

- Set

- Sorted Set

- Hash

- Bitmaps

- Hyperloglogs

- Geospatial

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220518231920.png)

## 2.3. Redis 实现消息队列

消息队列设计需要满足以下几个需求：

- 消息顺序：支持按消息生产顺序消费。

- 消息持久化：支持消息的持久化，防止丢失。

- 消息重复消费：支持消息重复消费。

### 2.3.1. 基于 list 实现

1. rpush 生产消息，lpop 消费消息，没有消息时 sleep 或者使用 blpop。

2. 为了支持消息重复消费，可以利用 List 类型提供的 BRPOPLPUSH 命令。这个命令的作用是让消费者程序从一个 List 中读取消息，同时，Redis 会把这个消息再插入到另一个 List（可以叫作备份 List）留存。这样一来，如果消费者程序读了消息但没能正常处理，等它重启后，就可以从备份 List 中重新读取消息并进行处理了。同时需要注意消费成功后移除消息。

3. 消息支持多次消费，使用 pub / sub 模式可以达到 1: N 的消息队列

4. pub / sub 模式的缺点：消费者下线的情况下，生产的消息会丢失

5. 延时队列的实现：基于 SortedSet，以时间戳为 score，消息内容作为 key 来生产消息。调用 zrangebyscore 来获取 N 秒前的数据。

### 2.3.2. 基于 Streams 实现

Redis 5.0 推出了 Stream 数据结构，它借鉴了 Kafka 的设计思想，弥补了 List 和 PubSub 的不足。Stream 类型数据可以持久化、支持 ack 机制、支持多个消费者、支持回溯消费，基本上实现了队列中间件大部分功能，比 List 和 PubSub 更可靠。

Streams 是 Redis 专门为消息队列设计的数据类型，它提供了丰富的消息队列操作命令。

- XADD：插入消息，保证有序，可以自动生成全局唯一 ID。

- XREAD：用于读取消息，可以按 ID 读取数据。同时支持阻塞式读取。

- XREADGROUP：按消费组形式读取消息。

- XPENDING 和 XACK：XPENDING 命令可以用来查询每个消费组内所有消费者已读取但尚未确认的消息，而 XACK 命令用于向消息队列确认消息处理已完成。

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220519121100.png)

## 2.4. Redis 实现分布式锁

### 2.4.1. 基于单节点的锁

- 使用 `setnx` / `del` 添加和释放锁，可带生效时间设定。

- 删除锁时判断线程 ID 是否是自己，防止误删除。

- 重入性使用 state 值来判断。

- 判断锁释放可以通过轮询或者基于 Redis 的发布订阅机制实现。

### 2.4.2. 基于多节点的锁-RedLock

为了避免 Redis 实例故障而导致的锁无法工作的问题，Redis 的开发者 Antirez 提出了分布式锁算法 Redlock。

Redlock 算法的基本思路是让客户端和多个独立的 Redis 实例依次请求加锁，**如果客户端能够和半数以上的实例成功地完成加锁操作**，那么我们就认为，客户端成功地获得分布式锁了；否则加锁失败。这样一来，即使有单个 Redis 实例发生故障，因为锁变量在其它实例上也有保存，所以，客户端仍然可以正常地进行锁操作，锁变量并不会丢失。

RedLock 的加锁步骤如下：

1. 客户端获取当前时间。

2. 客户端按顺序依次向 N 个 Redis 实例执行加锁操作。

	这里的加锁操作和在单实例上执行的加锁操作一样，使用 `SET` 命令，带上 `NX`，`EX` / `PX` 选项，以及带上客户端的唯一标识。
	
	当然，如果某个 Redis 实例发生故障了，为了保证在这种情况下 Redlock 算法能够继续运行，我们需要给加锁操作设置一个超时时间。如果客户端在和一个 Redis 实例请求加锁时，一直到超时都没有成功，那么此时，客户端会和下一个 Redis 实例继续请求加锁。加锁操作的超时时间需要远远地小于锁的有效时间，一般也就是设置为几十毫秒。

3. 一旦客户端完成了和所有 Redis 实例的加锁操作，客户端就要计算整个加锁过程的总耗时。

客户端只有在满足下面的这两个条件时，才能认为是加锁成功：

1. 客户端从超过半数（ `>= N/2+1`）的 Redis 实例上成功获取到了锁。

2. 客户端获取锁的总耗时没有超过锁的有效时间。

在满足了这两个条件后，需要重新计算这把锁的有效时间，计算的结果是锁的最初有效时间减去客户端为获取锁的总耗时。如果锁的有效时间已经来不及完成共享数据的操作了，我们可以释放锁，以免出现还没完成数据操作，锁就过期了的情况。当然，如果客户端在和所有实例执行完加锁操作后，没能同时满足这两个条件，那么，客户端向所有 Redis 节点发起释放锁的操作。

在 Redlock 算法中，释放锁的操作和在单实例上释放锁的操作一样，只要执行释放锁的 Lua 脚本就可以了。释放锁时，要对所有节点释放（即使某个节点加锁失败了），因为加锁时可能发生服务端加锁成功，由于网络问题，给客户端回复网络包失败的情况，所以需要把所有节点可能存的锁都释放掉。这样一来，只要 N 个 Redis 实例中的半数以上实例能正常工作，就能保证分布式锁的正常工作了。

### 2.4.3. RedLock 的思考

#### 2.4.3.1. RedLock 失效场景之节点崩溃

假设一共有 5 个 Redis 节点：A, B, C, D, E。设想发生了如下的事件序列：

1.  客户端 1 成功锁住了 A, B, C，获取锁成功（但 D 和 E 没有锁住）。

2.  节点 C 崩溃重启了，但客户端 1 在 C 上加的锁没有持久化下来，丢失了。

3.  节点 C 重启后，客户端 2 锁住了 C, D, E，获取锁成功。

这样，客户端 1 和客户端 2 同时获得了锁（针对同一资源）。

针对这一情况，可以采取**延迟重启**（delayed restarts）的概念。也就是说，一个节点崩溃后，先不立即重启它，而是等待一段时间再重启，这段时间应该大于锁的有效时间 (lock validity time)。这样的话，这个节点在重启前所参与的锁都会过期，它在重启后就不会对现有的锁造成影响。

但是这个延迟时间需要根据锁的时间来控制，这个通常来说不可控，且延迟重启对集群也会有影响。

#### 2.4.3.2. RedLock 失效场景之时钟跳跃

Martin Kleppmann 在 2016-02-08 这一天发表了一篇 blog，名字叫” [How to do distributed locking ](https://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html) “，Martin 在这篇文章中谈及了分布式系统的很多基础性的问题（特别是分布式计算的异步模型），对分布式系统的从业者来说非常值得一读。其中部分篇幅是对 Redlock 本身的批评。Martin 指出，由于 Redlock 本质上是建立在一个同步模型之上，对系统的记时假设 (timing assumption) 有很强的要求，因此本身的安全性是不够的。Redis 的作者对此进行了[回复](http://antirez.com/news/101)，双方对此进行了许多探讨，许多人也参与到这个探讨中。

在失效场景之节点崩溃中，我们设想了节点发生崩溃未持久化数据而导致的锁失效问题。这是由于节点数据丢失导致的，还有另一种情况会产生类似的效果：时钟跳跃导致锁提前失效。

如果节点 C 上的时钟发生了向前跳跃，导致它上面维护的锁快速过期，那么另一个客户端就能获取到锁，从而导致互斥资源保护锁失效。

针对这一情况，就需要我们确保[[数据密集型系统设计4：分布式的挑战#3 不可靠的时钟|时钟混乱]] 的场景不会发生。

#### 2.4.3.3. RedLock 失效场景之客户端延迟

**即使我们拥有一个完美实现的分布式锁（带自动过期功能），在没有共享资源参与进来提供某种 fencing 机制的前提下，我们仍然不可能获得足够的安全性。**

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220521192441.png)

在上面的时序图中，假设锁服务本身是没有问题的，它总是能保证任一时刻最多只有一个客户端获得锁。上图中出现的 lease 这个词可以暂且认为就等同于一个带有自动过期功能的锁。

假设这样一个场景：客户端 1 在获得锁之后发生了很长时间的 GC pause，在此期间，它获得的锁过期了，而客户端 2 获得了锁。当客户端 1 从 GC pause 中恢复过来的时候，它不知道自己持有的锁已经过期了，它依然向共享资源（上图中是一个存储服务）发起了写数据请求，而这时锁实际上被客户端 2 持有，因此两个客户端的写请求就有可能冲突（锁的互斥作用失效了）。

对于 GC 来说是一种客户端延迟，**系统环境太复杂，仍然有很多原因导致进程的 pause**。比如虚存造成的缺页故障 (page fault)，再比如 CPU 资源的竞争。即使不考虑进程 pause 的情况，网络延迟也仍然会造成类似的结果。

Martin 给出了一种方法，称为 `fencing token`。`fencing token` 是一个单调递增的数字，当客户端成功获取锁的时候它随同锁一起返回给客户端。而客户端访问共享资源的时候带着这个 `fencing token`，这样提供共享资源的服务就能根据它进行检查，拒绝掉延迟到来的访问请求（避免了冲突）。

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220521192853.png)

在上图中，客户端 1 先获取到的锁，因此有一个较小的 fencing token，等于 33，而客户端 2 后获取到的锁，有一个较大的 fencing token，等于 34。客户端 1 从 GC pause 中恢复过来之后，依然是向存储服务发送访问请求，但是带了 fencing token = 33。存储服务发现它之前已经处理过 34 的请求，所以会拒绝掉这次 33 的请求。这样就避免了冲突。

#### 2.4.3.4. 基于 Zookeeper 的锁安全吗？

很多人（也包括 Martin 在内）都认为，如果你想构建一个更安全的分布式锁，那么应该使用 ZooKeeper，而不是 Redis。基于 ZooKeeper 的分布式锁能提供绝对的安全吗？它需要 `fencing token` 机制的保护吗？我们不得不提一下分布式专家 [Flavio Junqueira](https://fpj.me/) 所写的一篇 blog，题目叫 [“Note on fencing and distributed locks”](https://fpj.me/2016/02/10/note-on-fencing-and-distributed-locks/)。

Flavio Junqueira 是 ZooKeeper 的作者之一，他的这篇 blog 就写在 Martin 和 antirez 发生争论的那几天。他在文中给出了一个基于 ZooKeeper 构建分布式锁的描述（当然这不是唯一的方式），需要注意下面的例子并非 Zookeeper 实现分布式锁的最佳示例，这会出现[羊群效应](https://developer.aliyun.com/article/427024) ：

1. 客户端尝试创建一个 `znode` 节点，比如 `/lock`。那么第一个客户端就创建成功了，相当于拿到了锁；而其它的客户端会创建失败（`znode` 已存在），获取锁失败。

2. 持有锁的客户端访问共享资源完成后，将 `znode` 删掉，这样其它客户端接下来就能来获取锁了。

3. `znode` 应该被创建成 `ephemeral` 的。这是 `znode` 的一个特性，它保证如果创建 `znode` 的那个客户端崩溃了，那么相应的 `znode` 会被自动删除。这保证了锁一定会被释放。

看起来这个锁相当完美，没有 Redlock 过期时间的问题，而且能在需要的时候让锁自动释放。但仔细考察的话，并不尽然。

ZooKeeper 是怎么检测出某个客户端已经崩溃了呢？实际上，每个客户端都与 ZooKeeper 的某台服务器维护着一个 Session，这个 Session 依赖定期的心跳 (heartbeat) 来维持。如果 ZooKeeper 长时间收不到客户端的心跳（这个时间称为 Sesion 的过期时间），那么它就认为 Session 过期了，通过这个 Session 所创建的所有的 ephemeral 类型的 znode 节点都会被自动删除。

设想如下的执行序列：

1.  客户端 1 创建了 znode 节点 `/lock`，获得了锁。

2.  客户端 1 进入了长时间的 GC pause。

3.  客户端 1 连接到 ZooKeeper 的 Session 过期了。znode 节点 `/lock` 被自动删除。

4.  客户端 2 创建了 znode 节点 `/lock`，从而获得了锁。

5.  客户端 1 从 GC pause 中恢复过来，它仍然认为自己持有锁。

最后，客户端 1 和客户端 2 都认为自己持有了锁，冲突了。

这与之前 Martin 在文章中描述的由于 GC pause 导致的分布式锁失效的情况类似。虽然用 ZooKeeper 实现的分布式锁也不一定就是安全的。该有的问题它还是有。但是，ZooKeeper 作为一个专门为分布式应用提供方案的框架，它提供了一些非常好的特性，是 Redis 之类的方案所没有的。像前面提到的 ephemeral 类型的 znode 自动删除的功能与 watch 机制等。

所以我们再次强调一遍，**即使我们拥有一个完美实现的分布式锁（带自动过期功能），在没有共享资源参与进来提供某种 fencing 机制的前提下，我们仍然不可能获得足够的安全性。**

## 2.5. BigKey 有什么影响

- 网络阻塞，传输耗时长

- 超时阻塞，操作耗时长

- 内存占用高，空间分配不平衡

## 2.6. 如何查询固定前缀 key

- `Keys` 命令。会阻塞线程，查询时间复杂度是 O (n)，且查询结果是全量，不支持分页。KEYS 命令需要遍历存储的键值对，所以操作延时高。

- Scan 命令。不会阻塞线程，但有可能查出重复数据。不保证能得到查询期间被修改的元素。

	客户端通过执行 `SCAN $cursor COUNT $count` 可以得到一批 key 以及下一个游标 `$cursor`，然后把这个 `$cursor` 当作 `SCAN` 的参数，再次执行，以此往复，直到返回的 `$cursor` 为 0 时，就把整个实例中的所有 key 遍历出来了。
	
	使用 `SCAN` 命令时，不会漏 key，但可能会得到重复的 key，这和 Redis 的 `rehash` 机制有关：
	
	- 为什么不会漏 key？
	
		Redis 在 `SCAN` 遍历全局哈希表时，采用**高位进位**的方式遍历哈希桶，当哈希表扩容后，通过这种算法遍历，旧哈希表中的数据映射到新哈希表，依旧会保留原来的先后顺序，这样就可以保证遍历时不会遗漏也不会重复。
	
	- 为什么 SCAN 会得到重复的 key？
	
		这个情况主要发生在哈希表缩容。已经遍历过的哈希桶在缩容时，会映射到新哈希表没有遍历到的位置，所以继续遍历就会对同一个 key 返回多次。
		
	`SCAN` 是遍历整个实例的所有 key，另外 Redis 针对 `Hash` / `Set` / `Sorted Set` 也提供了 `HSCAN` / `SSCAN` / `ZSCAN` 命令，用于遍历一个 key 中的所有元素，建议在获取一个 bigkey 的所有数据时使用，避免发生阻塞风险。
	
	使用 `HSCAN` / `SSCAN` / `ZSCAN` 命令，返回的元素数量与执行 `SCAN` 逻辑可能不同。执行 `SCAN $cursor COUNT $count` 时一次最多返回 count 个数的 key，数量不会超过 count。但 `Hash` / `Set` / `Sorted Set` 元素数量比较少时，底层会采用 `intset` / `ziplist` 方式存储，如果以这种方式存储，在执行 `HSCAN` / `SSCAN` / `ZSCAN` 命令时，会无视 count 参数，直接把所有元素一次性返回，也就是说，得到的元素数量是会大于 count 参数的。当底层转为哈希表或跳表存储时，才会真正使用发 count 参数，最多返回 count 个元素。

## 2.7. Redis 变慢的可能原因

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220519164941.png)

`redis-cli` 命令提供了 `–intrinsic-latency` 选项，可以用来监测和统计测试期间内的最大延迟。

### 2.7.1. 客户端相关的原因

- 客户端与 Redis 本身网络故障。

- API 使用不合理，如大 key 查询或 keys 扫描，集合统计聚合计算等。

- 设置的过期时间太集中，Redis 清理过期数据工作耗时长。

### 2.7.2. 服务端相关原因

#### 2.7.2.1. 与服务端机器性能相关

- 服务器 CPU 负载过高。

- 内存资源不足造成的内存淘汰与磁盘读取。

- 网络带宽过载。

- 频繁短连接，导致时间耗费在连接建立与释放过程上。应当使用长连接/连接池替代。

- 其它应用程序的资源争夺，更严重的产生内存与磁盘的 swap，导致性能下降。

#### 2.7.2.2. 与 Redis 应用相关

- 持久化过程占用资源过多

	- AOF 过程中若刷盘策略设置为 `always`，则每次更新操作都是 `fsync` 刷盘。并且此时若有 AOF 重写线程的话，这两个线程还会抢占 IO 资源，造成性能下降。如果我们需要高性能，同时也允许数据丢失，可以将配置项 `no-appendfsync-on-rewrite ` 设置为 yes，避免 AOF 重写和 `fsync` 竞争磁盘 IO 资源，导致 Redis 延迟增加。
	
	- 当策略是 `EVERYSEC` 时，主线程会创建子线程去完成 `fsync` 操作。但是当 io 压力大的时候，也就是 `aof_buf` 有积压时。主线程在 `EVERYSEC` 模式下会去判断是否有 `aofwrite` 在执行且用时超过 2s。如果超过 2s 主线程不会 return，将继续等待。但是因为子线程在 `aof_fd` 上 `fsync`，所以 `write aof_fd` 的请求会被堵塞，这里 write 全是主线程在操作，堵塞直到 fsync 完成。
	
	- 在 RDB 文件生成过程中，fork 时会拷贝页表。如果实例很大，那么拷贝过程耗时长。
	
	- 主从同步时，也会生成 RDB 文件。
	
- 正在进行过期数据删除操作，默认情况下，Redis 每 100 毫秒会删除一些过期 key

	采样 `ACTIVE_EXPIRE_CYCLE_LOOKUPS_PER_LOOP` 个数的 key，并将其中过期的 key 全部删除。`ACTIVE_EXPIRE_CYCLE_LOOKUPS_PER_LOOP` 是 Redis 的一个参数，默认是 20，即一秒内基本有 200 个过期 key 会被删除。
	
	如果超过 25% 的 key 过期了，则重复删除的过程，直到过期 key 的比例降至 25% 以下。删除操作是阻塞的（Redis 4.0 后可以用异步线程机制来减少阻塞影响）。所以，一旦该条件触发，Redis 的线程就会一直执行删除，这样一来，就没办法正常服务其他的键值操作了，就会进一步引起其他键值操作的延迟增加，Redis 就会变慢。所以需要避免将过期时间设置在同一时刻。
	
- 内存大页：Linux 内核从 2.6.38 开始支持内存大页机制，该机制支持 2MB 大小的内存页分配，而常规的内存页分配是按 4KB 的粒度来执行的。在 Redis 持久化过程中，采用写时复制技术。内存大页会导致拷贝的页数据变大。

- 多核 CPU 运行时，若 Redis 与 CPU 核心绑定不合理，也会造成性能下降。

- Redis 数据频繁操作，导致内存碎片的产生。碎片整理 Redis 提供了许多可配置的选项，碎片整理是在主线程进行的，所以需要评估影响进行合适的配置。

	可以通过 `info memory` 查看碎片情况，其中包括但不限于以下几个指标：
	
	 -  `mem_fragmentation_ratio` ：表示系统分配给 Redis 的内存和实际使用的内存的比值。正常情况下这个比值大于 1，当**小于 1 的时候就意味着部分数据被 swap 到磁盘去了**，这个时候如果读取这些数据就要从磁盘加载到内存了。
	
	- `active-defrag-cycle-min 25` ： 表示自动清理过程所用 CPU 时间的比例不低于 25%，保证清理能正常开展。
	
	- `active-defrag-cycle-max 75` ：表示自动清理过程所用 CPU 时间的比例不高于 75%，一旦超过，就停止清理，从而避免在清理时，大量的内存拷贝阻塞 Redis，导致响应延迟升高。
	
	- `active-defrag-ignore-bytes 100mb` ：表示内存碎片的字节数达到 100MB 时，开始清理。
	
	- `active-defrag-threshold-lower 10` ：表示内存碎片空间占操作系统分配给 Redis 的总空间比例达到 10% 时，开始清理。

### 2.7.3. 慢日志分析

可以使用 Redis 日志（慢查询日志）和 `latency monitor` 来排查执行较慢的命令操作。在使用慢查询日志前，我们需要设置两个参数：

- slowlog-log-slower-than：慢查询日志对执行时间大于多少微秒的命令进行记录。

- slowlog-max-len：慢查询日志最多能记录多少条命令记录。

我们可以使用 `SLOWLOG GET` 命令，来查看慢查询日志中记录的命令操作。Redis 从 `2.8.13` 版本开始，还提供了 `latency monitor` 监控工具，这个工具可以用来监控 Redis 运行过程中的峰值延迟情况。这个工具是通过扫描数据库来查找 bigkey 的，所以，在执行的过程中，会对 Redis 实例的性能产生影响。

### 2.7.4. bigkey 排查

- Redis 可以在执行 `redis-cli` 命令时带上 `–bigkeys` 选项，进而对整个数据库中的键值对大小情况进行统计分析。

- 使用 scan 命令扫描 key，然后手动统计。可以使用 `MEMORY USAGE` 命令查询某个 key 占用的内存空间。

- 利用第三方工具，如 `rdb-tools`。这是基于 rdb 文件统计的一个工具。

### 2.7.5. 数据倾斜问题

数据倾斜有两类：

1. 数据量倾斜：在某些情况下，实例上的数据分布不均衡，某个实例上的数据特别多。

	1. 存在 bigkey。对于 bigkey，尽量将数据打散，将集合类型数据拆分成小集合。业务层避免创建 bigkey 数据。
	
	2. `Slot` 分配不均匀。查看每个节点的 `Slot` 分配与使用情况，必要时重新分配 `Slot`。
	
	3. 使用 `Hash Tag` 导致数据集中到同一个 `Slot`。`Hash Tag` 是命令操作的 key 包含 `{}` 符号，此时 crc16 计算时只会计算 `{}` 符号内的内容从而导致数据落到一个 `Slot` 上。`Hash Tag` 可用于事务命令执行，因为大多事务要求在同一个节点执行。
	
2. 数据访问倾斜：虽然每个集群实例上的数据量相差不大，但是某个实例上的数据是热点数据，被访问得非常频繁。

	存在热点数据：对于热点数据加随机前缀或后缀，将其打散到多个节点。这种方式适合读多写少的数据，否则数据同步更新是一个问题。

## 2.8. 缓存问题

### 2.8.1. 缓存雪崩

原因：由于大批量的缓存突然失效导致请求都打到了数据库上。特别要注意一致性哈希环的场景，某个节点挂点数据分散到其他集群导致其他集群也挂掉。

措施：

- 缓存失效时间分开，设置随机失效时间。

- 控制数据库写入操作，只允许一个线程写入。

- 服务降级，熔断，限流机制。

### 2.8.2. 缓存穿透

原因：查询缓存中没有的数据，请求打到了数据库。更有甚者，数据库也没有这条数据，导致无法将数据更新到缓存从而导致请求一直到达数据库。

措施：

- 使用布隆过滤器拦截。

	布隆过滤器会有误判：由于采用固定 bit 的数组，使用多个哈希函数映射到多个 bit 上，有可能会导致两个不同的值都映射到相同的一组 bit 上。即布隆过滤器说不存在的一定不存在，说存在的不一定存在。
	
	布隆过滤器误判率和空间使用的计算：误判本质是因为哈希冲突，降低误判的方法是增加哈希函数 + 扩大整个 bit 数组的长度，但增加哈希函数意味着影响性能，扩大数组长度意味着空间占用变大，所以使用布隆过滤器，需要在误判率和性能、空间作一个平衡。
	
	布隆过滤器可以放在缓存和数据库的最前面。但需要注意**布隆过滤器的 bigkey 问题**：Redis 布隆过滤器是使用 String 类型实现的，存储的方式是一个 bigkey。
	
- 缓存空的数据，并设置过期时间。

- 对于热点数据不设置失效时间。

### 2.8.3. 缓存预热

原理：自动将热点数据加载到缓存中

### 2.8.4. 缓存更新

#### 2.8.4.1. Cache-Aside 旁路缓存模式

做法：

- 读请求

	1. 先查询缓存
	
	2. 缓存中有直接返回
	
	3. 缓存中没有查询数据库更新缓存
	
- 写请求

	1. 先更新数据库
	
	2. 然后删除缓存

选择原因：

- 为何不先删除缓存后更新数据库？

	若先删除缓存，在缓存删除期间产生读请求，可能会将未更新的数据查询到缓存中导致缓存脏数据。
	
- 若选择先删除缓存后更新数据库，如何解决一致性问题？

	采用延时双删，更新数据库后延时一段时间再次删除缓存，总共删除两次。这种做法不但需要两次删除而且有延迟，所以不推荐使用。
	
- 为何是删除缓存而不是更新缓存？

	若产生两个并发的写请求，因为各种原因导致先来请求缓存更新操作晚于后来的请求，同样会导致缓存脏数据。
	
- Cache-Aside 存在数据不一致的可能吗？

	存在，若缓存失效期间同时产生写请求与读请求，且读请求的缓存更新操作晚于写请求的缓存删除操作，这个时候也会出现数据不一致问题。这种情况要求缓存失效且读写同时触发，条件比较复杂。
	
	另一种情况是读请求先读到了缓存，写请求更新了数据。这个时候缓存与数据库数据不一致，也是一种一致性问题。若是业务对此要求严格一致，可采取加锁方式解决。

Cache-Aside 的异常补偿机制，删除缓存时存在缓存删除失败的问题，需要对此作出补偿策略：

- 删除重试机制，同步删除重试影响性能，因此可以使用异步重试删除，如使用 MQ。但这样引入了 MQ 中间件，以及删除失败后的逻辑可能需要业务作一定补偿。

- 监听 binlog 日志，解析 binlog 日志来处理缓存删除失败的问题。注意这里优先选择监听从数据库的 binlog 日志，防止主节点事务还未完成就过早的删除了缓存。

- 采用云服务商提供的 DTS (数据传输) 服务，DTS 服务适配了常见的数据源与数据操作场景，解决了如 binlog 日志回收，主备切换场景下的高可用问题。

适用点：

- 缓存数据计算逻辑复杂

- 数据一致性要求高

- 不存在 bigkey 或热点数据

#### 2.8.4.2. Read-Through 读穿透模式

流程和 Cache-Aside 模式相似，不同点在于 Read-Through 多了访问控制层，读请求只和访问控制层交换，缓存能否命中与读请求无关。

#### 2.8.4.3. Write-Through 直写模式

同样提供了访问控制层来进行更高程度的封装。不同于 Cache-Aside 模式的是 Write-Through 不是删除缓存而是更新缓存。该模式适合写操作多且对一致性要求高的场景。

#### 2.8.4.4. Write-Behind 异步回写模式

写请求只更新缓存不更新数据库，数据库在合适的时机异步批量更新。

这种模式写延迟低，吞吐性好，但一致性弱，需要缓存做好高可用，适合于大量的写请求场景。如秒杀，MQ 的消息存储机制等。

#### 2.8.4.5. Write-Around

写请求不更新缓存，缓存设定失效时间，由失效时间自动更新。这种模式适用于一致性要求不高的业务场景。

### 2.8.5. 缓存降级

缓存失效或缓存服务器挂掉的情况下不去访问数据库直接返回内存数据或默认数据，以此减少降级对业务对影响操作，这是饮鸩止渴的操作，为了保证服务正常运行而不得已做出的措施。

在发生缓存雪崩时，为了防止引发连锁的数据库雪崩甚至是整个系统的崩溃，我们暂停业务应用对缓存系统的接口访问。再具体点说，就是业务应用调用缓存接口时缓存客户端请求，并不把请求发给 Redis 缓存实例而是直接返回，等到 Redis 缓存实例重新恢复服务后再允许应用请求发送到缓存系统。

服务熔断虽然可以保证数据库的正常运行，但是暂停了整个缓存系统的访问，对业务应用的影响范围大。为了尽可能减少这种影响，我们也可以进行请求限流。这里说的请求限流，就是指，我们在业务系统的请求入口前端控制每秒进入系统的请求数，避免过多的请求被发送到数据库。

### 2.8.6. 客户端缓存

`Redis 6.0` 新增了一个重要的特性，就是实现了**服务端协助的客户端缓存功能**，也称为跟踪（`Tracking`）功能。有了这个功能，业务应用中的 Redis 客户端就可以把读取的数据缓存在业务应用本地了，应用就可以直接在本地快速读取数据了。

`6.0` 实现的 `Tracking` 功能实现了两种模式：

1. 普通模式

	实例会在服务端记录客户端读取过的 key，并监测 key 是否有修改。一旦 key 的值发生变化，服务端会给客户端发送 `invalidate` 消息，通知客户端缓存失效了。
	
	在使用普通模式时，服务端对于记录的 key **只会报告一次 invalidate 消息**，也就是说，服务端在给客户端发送过一次 `invalidate` 消息后，如果 key 再被修改，此时服务端就不会再次给客户端发送 `invalidate` 消息。只有当客户端再次执行读命令时，服务端才会再次监测被读取的 key，并在 key 修改时发送 invalidate 消息。
	
	这样设计的考虑是节省有限的内存空间。毕竟如果客户端不再访问这个 key 了，而服务端仍然记录 key 的修改情况，就会浪费内存资源。
	
2. 广播模式

	服务端会给客户端广播所有 key 的失效情况。广播模式下，如果 key 被频繁修改，服务端会发送大量的失效广播消息，这就会消耗大量的网络带宽资源。
	
	在实际应用时，我们会让客户端注册希望跟踪的 key 的前缀，当带有注册前缀的 key 被修改时，服务端会把失效消息广播给所有注册的客户端。
	
	和普通模式不同，在**广播模式下，即使客户端还没有读取过 key，但只要它注册了要跟踪的 key，服务端都会把 key 失效消息通知给这个客户端**。

# 3. 高性能

## 3.1. Redis 效率高的原因

- C 语言实现，效率高。

- 纯内存操作。

- 基于非阻塞式的 IO 复用模型。

	- 基于 `select` / `epoll` 的事件回调机制，使 Redis 可以在网络 IO 阻塞时处理其他事情。
	
	- 由操作系统内核与客户端建立连接，Redis 监听这些连接的事件进行处理。
	
	- 注意这里网络 IO 操作是非阻塞的。对于 Redis 的 server 层来说，如果阻塞的话则会影响后面的所有命令，比如 bigkey 操作、复杂命令的执行、缓存淘汰和刷盘机制等。
	
- 单线程避免上下文切换，各种锁操作。

	- Redis 的**瓶颈往往在于内存和网络带宽**，不在 CPU。
	
	- 单线程避免调度切换开销。
	
- 丰富的数据结构，对数据存储做了优化，如压缩表、跳表。

## 3.2. Redis 为什么是单线程的

Redis 中的多线程：

- 在 `Redis 4.0` 中引入了多线程处理异步任务

- 在 `Redis 6.0` 中对网络模型实现了多线程 IO

	多线程用于处理网络数据的读写和协议解析：主线程将可读 Socket 分发给 IO 线程组进行并行请求解析，解析完毕的命令执行还是单线程的，执行结果交给 IO 线程组写回 Socket 是并行的。
	
	即将与 Socket 相关的读写操作变为并行执行的，以此减轻网络 IO 的负担。但是命令执行还是由主线程执行的，仍是单线程，且是线程安全的。

## 3.3. Redis 数据结构

### 3.3.1. Redis 的数据存储结构

#### 3.3.1.1. RedisObject

对于 Redis 的数据，不同数据类型都有些相同的元数据要记录（比如最后一次访问的时间、被引用的次数等），所以 Redis 会用一个 `RedisObject` 结构体来统一记录这些元数据，同时存储一个指针指向实际数据。因此对于存储在 Redis 中的对象，其结构如下：

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220518222536.png)

- 元数据信息占 8 字节

- 真实数据指针占 8 字节

为了节省内存空间，Redis 还对 Long 类型整数和 SDS 的内存布局做了专门的设计。

- 当保存的是 Long 类型整数时

	RedisObject 中的指针就直接赋值为整数数据，这样就不用额外的指针再指向整数了，节省了指针的空间开销。

- 当保存的是字符串数据时

	- 若字符串小于等于 44 字节时，RedisObject 中的元数据、指针和 SDS 是一块连续的内存区域，这样就可以避免内存碎片。这种布局方式也被称为 `embstr` 编码方式。
	
	- 当字符串大于 44 字节时，SDS 的数据量就开始变多了，Redis 就不再把 SDS 和 RedisObject 布局在一起了，而是会给 SDS 分配独立的空间，并用指针指向 SDS 结构。这种布局方式被称为 `raw` 编码模式。

	上述 44 字节的计算方式为：`len`、`cap` 以及新增的 `flag` 使用的都是 `int8` 类型，只占用 1 个字节，这样 64 (`cpu cache line` 为 64)-16（ReadObj 头部）-3（sds 头部）-1（buf 末尾 `'\0'`）= 44。
	
	![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220518223052.png)

#### 3.3.1.2. Hash dictEntry

Redis 会使用一个全局哈希表保存所有键值对，哈希表的每一项是一个 `dictEntry` 的结构体，用来指向一个键值对。`dictEntry` 结构中有三个 8 字节的指针，分别指向 `key`、`value` 以及下一个 `dictEntry`，三个指针共 24 字节，如下图所示：

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220518223353.png)


![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220518165826.png)

### 3.3.2. 简单动态字符串

字符串类型在 Redis 中的实现为 `简单动态字符串 (Simple Dynamic String，SDS)` 格式，其存储结构如下：

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220518222037.png)

- `buf` ：字节数组，保存实际数据。数组最后额外的一个 `'\0'`，用于表示字节数组的结束。这会额外占用 1 个字节的开销。

- `len` ：占 4 个字节，表示 buf 的已用长度。新版的 SDS 使用 int8 存储，只占 1 字节。

- `alloc` ：也占个 4 字节，表示 buf 的实际分配长度，一般大于 len。新版的 SDS 使用 int8 存储，只占 1 字节。

### 3.3.3. 压缩列表

#### 3.3.3.1. 压缩列表的定义

压缩列表实际上类似于一个数组，数组中的每一个元素都对应保存一个数据。和数组不同的是，压缩列表在表头有三个字段 `zlbytes`、`zltail` 和 `zllen`，分别表示列表字节长度、列表尾的偏移量和列表中的 `entry` 个数。压缩列表在表尾还有一个 `zlend`，表示列表结束。

在压缩列表中，如果我们要查找定位第一个元素和最后一个元素，可以通过表头三个字段的长度直接定位，复杂度是 O (1)。而查找其他元素时，就没有这么高效了，只能逐个查找，此时的复杂度就是 O (N) 了。

压缩列表中**每个数据可以具有不同的长度**，因此具有**压缩**的特性。

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220518172301.png)

#### 3.3.3.2. 压缩列表的存储

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220518224610.png)

压缩列表之所以能节省内存，在于它是用一系列连续的 `entry` 保存数据。每个 `entry` 的元数据包括下面几部分：

- `prev_len`，表示前一个 entry 的长度。

	`prev_len` 有两种取值情况：
	
	- `1 字节` ：取值 1 字节时，表示上一个 `entry` 的长度**小于 254 字节**。
	
		- 当上一个 entry 长度小于 `254` 字节时，`prev_len` 取值为 `1 字节`。
		
		- 对于 `254`，用于表示 `5 字节` 长度开始的标识。
		
		- 对于 `255`，为压缩列表中 `zlend` 默认取值。因此，就默认用 `255` 表示整个压缩列表的结束，其他表示长度的地方就不能再用 `255` 这个值了。
		
	- `5 字节` ：由两部分组成：
	
		- 第一部分为 `1 字节`，值为 `254`，表示这是一段 `5 字节` 表示的数据。
		
		- 第二部分为 `4 字节`，实际的长度。
		
- `encoding` ：表示编码方式，1 字节。

- `data` ：保存实际数据。

这些 `entry` 连续在内存中，不需要再用额外的指针进行连接，这样就可以节省指针所占用的空间。

压缩列表可以被用于 `Hash` 结构的实现，但当超过以下阈值时，`Hash` 结构将不再使用压缩列表而转为哈希表：

- `hash-max-ziplist-entries` ：表示用压缩列表保存时哈希集合中的最大元素个数。

- `hash-max-ziplist-value` ：表示用压缩列表保存时哈希集合中单个元素的最大长度。

我们往 `Hash` 集合中写入的元素个数超过了 `hash-max-ziplist-entries`，或者写入的单个元素大小超过了 `hash-max-ziplist-value`，Redis 就会自动把 `Hash` 类型的实现结构由压缩列表转为哈希表。

#### 3.3.3.3. 压缩列表的使用事项

当使用压缩列表存储时，我们尽量存储 `int` 数据，压缩列表在设计时每个 `entry` 都进行了优化，针对要存储的数据，会尽量选择占用内存小的方式存储（整数比字符串在存储时占用内存更小），这也有利于我们节省 Redis 的内存。

压缩列表是每个元素紧凑排列，而且每个元素存储了上一个元素的长度，所以**当修改其中一个元素超过一定大小时，会引发多个元素的级联调整**（前面一个元素发生大的变动，后面的元素都要重新排列位置，重新分配内存），这也会引发性能问题，需要注意。

采用压缩列表方式存储时，虽然可以节省内存空间，但是在查询指定元素时，都要遍历整个列表才能找到指定的元素。所以使用压缩列表方式存储时，虽然可以利用 CPU 高速缓存，但也不适合存储过多的数据（`hash-max-ziplist-entries` 和 `zset-max-ziplist-entries` 不宜设置过大），否则查询性能就会下降比较厉害。

### 3.3.4. 跳表

跳表在链表的基础上，增加了多级索引，通过索引位置的几个跳转，实现数据的快速定位。

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220518172416.png)

### 3.3.5. BitMap

`Bitmap` 本身是用 `String` 类型作为底层数据结构实现的一种**统计二值状态的数据类型**。`String` 类型是会保存为二进制的字节数组，所以，Redis 就把字节数组的每个 bit 位利用起来，用来表示一个元素的二值状态。

可以把 `Bitmap` 看作是一个 bit 数组。Bitmap 提供了 `GETBIT` / `SETBIT` 操作，使用一个偏移值 `offset` 对 bit 数组的某一个 bit 位进行读和写。不过，需要注意的是，`Bitmap` 的偏移量是从 0 开始算的，也就是说 `offset` 的最小值是 `0`。当使用 `SETBIT` 对一个 bit 位进行写操作时，这个 bit 位会被设置为 `1`。`Bitmap` 还提供了 `BITCOUNT` 操作，用来统计这个 bit 数组中所有 `1` 的个数。

### 3.3.6. HyperLogLog

`HyperLogLog` 是一种用于统计基数的数据集合类型，它的最大优势就在于，**当集合元素数量非常多时，它计算基数所需的空间总是固定的，而且还很小**。

在 Redis 中，每个 `HyperLogLog` 只需要花费 `12 KB` 内存，就可以计算接近 $2^{64}$ 个元素的基数。

`HyperLogLog` 的统计规则是基于概率完成的，所以它给出的统计结果是有一定误差的，标准误算率是 `0.81%`。这也就意味着，你使用 `HyperLogLog` 统计的 UV 是 100 万，但实际的 UV 可能是 101 万。虽然误差率不算大，但是，如果你需要精确统计结果的话，最好还是继续用 `Set` 或 `Hash` 类型。

### 3.3.7. Geospatial

GEO 类型的底层数据结构就是用 `Sorted Set` 来实现的，其中 `Set` 的 key 为数据的 key，而 value 为经纬度的编码值。

为了能高效地对经纬度进行比较，Redis 采用了业界广泛使用的 `GeoHash` 编码方法，这个方法的基本原理就是 `二分区间，区间编码`。

在进行第一次二分区时，经度范围\[-180, 180]会被分成两个子区间：\[-180, 0) 和\[0, 180]。此时，我们可以查看一下要编码的经度值落在了左分区还是右分区。如果是落在左分区，我们就用 0 表示；如果落在右分区，就用 1 表示。

这样一来，每做完一次二分区，我们就可以得到 1 位编码值。然后，我们再对经度值所属的分区再做一次二分区，同时再次查看经度值落在了二分区后的左分区还是右分区，按照刚才的规则再做 1 位编码。当做完 N 次的二分区后，经度值就可以用一个 N bit 的数来表示了。

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220518234740.png)
![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220518234746.png)
![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220518234819.png)

当一组经纬度值都编完码后，我们再把它们的各自编码值组合在一起，组合的规则是：最终编码值的偶数位上依次是经度的编码值，奇数位上依次是纬度的编码值，即偶数位从 0 开始，奇数位从 1 开始。

用了 `GeoHash` 编码后，原来无法用一个权重分数表示的一组经纬度（116.37，39.86）就可以用 1110011101 这一个值来表示，就可以保存为 `Sorted Set` 的权重分数了。

使用 `GeoHash` 编码后，我们相当于把整个地理空间划分成了一个个方格，每个方格对应了 `GeoHash` 中的一个分区。所以，我们使用 `Sorted Set` 范围查询得到的相近编码值，在实际的地理空间上，也是相邻的方格，这就可以实现 LBS 应用“搜索附近的人或物”的功能了。

有的编码值虽然在大小上接近，但实际对应的方格却距离比较远。例如，我们用 4 位来做 GeoHash 编码，把经度区间\[-180, 180]和纬度区间\[-90, 90]各分成了 4 个分区，一共 16 个分区，对应了 16 个方格。编码值为 0111 和 1000 的两个方格就离得比较远，如下图所示：所以，为了避免查询不准确问题，我们可以同时查询给定经纬度所在的方格周围的 4 个或 8 个方格。

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220518235015.png)

## 3.4. Pipline

`pipline` 将多次 IO 压缩成一次，但是要求管道中的指令没有因果关系。使用 `pipline` 可以实现请求/响应的功能：客户端未读取服务端响应时服务端可处理新的请求，客户端发送多个命令时只需等待服务端最终结果。

## 3.5. 引起 Redis 阻塞的几种操作

- 集合全量查询和聚合操作。

- bigkey 删除，可以利用子线程进行，称之为 `lazy-free`。

	`lazy-free` 是 `Redis 4.0` 新增的功能，但是默认是关闭的，需要手动开启。手动开启 `lazy-free` 时，有 4 个选项可以控制，分别对应不同场景下，是否开启异步释放内存机制： 
	
	- `lazyfree-lazy-expire` ：key 在过期删除时尝试异步释放内存。
	
	- `lazyfree-lazy-eviction` ：内存达到 `max memory` 并设置了淘汰策略时尝试异步释放内存。
	
	- `lazyfree-lazy-server-del` ：执行 `RENAME` / `MOVE` 等命令或需要覆盖一个 key 时，删除旧 key 尝试异步释放内存。
	
	- `replica-lazy-flush` ：主从全量同步，从库清空数据库时异步释放内存

- 清空数据库。

- AOF 日志同步写，可以利用子线程进行。

- 从库加载 RDB 文件。

### 3.5.1. Lazy-free

即使开启了 `lazy-free`，如果直接使用 `DEL` 命令还是会同步删除 key，只有使用 `UNLINK` 命令才会可能异步删除 key。

上面提到开启 `lazy-free` 的场景，除了 `replica-lazy-flush` 之外，其他情况都只是可能去异步释放 key 的内存，并不是每次必定异步释放内存的。 

开启 `lazy-free` 后，Redis 在释放一个 key 的内存时首先会评估代价，如果释放内存的代价很小，那么就直接在主线程中操作了，没必要放到异步线程中执行（不同线程传递数据也会有性能消耗）。什么情况才会真正异步释放内存？这和 key 的类型、编码方式、元素数量都有关系（详细可参考源码中的 `lazyfreeGetFreeEffort` 函数）：

- 当 `Hash` / `Set` 底层采用哈希表存储（非 `ziplist` / `int` 编码存储）时，并且元素数量超过 64 个。

- 当 `ZSet` 底层采用跳表存储（非 `ziplist` 编码存储）时，并且元素数量超过 64 个。

- 当 `List` 链表节点数量超过 64 个（注意，不是元素数量，而是链表节点的数量，List 的实现是在每个节点包含了若干个元素的数据，这些元素采用 `ziplist` 存储）。

只有以上这些情况，在删除 key 释放内存时，才会真正放到异步线程中执行，其他情况一律还是在主线程操作。也就是说 `String`（不管内存占用多大）、`List`（少量元素）、`Set`（`int` 编码存储）、`Hash` / `ZSet`（`ziplist` 编码存储）这些情况下的 key 在释放内存时，依旧在主线程中操作。 

可见，即使开启了 `lazy-free`，`String` 类型的 bigkey 在删除时依旧有阻塞主线程的风险。所以，即便 Redis 提供了 `lazy-free`，还是尽量不要在 Redis 中存储 bigkey。 

**Redis 在设计评估释放内存的代价时，不是看 key 的内存占用有多少，而是关注释放内存时的工作量有多大**。从上面分析基本能看出，如果需要释放的内存是连续的，Redis 作者认为释放内存的代价比较低，就放在主线程做。如果释放的内存不连续（大量指针类型的数据），这个代价就比较高，所以才会放在异步线程中去执行。

## 3.6. Redis 高效利用 CPU 的方式

### 3.6.1. CPU 架构之 NUMA 架构

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220519125629.png)

多核 CPU 通过共享总线对内存进行访问，那么对总线对争用将会称为性能对阻碍。

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220519125740.png)

`Non-Uniform Memory Access`，`NUMA` 架构通过把 CPU 和临近的 RAM 当做一个 `node`，CPU 会优先访问距离近的 RAM。同时，CPU 直接有一个快速通道连接，所以每个 CPU 还是访问到所有的 RAM 位置（只是速度会有差异）。

采用 `NUMA` 架构虽然减少了总线的争用，但是**若程序发生了 CPU 的切换，那么 CPU 中的缓存数据就要重新加载**，这是 `NUMA` 架构存在的问题：

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220519130036.png)

### 3.6.2. Redis 进行 CPU 层面的优化

#### 3.6.2.1. 绑定 CPU 与 Redis 程序

如果在 CPU 多核场景下，Redis 实例被频繁调度到不同 CPU 核上运行的话，那么，对 Redis 实例的请求处理时间影响就更大了。每调度一次，**一些请求就会受到运行时信息、指令和数据重新加载过程的影响**，这就会导致某些请求的延迟明显高于其他请求。因此可以把 Redis 与运行的 CPU 进行绑定，避免 CPU 的切换带来的数据重载问题：

```bash

taskset -c 0 ./redis-server

```

 Redis 实例和网络中断程序的数据交互：网络中断处理程序从网卡硬件中读取数据，并把数据写入到操作系统内核维护的一块内存缓冲区。内核会通过 epoll 机制触发事件，通知 Redis 实例，Redis 实例再把数据从内核的内存缓冲区拷贝到自己的内存空间。那么，在 CPU 的 `NUMA` 架构下，当网络中断处理程序、Redis 实例分别和 CPU 核绑定后，就会有一个潜在的风险：如果网络中断处理程序和 Redis 实例各自所绑的 CPU 核不在同一个 CPU Socket 上，那么，Redis 实例读取网络数据时，就需要跨 CPU Socket 访问内存，这个过程会花费较多时间。
 
![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220519130422.png)
![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220519130432.png)

所以，为了避免 Redis 跨 CPU Socket 访问网络数据，我们最好把网络中断程序和 Redis 实例绑在同一个 CPU Socket 上，这样一来，Redis 实例就可以直接从本地内存读取网络数据了，如下图所示：

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220519130503.png)

#### 3.6.2.2. 绑核带来的问题

当我们把 Redis 实例绑到一个 CPU 逻辑核上时，就会导致子进程、后台线程和 Redis 主线程竞争 CPU 资源，一旦子进程或后台线程占用 CPU 时，主线程就会被阻塞，导致 Redis 请求延迟增加。

对于上述情况，我们可以采取以下策略优化：

- 一个 Redis 实例对应绑一个物理核：在给 Redis 实例绑核时，不要把一个实例和一个逻辑核绑定，而要和一个物理核绑定。也就是说，把一个物理核的 2 个逻辑核都用上。

- 优化 Redis 源码：通过修改 Redis 源码，把子进程和后台线程绑到不同的 CPU 核上。Redis 6.0 后，可以支持 CPU 核绑定的配置操作了。

虽然与 CPU 进行关系绑定可以在一定程度上可以带来收益，但是我们仍需要关注 CPU 层面的使用情况。若绑定的 CPU 处于忙碌状态，上面已经绑定了其他的程序，那么 CPU 缓存的公用就会带来淘汰问题。

# 4. 高可用

## 4.1. Redis 的主从同步

### 4.1.1. 主从同步的过程

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220517231216.png)

主从同步用于备份主库的数据，从库通过与主库建立连接而不断获取到主库到数据。建立连接的步骤如下：

1. 从库使用 `replicaof` / `slaveof` (5.0 之前) 来开始主库建立连接。

2. 从库发送 `psync` 命令给主库建立连接，首次建立连接 `runId` 为 -1，`offset` 为 -1。`runId` 为机器 ID，`offset` 为复制进度，首次 -1 表示从头开始。

3. 主库通过 `FULLRESYNC` 响应命令带上两个参数：主库 `runID` 和主库目前的复制进度 `offset`，返回给从库。

4. 主库执行 `bgsave` 命令得到 RDB 文件给从库。从库接收到文件后清空当前数据，然后解析 RDB 恢复数据。使用 RDB 文件有以下几点原因：

	- RDB 文件是压缩过的二进制文件，文件内容小。对于网络传输快。
	
	- RDB 文件直接使用二进制协议解析还原数据，避免 AOF 文件的命令重复执行（一条数据的多次修命令）。
	
5. 对于生成 RDB 文件期间和从库解析 RDB 文件期间产生的数据操作命令，记录到了主库的 `replication buffer` 中，待后续从库解析完后再根据这些命令补充数据。

6. 首次备份完毕后主从之间通过长连接维护通信，主库将每次写入命令都推送到从库。

在 `Redis Cluster` 模式下，若发生了主从切换，切换时间由于太长心跳检测超时会被集群标记为异常。当过多节点出现这种情况时就会导致集群异常。所以需要注意心跳检测时间 `cluster-node-timeout` 的配置。

### 4.1.2. 主从同步的重连

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220517234150.png)

主从连接建立完毕以后就可以正常同步数据了，但是当主从之间因为某些原因断开重新建立连接后，双方数据怎么继续同步呢？

对于主库，在每次收到写入操作后，会将命令存入到 `repl_backlog_buffer` 环形缓冲区。主从之间的通信，会有一个 `replication buffer`。`repl_backlog_buffer` 用于记录主库的写入命令，对于主库来说只有一个。而 `replication buffer` 是对每一个从库都有的，这用于主从之间发送缓冲数据使用。这两者之间更为详细的对比如下：

- `repl_backlog_buffer` 是为了从库断开之后，如何找到主从差异数据而设计的环形缓冲区，从而避免全量同步带来的性能开销。各个从节点的 offset 偏移量都是相对该缓冲区而言的。若一个从库都没有，那么这个缓冲区存在就没有意义了，此时会被释放。

- `replication buffer` 是为了主从之间数据通信**缓冲**所使用的，就像 Mysql 中的 [[Mysql大纲#4 3 数据流转过程|net_buffer]]。这个 buffer 是针对每一个从库都会有一个的。不管是全量同步还是增量同步，都会使用到这个缓冲区。

- 如果主从在传播命令时，因为某些原因从库处理得非常慢，那么主库上的这个 `replication buffer` 就会持续增长，消耗大量的内存资源，甚至 OOM。所以 Redis 提供了 `client-output-buffer-limit` 参数限制这个 buffer 的大小，如果超过限制，主库会强制断开这个 client 的连接，也就是说从库处理慢导致主库内存 buffer 的积压达到限制后，主库会强制断开从库的连接，此时主从复制会中断，中断后如果从库再次发起复制请求，那么此时可能会导致恶性循环，引发复制风暴，这种情况需要格外注意。

对于缓冲区 `repl_backlog_buffer`，主库使用 `master_repl_offset` 来记录写入的位置。从库使用 `slave_repl_offset` 记录自己从主库读取数据的位置。这样在主从之间数据通信的时候，只需要将两个 offset 之间的数据进行同步即可。

对于主从重新建立连接后，有两种情况：

1. 主库的 `master_repl_offset` 还未追赶上从库的 `slave_repl_offset`，这时只要发送 offset 之间的数据即可。

2. 而如果主库的 `master_repl_offset` 已经追赶上从库的 `slave_repl_offset`，那么将会进行数据的覆盖。数据覆盖后从库就不能通过 offset 进行数据同步了，此时只能进行全量数据同步。

因此需要特别留意 `repl_backlog_size` 这个配置参数。如果它配置得过小，在增量复制阶段，可能会导致从库的复制进度赶不上主库，进而导致从库重新进行全量复制。

对于 `repl_backlog_buffer` 的大小，可以通过 `repl_backlog_size` 参数进行控制。这个参数和所需的缓冲空间大小有关。缓冲空间的计算公式是：$缓冲空间大小 = 主库写入命令速度 \times 操作大小 - 主从库间网络传输命令速度 \times 操作大小$。在实际应用中，考虑到可能存在一些突发的请求压力，我们通常需要把这个缓冲空间扩大一倍，即 $repl\_backlog\_size = 缓冲空间大小 \times 2$，这也就是 `repl_backlog_size` 的最终值。

### 4.1.3. 主从数据不一致问题

主从数据可能出现不一致的场景如下：

- 主库数据更改命令未及时同步到从库，或者已经同步到从库但从库在执行其他命令如集合操作而阻塞。所以主从数据不一致。

- 主库数据已过期，但未删除。当触发删除逻辑时会给从库同步删除命令。在 3.2 版本以前，从库若读到过期数据是不会触发删除逻辑的，惰性删除策略只在主库有。在 3.2 版本后若从库读到过期数据会返回空。

- 主库数据接收到了设置过期命令的命令，该命令传到从库会有延迟。如 `expire key 60` 设定 60s 后过期，可是从库接收到时开始执行时间和主库已经不一致了，会有这段延迟。使用 `expire at` 没有这个问题。

可以使用 `slave-serve-stale-data` 来配置从库读取命令的一些策略。当一个 slave 与 master 失去联系时，或者复制正在进行的时候，slave 应对请求的行为：

- yes：slave 仍然会应答客户端请求，但返回的数据可能是过时，或者数据可能是空的（在第一次同步的时候）。

- no ：在执行除了 `info` 和 `salveof` 之外的其他命令时，slave 都将返回一个 `SYNC with master in progress` 的错误。

### 4.1.4. 脑裂问题

当由于主库出现网络阻塞或者资源被抢占导致哨兵监听无法正常响应时，主库会被标记为客观下线并选举出新的主库。而若是主库只是短暂的不可用在哨兵选举期间恢复正常，原主库与新主库共存，就出现了脑裂问题。等到哨兵让原主库执行 `slave of` 后，才会只保留新主库。

在新主库完全切换完之前，客户端请求仍能正常写入主库。等到哨兵选举出新的主库而原主库这段期间接收到的请求未同步给从库，从而导致新主库无这些数据。造成数据不一致。

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220521220627.png)

这个问题是出在原主库发生假故障后仍然能接收请求上，我们就开始在主从集群机制的配置项中查找是否有限制主库接收请求的设置：

- `min-slaves-to-write` ：这个配置项设置了主库能进行数据同步的最少从库数量。

- `min-slaves-max-lag` ：这个配置项设置了主从库间进行数据复制时，从库给主库发送 ACK 消息的最大延迟（以秒为单位）。通常配置  $min\mbox{-}slaves\mbox{-}max\mbox{-}lag  \leq  down\mbox{-}after\mbox{-}milliseconds$ 

我们可以把 `min-slaves-to-write` 和 `min-slaves-max-lag` 这两个配置项搭配起来使用，分别给它们设置一定的阈值，假设为 N 和 T。这两个配置项组合后的要求是，主库连接的从库中至少有 N 个从库，和主库进行数据复制时的 ACK 消息延迟不能超过 T 秒，否则，主库就不会再接收客户端的请求了。

即使原主库是假故障，它在假故障期间也无法响应哨兵心跳，也不能和从库进行同步，自然也就无法和从库进行 ACK 确认了。这样一来，`min-slaves-to-write` 和 `min-slaves-max-lag` 的组合要求就无法得到满足，原主库就会被限制接收客户端请求，客户端也就不能在原主库中写入新数据了。等到新主库上线时，就只有新主库能接收和处理客户端请求，此时，新写的数据会被直接写到新主库中。而原主库会被哨兵降为从库，即使它的数据被清空了，也不会有新数据丢失。

脑裂产生问题的本质原因是，**Redis 主从集群内部没有通过共识算法，来维护多个节点数据的强一致性**。它不像 Zookeeper 那样，每次写请求必须大多数节点写成功后才认为成功。当脑裂发生时，Zookeeper 主节点被孤立，此时无法写入大多数节点，写请求会直接返回失败，因此它可以保证集群数据的一致性。Redis 是出于高可用的目的放弃一致性的，[[分布式大纲#2 2 分布式事务缺陷之 CAP 定理|CAP]] 三者放弃了 C（一致性）。

## 4.2. 哨兵机制

基于主从模式，我们可以将数据分散到多个节点。主节点可以接收读写请求，从节点接收读请求。但是当主从之间出问题时该如何处理呢？从节点挂了影响尚小，可要是主节点挂了呢？我们根据什么判断主节点挂了，主节点挂了以后将哪个从库推选为主节点，如何将新的主节点通知给其他从库呢？

### 4.2.1. 哨兵机制的概念

哨兵是一个处于特殊模式的 Redis 进程，它负责以下三个任务：

1. 监控：监控主库运行状态，并判断主库是否客观下线

	1. 哨兵会周期性的给所有主从库发送 ping 命令以检测服务是否在线。如果检测失败，就会被标记下线。若主库被标记下线，启动自动选举主库流程。
	
	2. 对于从库，哨兵对从库的 ping 超时未回复就可以将其标记为下线。
	
	3. 对于主库，若某个哨兵 ping 超时则将其标记为主观下线。需要多个哨兵都认为下线了才可将其标记成客观下线，此时主库真的会下线，开始执行选主流程。
	
	简单来说，“客观下线”的标准就是，当有 N 个哨兵实例时，最好要有 $\frac{N}{2} + 1$ 个实例判断主库为“主观下线”，才能最终判定主库为“客观下线”。这样一来，就可以减少误判的概率，也能避免误判带来的无谓的主从库切换。

1. 选主：在主库客观下线后，选取新主库

	主节点选择顺序如下：
	
	1. 筛选出不满足条件的节点，如除了检查从库的当前在线状态，还要判断它之前的网络连接状态。若一个节点过去频繁超时掉线，这个节点被认为不可靠。例如配置 $down\mbox{-}after\mbox{-}milliseconds \times 10$ 控制超时时间与次数。
	
	2. 哨兵将从剩余的从节点中挑选出一个新的主节点。选主时会综合判断从节点的状态，哨兵将会从多个维度对从节点进行打分操作，选择一个分高的从节点作为新的主节点。

	打分操作如下：
	
	1. 第一轮会从优先级判断，若手动给从库设置了高优先级，则高优先级优先。
	
	4. 优先级相同对按照与旧主库的数据同步进度，同步进度快的优先。哨兵监视期间，从库会使用 `info` 命令将自身信息同步给哨兵，其中就包括 `offset` 信息。
	
	5. 进度相同的情况下按照从库 ID 号优先。

1. 通知：选出新主库后，通知从库和客户端

	1. 哨兵将新的主节点通知给其他从节点，让从节点与新的主节点开始数据同步。
	
	2. 哨兵会把新主库的地址写入自己实例的 `pubsub（switch-master）` 中。客户端需要订阅这个 `pubsub`，当这个 `pubsub` 有数据时，客户端就能感知到主库发生变更，同时可以拿到最新的主库地址，然后把写请求写到这个新主库即可，这种机制属于哨兵主动通知客户端。
	
	3. 客户端需要访问主从库时，不能直接写死主从库的地址，需要从哨兵集群中获取最新的地址（`sentinel get-master-addr-by-name` 命令）。这样当实例异常时，哨兵切换后或者客户端断开重连，都可以从哨兵集群中拿到最新的实例地址。

### 4.2.2. 哨兵集群

我们可以通过哨兵来进行主从数据库的监控选举与通知，但是如果哨兵节点挂机了要怎么办呢？因此我们需要部署多个哨兵构建哨兵集群来保证哨兵的高可用。

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220518115624.png)
![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220518115632.png)

哨兵集群的原理：

1. 哨兵通过 Redis 的发布订阅机制来获取 Redis 主库信息以及和其他哨兵建立联系。

	1. 哨兵首先与主库建立连接，监控主库信息。
	
	2. 除了自身以外，哨兵还会通过发布订阅机制从主库获取到其他哨兵节点构建哨兵集群。
	
	3. 所有哨兵通过主库的 `__sentinel__:hello` 频道来互相发现与通信。
	
2. 哨兵连接到主库后，就可以获取到主库信息和其他哨兵信息了。对于从库，哨兵会通过 `info` 命令从主库获取到所有的从库，从而再次和从库建立连接进行监听。

3. 除了从库外，哨兵还需要和客户端建立连接，以通知客户端主库切换信息，以及客户端关心的其他信息。如通过 `+switch-master` 命令得知主库地址切换，`+sdown` 实例进入主观下线状态等。

	![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220518120328.png)

### 4.2.3. 哨兵选主

当主库被标记为客观下线后，就会开始进行主库的切换。那么由哪个哨兵来进行这个切换的过程呢？这就需要哨兵选举出一个 leader，由 leader 来进行切换操作：

1. 首先需要将主库标记为客观下线状态，这需要哨兵集群中大多数哨兵达成共识。

	任何一个哨兵只要自身判断主库“主观下线”后，就会给其他实例发送 `is-master-down-by-addr` 命令。接着，其他实例会根据自己和主库的连接情况，做出 Y 或 N 的响应，Y 相当于赞成票，N 相当于反对票。一个哨兵获得了仲裁所需的赞成票数 `quorum` 后，就可以标记主库为“客观下线”。
	
	这里就要求所有的哨兵将主库的 `down-after-milliseconds` 参数设置为一致，否则就会出现下线行为认为不一致的现象。
	
2. 主库被标记为下线状态后，哨兵集群需要选举出一个 leader 来进行切换操作。称为 leader 的如下：

	1. 拿到半数以上的赞成票。**这意味着若有超过半数以上哨兵节点宕机，则选举操作无法进行。**所以我们要求哨兵节点数量至少为 3，否则若一台哨兵节点宕机，则无法进行选举。当然哨兵节点数量也不要过多，否则会造成节点选举耗时时间长。
	
	2. 拿到的票数同时还需要大于等于哨兵配置文件中的 quorum 值。
	
3. 选举出 leader 之后，由 leader 完成主库的切换操作。

哨兵 leader 的选举过程：

1. 当哨兵节点判断主库下线后，会向其他哨兵节点发送投票通知让其他节点投票给自己。同时该哨兵也会给自己投一票。

2. 若一个哨兵节点接收到投票通知后，自身还未投过票，则同意该通知，返回 Y。否则若已经给自己和其他节点投过票，返回 N。

3. 投票完成后若有节点获得超过半数以上赞成票且票数大于 quorum 值，则其称为 leader，否则将会再次选举。选举周期为哨兵故障转移超时时间。

## 4.3. Redis 的持久化方式

### 4.3.1. AOF (Append-only-file)

#### 4.3.1.1. AOF 原理

- 原理：将所有命令以 Redis 命令请求协议存储，保存为 AOF 文件。命令保存时机由以下参数配置：

	- `Always` ：同步写回。每个写命令执行完，立马同步地将日志写回磁盘。崩溃时会丢失一次事件循环的命令。在同步时下一次事件循环才会把上一次的事件产生的缓冲 fsync 同步到磁盘中。
	
	- `Everysec` ：每秒写回。每个写命令执行完，只是先把日志写到 AOF 文件的内存缓冲区，每隔一秒把缓冲区中的内容写入磁盘。崩溃时会丢失一秒内的命令。
	
	- `No` ：操作系统控制的写回。每个写命令执行完，只是先把日志写到 AOF 文件的内存缓冲区，由操作系统决定何时将缓冲区内容写回磁盘。崩溃时会丢失不可控数据的命令。

- 优点

	- 数据安全，一次操作就可以备份一次
	
	- 通过 append 模式，即使中途宕机也可以回复数据
	
	- 有 rewrite 模式，当 aof 文件过大时可以进行命令合并
	
- 缺点

	- AOF 文件偏大，恢复慢
	
	- 数据集大时，比 RDB 启动效率低

#### 4.3.1.2. AOF 重写

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220517181251.png)

当操作命令积累时，AOF 文件会变大。AOF 重写支持以当前数据库的所有数据，对其生成每一条 set 命令，从而产生一个全新的 AOF 文件。重写机制具有“多变一”功能，该 AOF 文件理论上会比原始文件小许多，这个操作称为 AOF 重写。

每次 AOF 重写时，Redis 会先执行一个内存拷贝，用于重写。然后，使用两个日志保证在重写过程中，新写入的数据不会丢失。而且 Redis 采用额外的线程进行数据重写，所以，这个过程并不会阻塞主线程。

但是 AOF 重写依然在某些情况下存在阻塞主线程的风险：

1. fork 这个瞬间一定是会阻塞主线程的。

	fork 采用操作系统提供的写时复制 (`Copy On Write`) 机制，就是为了避免一次性拷贝大量内存数据给子进程造成的长时间阻塞问题。但 fork 子进程需要拷贝进程必要的数据结构，其中有一项就是拷贝内存页表（虚拟内存和物理内存的映射索引表），这个拷贝过程会消耗大量 CPU 资源，拷贝完成之前整个进程是会阻塞的，阻塞时间取决于整个实例的内存大小，实例越大，内存页表越大，fork 阻塞时间越久。

2. 拷贝内存页表完成后，父进程也可能会产生阻塞的风险。

	子进程与父进程指向相同的内存地址空间，也就是说此时虽然产生了子进程，但是并没有申请与父进程相同的内存大小。“写实复制”是在写发生时，才真正拷贝内存真正的数据，这个过程中：

	1. fork 出的子进程指向与父进程相同的内存地址空间，此时子进程就可以执行 AOF 重写，把内存中的所有数据写入到 AOF 文件中。但是此时父进程依旧是会有流量写入的，如果父进程操作的是一个已经存在的 key，那么这个时候父进程就会真正拷贝这个 key 对应的内存数据，申请新的内存空间。这样逐渐地父子进程内存数据开始分离，父子进程逐渐拥有各自独立的内存空间。
	
	2. 因为内存分配是以页为单位进行分配的，默认 4k。如果父进程此时操作的是一个 bigkey，重新申请大块内存耗时会变长，可能会产阻塞风险。另外，如果操作系统开启了内存大页机制 (`Huge Page`，页面大小 2M)，那么父进程申请内存时阻塞的概率将会大大提高，所以在**Redis 机器上需要关闭 Huge Page 机制**。Redis 每次 fork 生成 RDB 或 AOF 重写完成后，都可以在 Redis log 中看到父进程重新申请了多大的内存空间。

AOF 重写的配置：

- `auto-aof-rewrite-min-size 64mb` 配置 AOF 文件运行时最大容量。

- `auto-aof-rewrite-percentage 100` 配置 AOF 文件运行时本次 AOF 文件与上一次 AOF 文件体积增量比。

AOF 重写触发时机：

- 在 aof 文件体量超过 `auto-aof-rewrite-min-size`，且比上次重写后的体量增加了 `auto-aof-rewrite-percentage` 时自动触发重写。

- 可以手动发送 `bgrewriteaof` 指令触发一次重写。

`Redis4.0` 以后，Redis 的 AOF 重写的时候就直接把 RDB 的内容写到 AOF 文件开头，将增量改动的以指令的方式 Append 到 AOF，这样做的好处是可以结合 RDB 和 AOF 的优点, 快速加载同时避免丢失过多的数据。缺点是 AOF 里面的 RDB 部分就是压缩格式不再是 AOF 格式，可读性较差。

### 4.3.2. RDB (Redis DataBase Back File)

#### 4.3.2.1. RDB 原理

![](https://static001.geekbang.org/resource/image/a2/58/a2e5a3571e200cb771ed8a1cd14d5558.jpg)

原理：定期生成所有数据的快照，依赖快照恢复。采用写时复制技术实现，不会阻塞 Redis 读写操作。

- 优点

	- 只有一个 `dump.rdb` 文件，方便持久化。
	
	- 单个文件方便存储。
	
	- 性能最大化，fork 子线程备份，主线程不阻塞，IO 最大化.
	
	- 数据集大时，比 AOF 启动效率高。
	
- 缺点

	- 间隔时间太长，容易丢失数据。间隔事件太短，对磁盘和 CPU 的抢占率高，容易阻塞主线程。
	
	- 在写请求高于读请求，且写请求分散在大量数据的情况下，RDB 进行的时候会发生以下风险：
	
		- 内存资源风险，大量写请求会导致主线程进行内存复制与分离，这时内存使用率会上升。当到达机器容量时，若未配置 swap 策略则会出现 OOM 现象，若配置了会导致内存热点数据与磁盘的交换，导致性能下降。
		
		- CPU 资源风险：在 CPU 核心少时，主线程已占用了一个线程。fork 的子线程会再占用一个且消耗资源较高。同时还存在刷盘，异步关闭文件符这些操作的线程，以及系统内其他的线程。这些线程的争用导致性能下降。

### 4.3.3. AOF 与 RDB 混合模式

`Redis4.0` 引入了混合模式，支持 RDB 与 AOF 同时使用。新的 AOF 文件前半段是 RDB，后半段是增量的 AOF。采用混合模式既可以利用 RDB 模式备份文件小和全的特性，又可以利用 AOF 一致性高的特性。

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220517194638.png)

# 5. 高扩展

## 5.1. Redis Cluster 集群

基于主从，我们可以将节点划分为读写节点以此承担压力，并且利用主从维持了节点之间数据的一致性。之后可以利用哨兵，对主从进行监控，从而可以使节点发生故障时可以切换。但是无论是主从还是哨兵，这里所针对的情况都是数据在一台节点完全存储的，如果单台节点容量不够了呢？

对于容量负载，我们有两种方式，纵向扩展与横向扩展。纵向扩展即升配机器，横向扩展即增加机器。这二者对比如下：

- 纵向扩展

	- 实施起来简单、直接。
	
	- 数据量增加，需要的内存也会增加，主线程 fork 子进程时就可能会阻塞。
	
	- 会受到硬件和成本的限制
	
	- 数据存在哪儿，客户端访问哪儿，对外访问方式简单。
	
- 横向扩展

	- 增减机器比升配更容易做到，可以利用多台一般的机器构建一个高可靠的集群。
	
	- 单台机器容量小，fork 线程压力小。
	
	- 需要管理多台机器的数据，对于请求分发与数据存储需要做合理划分。
	
	- 客户端可能需要和多台机器打交道，多机器集群管理问题。

### 5.1.1. Redis Cluster 集群的构建

#### 5.1.1.1. 初始哈希槽的划分

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220518160601.png)

在 Redis Cluster 方案中，一个切片集群共有 [[数据密集型系统设计2：复制与分区#2.3.1.1. 固定数量分区| 16384 个哈希槽]]，这些哈希槽类似于数据分区，每个键值对都会根据它的 key，被映射到一个哈希槽中。映射的规则如下：

1. 首先根据键值对的 key，按照 `CRC16` 算法计算一个 16 bit 的值。

2. 用这个 16bit 值对 16384 取模，得到 0~16383 范围内的模数，每个模数代表一个相应编号的哈希槽。

将数据划分到哈希槽以后，接下来就是需要将槽分配给 Redis 实例了。有以下几种方式：

- 部署 Redis Cluster 方案时，可以使用 `cluster create` 命令创建集群，此时，Redis 会自动把这些槽平均分布在集群实例上。

- 使用 `cluster meet ` 命令手动建立实例间的连接，形成集群，再使用 `cluster addslots` 命令，指定每个实例上的哈希槽个数，按照机器配置或者优先级来手动分配。在手动分配哈希槽时，需要把 16384 个槽都分配完，否则 Redis 集群无法正常工作。

当哈希槽分配完毕后，每个实例会负责一部分哈希槽。这些集群节点之间还会互相通知自己所负责的哈希槽，这样集群中所有节点都会知道槽的映射关系。

#### 5.1.1.2. 哈希槽的重建

随着数据的的分布不均或者 Redis 实例的增减，出于负载均衡策略就需要将哈希槽需要重新划分，将原属于一个实例的哈希槽分配给其他实例。

数据迁移过程是同步的，迁移一个 key 会同时阻塞源节点和目标节点。

#### 5.1.1.3. 利用哈希槽的原因

1. Redis Cluster 集群采取去中心化的思想，客户端与服务端直连，如果这个访问的 key 不在这个节点上，需要**服务端有纠错的能力。即节点保存了完整的映射关系，发现错误请求进行 `MOVED` 响应处理**。对于其他集群的实现，如 Codis 的中心化模式，有 proxy 层进行请求分发与节点管理，这要求 proxy 层也要做到高可用。

2. 增加一层哈希槽，可以把**数据和节点解耦**，key 通过 Hash 计算，只需要关心映射到了哪个哈希槽，然后再通过哈希槽和节点的映射表找到节点。相当于消耗了很少的 CPU 资源，不但让数据分布更均匀，还可以让这个映射表变得很小，利于客户端和服务端保存，节点之间交换信息时也变得轻量。

3. 当集群在扩容、缩容、数据均衡时，节点之间的操作例如数据迁移，都以哈希槽为基本单位进行操作，简化了节点扩容、缩容的难度，便于集群的维护和管理。

Redis 哈希槽的概念和 `一致性 hash` 很像，但它们的侧重点不同：

1. `一致性 hash` 通常用来解决当集群节点数变化时，大量缓存无法命中的情况。因为 hash 计算方式为 $hashcode\div节点数$，节点数的变化使得大部分计算结果都发生了变化。一致性 hash 可以保证同一个 key 在节点数变化的时候，大部分 $hashcode\div节点数$ 仍然能计算出相同结果。

2. 哈希槽的设计也可以保证这一点：只要把那个节点对应的槽的数据做迁移即可，其余数据不用移动。甚至可以做细粒度的控制，比如机器性能不一样，有的机器可以分配多几个槽。

3. 使用哈希槽避免了从 hash 环上寻找实例节点，中间可能还有从虚拟节点到实际节点的过渡。

4. 使用哈希槽灵活性更强，如将某个实例上的热点槽迁移到其他机器上去，其他槽数据保持不动。

### 5.1.2. Redis Cluster 集群的使用

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220518162735.png)

集群划分好以后就涉及到客户端的使用了，那么客户端如何定位到 Redis 实例呢？这其实是一个[[数据密集系统设计/数据密集型系统设计2：复制与分区#2 4 请求路由|请求路由]]的问题，客户端通过与 Redis 集群建立连接后，可以知道 Redis 集群中所有哈希槽的映射关系，在操作数据时，按以下步骤进行：

1. 根据操作 key 计算对应的哈希槽。

2. 根据映射信息请求哈希槽所在的实例。对请求到的实例可能出现以下几种情况：

	- 当前实例正常，直接执行命令。
	
	- 当前 key 所属哈希槽已经不归属于此实例，返回 `MOVED` 命令响应结果，结果中就包含新实例的访问地址。客户端更新哈希槽映射关系并请求新的实例信息。
	
	- 当前哈希槽正在迁移，返回 `ASK` 命令。客户端询问新实例该 key 的情况决定下一步操作。`ASK` 命令表示两层含义：
	
		- 第一，表明 `Slot` 数据还在迁移中。
		
		- 第二，`ASK` 命令把客户端所请求数据的最新实例地址返回给客户端，此时，客户端需要给实例 3 发送 `ASKING` 命令，然后再发送操作命令。
		
		- 和 `MOVED` 命令不同，`ASK` 命令并不会更新客户端缓存的哈希槽分配信息。这也就是说，`ASK` 命令的作用只是**让客户端能给新实例发送一次请求**，而不像 `MOVED` 命令那样，会更改本地缓存，让后续所有命令都发往新实例。 

### 5.1.3. 集群的通信

Redis Cluster 在运行时，每个实例上都会保存 `Slot` 和实例的对应关系（也就是 `Slot 映射表`），以及自身的状态信息。为了让集群中的每个实例都知道其它所有实例的状态信息，实例之间会按照一定的规则进行通信。这个规则就是 `Gossip 协议`。

`Gossip 协议` 的工作原理可以概括成两点：

1. 每个实例之间会按照一定的频率，从集群中随机挑选一些实例，把 PING 消息发送给挑选出来的实例，用来检测这些实例是否在线，并交换彼此的状态信息。PING 消息中封装了发送消息的实例自身的状态信息、部分其它实例的状态信息，以及 `Slot 映射表`。

2. 一个实例在接收到 PING 消息后，会给发送 PING 消息的实例，发送一个 PONG 消息。PONG 消息包含的内容和 PING 消息一样。

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220521235556.png)

`Gossip 协议` 可以保证在一段时间后，集群中的每一个实例都能获得其它所有实例的状态信息。这样一来，即使有新节点加入、节点故障、Slot 变更等事件发生，实例间也可以通过 PING、PONG 消息的传递，完成集群状态在每个实例上的同步。

#### 5.1.3.1. 集群通信的频率

Redis Cluster 的实例启动后，默认会每秒从本地的实例列表中随机选出 5 个实例，再从这 5 个实例中找出一个最久没有通信的实例，把 PING 消息发送给该实例。这是实例周期性发送 PING 消息的基本做法。但是，这里有一个问题：实例选出来的这个最久没有通信的实例，毕竟是从随机选出的 5 个实例中挑选的，这并不能保证这个实例就一定是整个集群中最久没有通信的实例。所以，这有可能会出现，有些实例一直没有被发送 PING 消息，导致它们维护的集群状态已经过期了。

为了避免这种情况，Redis Cluster 的实例会按照每 100ms 一次的频率，扫描本地的实例列表，如果发现有实例最近一次接收 PONG 消息的时间，已经大于配置项 `cluster-node-timeout` （故障的心跳超时时间）的一半了，就会立刻给该实例发送 PING 消息，更新这个实例上的集群状态信息。

当集群规模扩大之后，因为网络拥塞或是不同服务器间的流量竞争，会导致实例间的网络通信延迟增加。如果有部分实例无法收到其它实例发送的 PONG 消息，就会引起实例之间频繁地发送 PING 消息，这又会对集群网络通信带来额外的开销了。

所以，为了避免过多的心跳消息挤占集群带宽，我们可以调大 `cluster-node-timeout` 值，比如说调大到 20 秒或 25 秒。这样一来， PONG 消息接收超时的情况就会有所缓解，单实例也不用频繁地每秒执行 10 次心跳发送操作了。

当然，我们也不要把 `cluster-node-timeout` 调得太大，否则，如果实例真的发生了故障，我们就需要等待 `cluster-node-timeout` 时长后，才能检测出这个故障，这又会导致实际的故障恢复时间被延长，会影响到集群服务的正常使用。为了验证调整 `cluster-node-timeout` 值后，是否能减少心跳消息占用的集群网络带宽，我给你提个小建议：你可以在调整 `cluster-node-timeout` 值的前后，使用 tcpdump 命令抓取实例发送心跳信息网络包的情况。

## 5.2. Codis 集群

#### 5.2.1. Codis 的架构

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220521224427.png)
![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220521224738.png)
![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220521224718.png)

Codis 集群中包含了 4 类关键组件：

- codis server：这是进行了二次开发的 Redis 实例，其中增加了额外的数据结构，支持数据迁移操作，主要负责处理具体的数据读写请求。

	Codis 集群一共有 1024 个 Slot，编号依次是 0 到 1023。可以把这些 Slot 手动分配给 codis server，每个 server 上包含一部分 Slot。也可以让 codis dashboard 进行自动分配，例如，dashboard 把 1024 个 Slot 在所有 server 上均分。
	
	当客户端要读写数据时，会使用 CRC32 算法计算数据 key 的哈希值，并把这个哈希值对 1024 取模。而取模后的值，则对应 Slot 的编号。此时，根据第一步分配的 Slot 和 server 对应关系，就可以知道数据保存在哪个 server 上了。
	
- codis proxy：接收客户端请求，实现[[数据密集系统设计/数据密集型系统设计2：复制与分区#2 4 请求路由|请求路由]]把请求转发给 codis server。

- Zookeeper 集群：保存集群元数据，例如数据位置信息和 codis proxy 信息。

	- Slot 和 codis server 的映射关系称为数据路由表（简称路由表）。我们在 codis dashboard 上分配好路由表后，dashboard 会把路由表发送给 codis proxy，同时，dashboard 也会把路由表保存在 Zookeeper 中。
	
	- codis-proxy 会把路由表缓存在本地，当它接收到客户端请求后，直接查询本地的路由表，就可以完成正确的请求转发了。
	
- codis dashboard 和 codis fe：共同组成了集群管理工具。其中，codis dashboard 负责执行集群管理工作，包括增删 codis server、codis proxy 和进行数据迁移。而 codis fe 负责提供 dashboard 的 Web 操作界面，便于我们直接在 Web 界面上进行集群管理。

#### 5.2.2. Coid 的数据迁移

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220521224840.png)

增加 codis server，这个过程主要涉及到两步操作：

1. 启动新的 codis server，将它加入集群。

2. 把部分数据迁移到新的 server。

Codis 集群按照 Slot 的粒度进行数据迁移：

1. 在源 server 上，Codis 从要迁移的 Slot 中随机选择一个数据，发送给目的 server。

2. 目的 server 确认收到数据后，会给源 server 返回确认消息。这时，源 server 会在本地将刚才迁移的数据删除。

	当源 server 把数据发送给目的 server 后，就可以处理其他请求操作了，不用等到目的 server 的命令执行完。而目的 server 会在收到数据并反序列化保存到本地后，给源 server 发送一个 ACK 消息，表明迁移完成。此时，源 server 在本地把刚才迁移的数据删除。

第一步和第二步就是单个数据的迁移过程。Codis 会不断重复这个迁移过程，直到要迁移的 Slot 中的数据全部迁移完成。

这个过程中，迁移的数据会被设置为只读，所以，源 server 上的数据不会被修改，自然也就不会出现“和目的 server 上的数据不一致”的问题了。

对于 bigkey，异步迁移采用了拆分指令的方式进行迁移。具体来说就是，对 bigkey 中每个元素，用一条指令进行迁移，而不是把整个 bigkey 进行序列化后再整体传输。

#### 5.2.3. Codis 的高可用

codis server 其实就是 Redis 实例，只不过增加了和集群操作相关的命令。Redis 的主从复制机制和哨兵机制在 codis server 上都是可以使用的，所以，Codis 就使用主从集群来保证 codis server 的可靠性。简单来说就是，Codis 给每个 server 配置从库，并使用哨兵机制进行监控，当发生故障时，主从库可以进行切换，从而保证了 server 的可靠性。

Codis 集群中，客户端是和 codis proxy 直接连接的，所以，当客户端增加时，一个 proxy 无法支撑大量的请求操作，此时，我们就需要增加 proxy。增加 proxy 比较容易，我们直接启动 proxy，再通过 codis dashboard 把 proxy 加入集群就行。

此时，codis proxy 的访问连接信息都会保存在 Zookeeper 上。所以，当新增了 proxy 后，Zookeeper 上会有最新的访问列表，客户端也就可以从 Zookeeper 上读取 proxy 访问列表，把请求发送给新增的 proxy。这样一来，客户端的访问压力就可以在多个 proxy 上分担处理了。

proxy 上的信息源头都是来自 Zookeeper（例如路由表）。而 Zookeeper 集群使用多个实例来保存数据，只要有超过半数的 Zookeeper 实例可以正常工作， Zookeeper 集群就可以提供服务，也可以保证这些数据的可靠性。所以，codis proxy 使用 Zookeeper 集群保存路由表，可以充分利用 Zookeeper 的高可靠性保证来确保 codis proxy 的可靠性，不用再做额外的工作了。当 codis proxy 发生故障后，直接重启 proxy 就行。重启后的 proxy，可以通过 codis dashboard 从 Zookeeper 集群上获取路由表，然后，就可以接收客户端请求进行转发了。这样的设计，也降低了 Codis 集群本身的开发复杂度。

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220521225253.png)
![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220521225301.png)

#### 5.2.4. Redis Cluster 与 Codis 的对比

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220521225421.png)

- 从稳定性和成熟度来看，Codis 应用得比较早，在业界已经有了成熟的生产部署。虽然 Codis 引入了 proxy 和 Zookeeper，增加了集群复杂度，但是，proxy 的无状态设计和 Zookeeper 自身的稳定性，也给 Codis 的稳定使用提供了保证。而 Redis Cluster 的推出时间晚于 Codis，相对来说，成熟度要弱于 Codis，如果你想选择一个成熟稳定的方案，Codis 更加合适些。

- 从业务应用客户端兼容性来看，连接单实例的客户端可以直接连接 codis proxy，而原本连接单实例的客户端要想连接 Redis Cluster 的话，就需要开发新功能。所以，如果你的业务应用中大量使用了单实例的客户端，而现在想应用切片集群的话，建议你选择 Codis，这样可以避免修改业务应用中的客户端。

- 从使用 Redis 新命令和新特性来看，Codis server 是基于开源的 Redis 3.2.8 开发的，所以，Codis 并不支持 Redis 后续的开源版本中的新增命令和数据类型。另外，Codis 并没有实现开源 Redis 版本的所有命令，比如 `BITOP`、`BLPOP`、`BRPOP`，以及和与事务相关的 `MUTLI`、`EXEC` 等命令。Codis 官网上列出了不被支持的命令列表，你在使用时记得去核查一下。所以，如果你想使用开源 Redis 版本的新特性，Redis Cluster 是一个合适的选择。

- 从数据迁移性能维度来看，Codis 能支持异步迁移，异步迁移对集群处理正常请求的性能影响要比使用同步迁移的小。所以，如果你在应用集群时，数据迁移比较频繁的话，Codis 是个更合适的选择。

- `mget` 这种批量命令，codis 的 proxy 会多线程执行多个查询操作，而 redis-cluster 对这种 mget 操作是单线程顺序执行，性能上不如 codis。

## 5.3. 集群优化策略

- Cluster

	- 所有节点互相连接
	
	- 集群消息通过集群总线通信
	
	- 节点与节点通过二进制协议通信
	
	- 客户端与集群节点正常文本协议通信
	
	- 集群节点不代理查询
	
	- 数据按 Slot 存储在多个 Redis 实例上。Redis 集群内置 16384 个哈希槽，将 key 按 CRC16 算法计算后对 16484 取余。
	
	- 集群节点宕机时自动故障转移
	
	- 可以平滑扩/缩容
	
- 集群优化策略

	- Master 不做持久化工作
	
	- 数据交给 Slave 开启持久化备份
	
	- Master 和 Slave 保持在同一个局域网
	
	- 避免在压力大的主库上增加从库
	
	- 主从复制避免图结构，使用单项链表保持节点切换简单

# 6. 运行原理

## 6.1. 文件事件模型

- 处理器结构

	- 多个 Socket
	
	- IO 多路复用程序
	
	- 文件事件分派器
	
	- 事件处理器
	
		- 连接应答处理器
		
		- 命令请求处理器
		
		- 命令回复处理器
		
- 处理器流程

	1. 客户端 Socket 与 Redis 的 Server Socket 请求建立连接
	
	2. Server Socket 产生 `AE_READABLE` 事件，事件压入队列中
	
	3. 文件事件分派器从队列中获取事件，交给连接处理器
	
	4. 连接处理器建立通信，将 `AE_READABLE` 事件与命令处理器关联
	
	5. 客户端发起操作命令，产生 `AE_READABLE` 事件压入队列。事件分派器将事件交给命令处理器。
	
	6. 命令处理器处理事件，然后将 socket 的 `AE_WRITABLE` 事件与回复处理器关联。
	
	7. 客户端准备好接收结果时，产生 `AE_WRITABLE` 事件压入队列，命令回复器回复结果。
	
	8. 最后操作完成，解除命令回复器与 `AE_WRITABLE` 事件的关联。

## 6.2. Redis 的内存淘汰策略

### 6.2.1. Redis 提供的淘汰策略

- `volatile-lru` ：从已设置过期时间的数据集中挑选最近最少使用的数据。这是 3.0 版本以前默认的策略。

- `volatile-lfu` ：从已设置过期时间的数据集中挑选最不经常使用的数据。

- `volatile-ttl` ：从已设置过期时间的数据集中挑选即将过期的数据。

- `volatile-random` ：从已设置过期时间的数据集中任意挑选数据。

- `allkeys-lru` ：从所有键中挑选最近最少使用的数据。

- `allkeys-random` ：从所有键中中任意挑选数据。

- `no-enviction` ：禁止淘汰数据。新写入操作会报错。这是 3.00 版本后默认的策略。

基于 volatile 策略时若没有键设置了超时时间，那么表现效果和 allkeys 效果一致。

### 6.2.2. Redis-LRU 算法的实现

LRU 算法在实际实现时，需要用链表管理所有的缓存数据，这会带来额外的空间开销。而且当有数据被访问时，需要在链表上把该数据移动到前面，如果有大量数据被访问，就会带来很多链表移动操作，会很耗时，进而会降低 Redis 缓存性能。

所以，在 Redis 中 LRU 算法被做了简化，以减轻数据淘汰对缓存性能的影响。具体来说，Redis 默认会记录每个数据的最近一次访问的时间戳（由键值对数据结构 RedisObject 中的 `lru 字段` 记录）。

然后，Redis 在决定淘汰的数据时，第一次会随机选出 N 个数据，把它们作为一个候选集合。接下来，Redis 会比较这 N 个数据的 `lru 字段`，把 `lru 字段` 值最小的数据从缓存中淘汰出去。

可以通过 `maxmemory-samples` 设置淘汰的候选数据集个数。当需要再次淘汰数据时，Redis 需要挑选数据进入第一次淘汰时创建的候选集合。这儿的挑选标准是：能进入候选集合的数据的 `lru 字段` 值必须小于候选集合中最小的 `lru 字段` 值。当有新数据进入候选数据集后，如果候选数据集中的数据个数达到了 `maxmemory-samples`，Redis 就把候选数据集中 `lru 字段` 值最小的数据淘汰出去。

### 6.2.3. Redis-LFU 算法的实现

Redis 在实现 LRU 策略时使用了两个近似方法：

- Redis 是用 RedisObject 结构来保存数据的，RedisObject 结构中设置了一个 `lru 字段`，用来记录数据的访问时间戳。

- Redis 并没有为所有的数据维护一个全局的链表，而是通过随机采样方式，选取一定数量（例如 10 个）的数据放入候选集合，后续在候选集合中根据 ` lru 字段` 值的大小进行筛选。

在此基础上，Redis 在实现 LFU 策略的时候，只是把原来 24bit 大小的 `lru 字段` 又进一步拆分成了两部分。

- `ldt` 值：`lru 字段` 的前 16bit，表示数据的访问时间戳，这意味着精确度只能到分钟级别。

- `counter` 值：`lru 字段` 的后 8bit，表示数据的访问次数。

总结一下：当 LFU 策略筛选数据时，Redis 会在候选集合中，根据数据 lru 字段的后 8bit 选择访问次数最少的数据进行淘汰。当访问次数相同时，再根据 lru 字段的前 16bit 值大小，选择访问时间最久远的数据进行淘汰。

对于访问次数，Redis 并未采用线性递增的方式。counter 字段 8 位最多表示 255，采用线性递增方式很快就用完了。

LFU 策略实现的计数规则是：每当数据被访问一次时：

1. 首先，用计数器当前的值乘以配置项 `lfu_log_factor` 再加 1，再取其倒数，得到一个 p 值，即 $\frac{1}{baseval \times server.lfu\_log\_factor + 1}$。

2. 然后，把这个 p 值和一个取值范围在（0，1）间的随机数 r 值比大小，只有 p 值大于 r 值时，计数器才加 1。

3. 当增长次数越大时，递增概率越低。

4. `lfu_log_factor` 设置越大，递增概率越低。

```c

double r = (double)rand()/RAND_MAX;
...
double p = 1.0/(baseval*server.lfu_log_factor+1);
if (r < p) counter++;   

```

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220520232722.png)

使用了非线性递增的计数器方法，即使缓存数据的访问次数成千上万，LFU 策略也可以有效地区分不同的访问次数，从而进行合理的数据筛选。

Redis 在实现 LFU 策略时，还设计了一个 `counter` 值的衰减机制：

1. LFU 策略使用衰减因子配置项 `lfu_decay_time` 来控制访问次数的衰减。LFU 策略会计算当前时间和数据最近一次访问时间的差值，并把这个差值换算成以分钟为单位。

2. LFU 策略再把这个差值除以 `lfu_decay_time` 值，所得的结果就是数据 counter 要衰减的值。

3. 因为采用 16 位记录访问时间，这个时间精度只能到分钟级别。所以存在同一分钟内 A 比 B 访问次数多，但是优先因为缓存因子而被淘汰的情况。


### 6.2.4. 淘汰策略选择的方式

- 先根据是否有始终会被频繁访问的数据（例如置顶消息），来选择淘汰数据的候选集，也就是决定是针对所有数据进行淘汰，还是针对设置了过期时间的数据进行淘汰。

- 候选数据集范围选定后，建议优先使用 LRU 算法，也就是，`allkeys-lru` 或 `volatile-lru` 策略。

## 6.3. Redis 的过期删除策略

Redis 提供了以下数据的过期删除策略：

- 定时删除策略：设定定时器，时间到了就删。此种方式资源消耗大。

- 定期删除策略：每隔一定时间扫描删除。不是扫描所有 key，而是随机抽一部分。

- 惰性删除：操作 key 时如果发现过期了就删除。

在主从模式下，从库不会自动删除过期的数据，原因考虑有以下几点：

- 主从同步存在延迟，若从库将数据给删了，那么主库延迟到达的续期命令就不会生效了。

- 如果发生了时钟跳跃，主从之间就会出现删除时机不一致的现象。因此将删除操作放在主库同步给从库是合理的。

- 若从库 `slave-read-only` 设置为 no，在 `Redis4.0` 以前就算在从库给 key 设置了过期时间，那么从库也不会在过期后删除该数据。在 4.0 以后从库才会删除那些直接在自己库上设置生效时间的数据。

## 6.4. Redis 的事务

- Redis 事务的特性

	- 事务失败时不支持回滚
	
	- 一个事务中出现运行错误，其余的命令会继续执行
	
- Redis 事务的使用

	- `MULTI` 开启一个事务：命令入队时就报错，会放弃事务执行，保证原子性。
	
	- `EXEC` 执行一个事务：命令入队时没报错，实际执行时报错，不保证原子性。
	
	- `DISCARD` 取消一个事务：用来主动放弃事务执行，把暂存的命令队列清空，起不到回滚的效果。
	
	- `WATCH` 提供 `Check And Set` 的能力，可以监控一个或多个键。一旦某个键被修改/删除，之后的事务就不会执行。
	
	- 可以使用 `pipline` 批量发送命令来减少事务的冲突，只能尽量减少做不到避免。要想完全避免需要使用 ` lua` 脚本执行。

## 6.5. Redis 的 hash 冲突与 hash 扩容

### 6.5.1. Hash 冲突

使用链表保存冲突的数据

### 6.5.2. Hash 扩容

扩容实现：`渐进式 hash`，从第一个索引开始一点一点的转移数据，具体的实现过程如下：

1. 给哈希表 2 分配更大的空间，例如是当前哈希表 1 大小的两倍。

2. 把哈希表 1 中的数据重新映射并拷贝到哈希表 2 中。

	- 拷贝的过程不会是直接全部拷贝，若这么做的话就会阻塞后面的请求。因此 Redis 采取了 `渐进式 rehash`。在拷贝数据时正常处理请求，每处理一个请求就将这个索引上的所有数据拷贝到 hash 表 2 中。等处理下一个请求时，再顺带拷贝哈希表 1 中的下一个索引位置的 entries。
	
	- 当 rehash 开始后，即使没有请求写入，也会定时当迁移数据到新的 hash 桶中。
	
3. 释放哈希表 1 的空间。

扩容时机：Redis 会使用装载因子（load factor）来判断是否需要做 rehash。装载因子的计算方式是：$哈希表中所有 entry 的个数 \div 哈希表的哈希桶个数$。Redis 会根据装载因子的两种情况，来触发 rehash 操作：

- $装载因子 \geq 1$，同时，哈希表被允许进行 rehash。

	- 如果装载因子等于 1，同时我们假设，所有键值对是平均分布在哈希表的各个桶中的，那么，此时，哈希表可以不用链式哈希，因为一个哈希桶正好保存了一个键值对。
	
	- 如果此时再有新的数据写入，哈希表就要使用链式哈希了，这会对查询性能产生影响。**在进行 RDB 生成和 AOF 重写时，哈希表的 rehash 是被禁止的，这是为了避免对 RDB 和 AOF 重写造成影响**。如果此时，Redis 没有在生成 RDB 和重写 AOF，那么，就可以进行 rehash。否则的话，再有数据写入时，哈希表就要开始使用查询较慢的链式哈希了。
	
- $装载因子 \geq 5$，哈希桶里会有大量的链式哈希存在，性能会受到严重影响，此时，就立马开始做 rehash。

## 6.6. Redis 分区

- 方案

	- 代理分区：将请求发送给代理，让代理决定对哪个节点读写。如 Twemporxy
	
	- 客户端分区：由客户端根据一定规则将数据分散到不同节点上
	
	- 查询路由：客户端随机请求一个 Redis 实例，Redis 转发给正确的 Redis 节点。
	
- 缺点

	- 涉及多 key 的操作不能很好的支持，如无法直接对分布到不同节点的 key 做交并集计算
	
	- 同时操作多个 key，无法使用事务
	
	- 分区的粒度是 key，无法使用一个非常长的排序 key 存储一个数据集
	
	- 数据处理会变得复杂，如备份时需要收集多个节点的数据
	
	- 需要处理动态缩扩容的问题

## 6.7. Redis 的缓冲区

### 6.7.1. 客户端输入输出缓冲区

为了避免客户端和服务器端的请求发送和处理速度不匹配，服务器端给每个连接的客户端都设置了一个输入缓冲区和输出缓冲区，我们称之为客户端输入缓冲区和输出缓冲区。

输入缓冲区会先把客户端发送过来的命令暂存起来，Redis 主线程再从输入缓冲区中读取命令，进行处理。当 Redis 主线程处理完数据后，会把结果写入到输出缓冲区，再通过输出缓冲区返回给客户端，如下图所示：

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220519234030.png)

### 6.7.2. 输入输出缓冲区溢出

#### 6.7.2.1. 输入缓冲区溢出

- 写入了 bigkey，比如一下子写入了多个百万级别的集合类型数据。

- 服务器端处理请求的速度过慢，例如，Redis 主线程出现了间歇性阻塞，无法及时处理正常发送的请求，导致客户端发送的请求在缓冲区越积越多。

可以使用 `client list` 命令查看客户端情况，其中有以下几个信息：

- `cmd`，表示客户端最新执行的命令。

- `qbuf`，表示输入缓冲区已经使用的大小。

- `qbuf-free`，表示输入缓冲区尚未使用的大小。

当缓冲区被填满无法接收数据的时候，此时，客户端再写入大量命令的话，就会引起客户端输入缓冲区溢出，Redis 的处理办法就是把客户端连接关闭，结果就是业务程序无法进行数据存取了。

对于客户端的缓冲区，是不支持配置的，为 1GB 的大小。因此我们需要注意客户端写入的速度，防止缓冲区溢出。

#### 6.7.2.2. 输出缓冲区溢出

Redis 为每个客户端设置的输出缓冲区也包括两部分：一部分，是一个大小为 16KB 的固定缓冲空间，用来暂存 OK 响应和出错信息；另一部分，是一个可以动态增加的缓冲空间，用来暂存大小可变的响应结果。

那什么情况下会发生输出缓冲区溢出呢？ 可能有以下几种：

- 服务器端返回 bigkey 的大量结果。

- 执行了 `MONITOR` 命令：`MONITOR` 命令是用来监测 Redis 执行的。执行这个命令之后，就会持续输出监测到的各个命令操作。

- 缓冲区大小设置得不合理：我们可以通过 `client-output-buffer-limit` 配置项，来设置缓冲区的大小。具体设置的内容包括两方面：

	- 设置缓冲区大小的上限阈值。
	
	- 设置输出缓冲区持续写入数据的数量上限阈值，和持续写入数据的时间的上限阈值。
	
	- 对于不同类型的客户端，也可以有不同的配置。如普通使用的客户端，监听频道订阅的客户端，从库客户端。
	
		- 普通客户端来说，它每发送完一个请求，会等到请求结果返回后，再发送下一个请求，这种发送方式称为阻塞式发送。在这种情况下，如果不是读取体量特别大的 bigkey，服务器端的输出缓冲区一般不会被阻塞的。所以，我们通常把普通客户端的缓冲区大小限制，以及持续写入量限制、持续写入时间限制都设置为 0，也就是不做限制。
		
		- 对于订阅客户端来说，一旦订阅的 Redis 频道有消息了，服务器端都会通过输出缓冲区把消息发给客户端。所以订阅客户端和服务器间的消息发送方式，不属于阻塞式发送。不过，如果频道消息较多的话，也会占用较多的输出缓冲区空间。因此，我们会给订阅客户端设置缓冲区大小限制、缓冲区持续写入量限制，以及持续写入时间限制。
		
		- 如果主从同步的 `client-output-buffer-limit` 设置过小，并且 master 数据量很大，主从全量同步时可能会导致 buffer 溢出，溢出后主从全量同步就会失败。如果主从集群配置了哨兵，那么哨兵会让 slave 继续向 master 发起全量同步请求，然后 buffer 又溢出同步失败，如此反复，会形成复制风暴，这会浪费 master 大量的 CPU、内存、带宽资源，也会让 master 产生阻塞的风险。

### 6.7.3. 主从集群缓冲区

#### 6.7.3.1. 全量复制缓冲区

主从集群间的数据复制包括全量复制和增量复制两种。全量复制是同步所有数据，而增量复制只会把主从库网络断连期间主库收到的命令，同步给从库。无论在哪种形式的复制中，为了保证主从节点的数据一致，都会用到缓冲区。

在全量复制过程中，主节点在向从节点传输 RDB 文件的同时，会继续接收客户端发送的写命令请求。这些写命令就会先保存在复制缓冲区中，等 RDB 文件传输完成后，再发送给从节点去执行。主节点上会为每个从节点都维护一个复制缓冲区，来保证主从节点间的数据同步。

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220519235134.png)

针对主从复制的缓冲区，我们可以做以下几点考虑：

- 控制主节点保存的数据量大小。按通常的使用经验，我们会把主节点的数据量控制在 2~4GB，这样可以让全量同步执行得更快些，避免复制缓冲区累积过多命令。

- 使用 ` client-output-buffer-limit` 配置项，来设置合理的复制缓冲区大小。设置的依据，就是主节点的数据量大小、主节点的写负载压力和主节点本身的内存大小。如估算每秒的请求写入量，每条请求的数据量来预设缓冲区大小。

- 主节点上复制缓冲区的内存开销，会是每个从节点客户端输出缓冲区占用内存的总和。如果集群中的从节点数非常多的话，主节点的内存开销就会非常大。所以，我们还必须得控制和主节点连接的从节点个数，不要使用大规模的主从集群。

#### 6.7.3.2. 增量复制缓冲区

主节点在把接收到的写命令同步给从节点时，同时会把这些写命令写入复制积压缓冲区。一旦从节点发生网络闪断，再次和主节点恢复连接后，从节点就会从复制积压缓冲区中，读取断连期间主节点接收到的写命令，进而进行增量同步。

这个增量缓冲区就是我们提到的 `repl_backlog_buffer`。复制积压缓冲区是一个大小有限的环形缓冲区。当主节点把复制积压缓冲区写满后，会覆盖缓冲区中的旧命令数据。如果从节点还没有同步这些旧命令数据，就会造成主从节点间重新开始执行全量复制。

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220519235655.png)

从本质上看，缓冲区溢出，无非就是三个原因：命令数据发送过快过大；命令数据处理较慢；缓冲区空间过小。明白了这个，我们就可以有针对性地拿出应对策略了。

- 针对命令数据发送过快过大的问题，对于普通客户端来说可以避免 bigkey，而对于复制缓冲区来说，就是避免过大的 RDB 文件。

- 针对命令数据处理较慢的问题，解决方案就是减少 Redis 主线程上的阻塞操作，例如使用异步的删除操作。

- 针对缓冲区空间过小的问题，解决方案就是使用 `client-output-buffer-limit` 配置项设置合理的输出缓冲区、复制缓冲区和复制积压缓冲区大小。当然，我们不要忘了，输入缓冲区的大小默认是固定的，我们无法通过配置来修改它，除非直接去修改 Redis 源码。