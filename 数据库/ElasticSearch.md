
# 1. ES 的架构

## 1.1. 数据划分

### 1.1.1. 数据组织形式

结构化数据：也称作行数据，是由二维表结构来逻辑表达和实现的数据，严格地遵循数据格式与长度规范，主要通过关系型数据库进行存储和管理。指具有固定格式或有限长度的数据，如数据库，元数据等。

非结构化数据：又可称为全文数据，不定长或无固定格式，不适于由数据库二维表来表现，包括所有格式的办公文档、XML、HTML、Word 文档，邮件，各类报表、图片和咅频、视频信息等。

对于结构化数据，因为它们具有特定的结构，所以我们一般都是可以通过关系型数据库（MySQL，Oracle 等）的二维表（Table）的方式存储和搜索，也可以建立索引。对于非结构化数据，我们一般使用全文搜索引擎进行存储，而 ES 就是一个全文搜索引擎。

### 1.1.2. 使用场景

ES 在全文检索、日志分析、监控分析等场景具有广泛应用。

#### 1.1.2.1. 日志实时分析

日志是互联网行业基础广泛的数据形式。典型日志有用来定位业务问题的运营日志，如慢日志、异常日志；用来分析用户行为的业务日志，如用户的点击、访问日志；以及安全行为分析的审计日志等。

Elastic 生态提供了完整的日志解决方案。通过简单部署，即可搭建一个完整的日志实时分析服务。ES 生态完美的解决了日志实时分析场景需求，这也是近几年 ES 快速发展的一个重要原因。

日志从产生到可访问一般在 10s 级，相比于传统大数据解决方案的几十分钟、小时级时效性非常高。

ES 底层支持倒排索引、列存储等数据结构，使得在日志场景可以利用 ES 非常灵活的搜索分析能力。通过 ES 交互式分析能力，即使在万亿级日志的情况下，日志搜索响应时间也是秒级。

日志处理的基本流程包含：日志采集 -> 数据清洗 -> 存储 -> 可视化分析。Elastic Stack 通过完整的日志解决方案，帮助用户完成对日志处理全链路管理。

![image.png](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202309110133197.png)

#### 1.1.2.2. 时序分析场景

时序数据是按时间顺序记录设备、系统状态变化的数据。典型的时序数据有传统的服务器监控指标数据、应用系统性能监控数据、智能硬件、工业物联网传感器数据等。

ES 提供灵活、多维度的统计分析能力，实现查看监控按照地域、业务模块等灵活的进行统计分析。另外，ES 支持列存储、高压缩比、副本数按需调整等能力，可实现较低存储成本。最后时序数据也可通过 Kibana 组件轻松实现可视化。

![image.png](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202309110135795.png)

#### 1.1.2.3. 搜索场景

搜索服务典型场景有像京东、拼多多、蘑菇街中的商品搜索；应用商店中的应用 APP 搜索；论坛、在线文档等站内搜索。

这类场景用户关注高性能、低延迟、高可靠、搜索质量等。如单个服务最大需达到 10w+ QPS，请求平均响应时间在 20ms 以内，查询毛刺低于 100ms，高可用如搜索场景通常要求 4 个 9 的可用性，支持单机房故障容灾等。

## 1.2. 倒排索引

非结构化数据的处理需要依赖全文搜索，而目前市场上开放源代码的最好全文检索引擎工具包就属于 Apache 的 Lucene 了。

但是 Lucene 只是一个工具包，它不是一个完整的全文检索引擎。Lucene 的目的是为软件开发人员提供一个简单易用的工具包，以方便的在目标系统中实现全文检索的功能，或者是以此为基础建立起完整的全文检索引擎。

目前以 Lucene 为基础建立的开源可用全文搜索引擎主要是 Solr 和 Elasticsearch。而 Lucene 能实现全文搜索主要是因为它实现了倒排索引的查询结构。

如何理解倒排索引呢？假如现有三份数据文档，文档的内容如下分别是：

```txt
Java is the best programming language.
PHP is the best programming language.
Javascript is the best programming language.
```

为了创建倒排索引，我们通过分词器将每个文档的内容域拆分成单独的词（我们称它为词条或 Term），创建一个包含所有不重复词条的排序列表，然后列出每个词条出现在哪个文档。这种结构由文档中所有不重复词的列表构成，对于其中每个词都有一个文档列表与之关联。这种由属性值来确定记录的位置的结构就是倒排索引。带有倒排索引的文件我们称为倒排文件。

![image.png](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202309101735990.png)

其中主要有如下几个核心术语需要理解：

- 词条（Term）
    索引里面最小的存储和查询单元，对于英文来说是一个单词，对于中文来说一般指分词后的一个词。
- 词典（Term Dictionary）
    字典，是词条 Term 的集合。搜索引擎的通常索引单位是单词，单词词典是由文档集合中出现过的所有单词构成的字符串集合，单词词典内每条索引项记载单词本身的一些信息以及指向“倒排列表”的指针。
- 倒排表（Post list）
    一个文档通常由多个词组成，倒排表记录的是某个词在哪些文档里出现过以及出现的位置。每条记录称为一个倒排项（Posting）。倒排表记录的不单是文档编号，还存储了词频等信息。
- 倒排文件（Inverted File）
    所有单词的倒排列表往往顺序地存储在磁盘的某个文件里，这个文件被称之为倒排文件，倒排文件是存储倒排索引的物理文件。

从上图我们可以了解到倒排索引主要由两个部分组成：

- 词典
- 倒排文件

词典和倒排表是 Lucene 中很重要的两种数据结构，是实现快速检索的重要基石。词典和倒排文件是分两部分存储的，词典在内存中而倒排文件存储在磁盘上。

## 1.3. 集群

ES 集群由一个或多个 Elasticsearch 节点组成，每个节点配置相同的 `cluster.name` 即可加入集群，默认值为 “elasticsearch”。需要确保不同的环境中使用不同的集群名称，否则最终会导致节点加入错误的集群。

一个 Elasticsearch 服务启动实例就是一个节点（Node）。节点通过 `node.name` 来设置节点名称，如果不设置则在启动时给节点分配一个随机通用唯一标识符作为名称。

### 1.3.1. 集群的发现

Zen Discovery 是 ES 的内置默认发现模块（发现模块的职责是发现集群中的节点以及选举 Master 节点）。它提供单播和基于文件的发现，并且可以扩展为通过插件支持云环境和其他形式的发现。Zen Discovery 可与其他模块集成，例如，节点之间的所有通信都使用 Transport 模块完成。节点使用发现机制通过 Ping 的方式查找其他节点。

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202309110126695.png)

ES 默认被配置为使用单播发现，以防止节点无意中加入集群。只有在同一台机器上运行的节点才会自动组成集群。如果集群的节点运行在不同的机器上，可以为 ES 提供一些尝试连接的节点列表。当一个节点联系到单播列表中的成员时，它就会得到整个集群所有节点的状态，然后它会联系 Master 节点，并加入集群。这意味着**单播列表不需要包含集群中的所有节点，它只是需要足够的节点，当一个新节点联系上其中一个并且通信就可以了**。该列表可由 `discovery.zen.ping.unicast.hosts` 指定。

节点启动后先 Ping ，如果 `discovery.zen.ping.unicast.hosts` 有设置，则 Ping 设置中的 Host ，否则尝试 ping localhost 的几个端口。ES 支持同一个主机启动多个节点，Ping 的 Response 会包含该节点的基本信息以及该节点认为的 Master 节点。

选举阶段，各节点会推举出自认为的 Master ，规则为按照 ID 的字典序排序，取第一个。如果各节点都没有认为的 Master ，则从所有节点中选择，规则同上。这里有个限制条件就是 `discovery.zen.minimum_master_nodes`，如果**节点数达不到最小值的限制，则循环上述过程，直到节点数足够可以开始选举**。

选举结果是肯定能选举出一个 Master ，如果只有一个 Local 节点那就选出的是自己。如果当前节点是 Master ，则开始等待节点数达到 `discovery.zen.minimum_master_nodes`，然后提供服务。如果当前节点不是 Master ，则尝试加入 Master 。ES 将以上服务发现以及选主的流程叫做 Zen Discovery 。

由于它支持任意数目的集群（ 1 - N ），所以不能像 Zookeeper 那样限制节点必须是奇数，也就无法用投票的机制来选主，而是通过一个规则。只要所有的节点都遵循同样的规则，得到的信息都是对等的，选出来的主节点肯定是一致的。但分布式系统的问题就出在信息不对等的情况，这时候很容易出现脑裂（Split-Brain）的问题。大多数解决方案就是设置一个 Quorum 值，要求**可用节点必须大于 Quorum（一般是超过半数节点），才能对外提供服务**。而 ES 中，这个 Quorum 的配置就是 `discovery.zen.minimum_master_nodes`。

### 1.3.2. 节点类型

每个节点既可以是候选主节点也可以是数据节点，通过在配置文件 `../config/elasticsearch.yml` 中设置即可，默认都为 true。数据节点负责数据的存储和相关的操作，例如对数据进行增、删、改、查和聚合等操作，所以数据节点（Data 节点）对机器配置要求比较高，对 CPU、内存和 I/O 的消耗很大。

通常随着集群的扩大，需要增加更多的数据节点来提高性能和可用性。候选主节点可以被选举为主节点（Master 节点），**集群中只有候选主节点才有选举权和被选举权**，其他节点不参与选举的工作。

主节点负责创建索引、删除索引、跟踪哪些节点是群集的一部分，并决定哪些分片分配给相关的节点、追踪集群中节点的状态等，稳定的主节点对集群的健康是非常重要的。

![image.png](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202309101842863.png)

每个节点上都保存了集群状态，但只有主节点才能修改集群状态信息（如创建索引、决定分片分布等），集群状态（Cluster State)，维护了一个集群中必要的信息   ：
- 所有的节点信息
- 所有的索引和其相关的 Mapping 与 Setting 信息
- 分片的路由信息

所以如果某个节点既是数据节点又是主节点，那么主节点产生影响时可能会对从而对整个集群的状态产生影响。因此为了提高集群的健康性，我们应该对 Elasticsearch 集群中的节点做好角色上的划分和隔离。可以使用几个配置较低的机器群作为候选主节点群。

主节点和其他节点之间通过 Ping 的方式互检查，主节点负责 Ping 所有其他节点，判断是否有节点已经挂掉。其他节点也通过 Ping 的方式判断主节点是否处于可用状态。当节点故障后，会将节点移出集群，并自动在其他节点上恢复故障节点上的分片。主分片故障时会提升其中一个副本分片为主分片。其他节点也会探活主节点，当主节点故障后，会触发内置的类 Raft 协议选主，并通过设置最少候选主节点数，避免集群脑裂。

![image.png](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202309110128550.png)

虽然对节点做了角色区分，但是**用户的请求可以发往任何一个节点，并由该节点负责分发请求、收集结果等操作，而不需要主节点转发**。这种节点可称之为协调节点，协调节点是不需要指定和配置的，集群中的任何节点都可以充当协调节点的角色。

### 1.3.3. 脑裂现象

如果由于网络或其他原因导致集群中选举出多个 Master 节点，使得数据更新时出现不一致，这种现象称之为脑裂，即集群中不同的节点对于 Master 的选择出现了分歧，出现了多个 Master 竞争。

“脑裂”问题可能有以下几个原因造成：

- 网络问题
    集群间的网络延迟导致一些节点访问不到 Master，认为 Master 挂掉了从而选举出新的 Master，并对 Master 上的分片和副本标红，分配新的主分片。
- 节点负载
    主节点的角色既为 Master 又为 Data，访问量较大时可能会导致 ES 停止响应（假死状态）造成大面积延迟，此时其他节点得不到主节点的响应认为主节点挂掉了，会重新选取主节点。
- 内存回收
    主节点的角色既为 Master 又为 Data，当 Data 节点上的 ES 进程占用的内存较大，引发 JVM 的大规模内存回收，造成 ES 进程失去响应。

为了避免脑裂现象的发生，我们可以从原因着手通过以下几个方面来做出优化措施：
- 适当调大响应时间，减少误判
    通过参数 `discovery.zen.ping_timeout` 设置节点状态的响应时间，默认为 3s，可以适当调大。
    
    如果 Master 在该响应时间的范围内没有做出响应应答，判断该节点已经挂掉了。调大参数（如 6s，`discovery.zen.ping_timeout:6`），可适当减少误判。
- 选举触发
    我们需要在候选集群中的节点的配置文件中设置参数 `discovery.zen.munimum_master_nodes` 的值。这个参数表示在选举主节点时需要参与选举的候选主节点的节点数，默认值是 1，官方建议取值 (`master_eligibel_nodes2)+1`，其中 `master_eligibel_nodes` 为候选主节点的个数。
    
    这样做既能防止脑裂现象的发生，也能最大限度地提升集群的高可用性，因为只要不少于 `discovery.zen.munimum_master_nodes` 个候选节点存活，选举工作就能正常进行。当小于这个值的时候，无法触发选举行为，集群无法使用，不会造成分片混乱的情况。
- 角色分离
    即候选主节点和数据节点进行角色分离，这样可以减轻主节点的负担，防止主节点的假死状态发生，减少对主节点“已死”的误判。

## 1.4. 分片

ES 支持 PB 级全文搜索，当索引上的数据量太大的时候，ES 通过水平拆分的方式将一个索引上的数据拆分出来分配到不同的数据块上，拆分出来的数据库块称之为一个分片。

这类似于 MySQL 的分库分表，只不过 MySQL 分库分表需要借助第三方组件而 ES 内部自身实现了此功能。在一个多分片的索引中写入数据时，通过路由来确定具体写入哪一个分片中，所以在创建索引的时候需要指定分片的数量，并且分片的数量一旦确定就不能修改。

分片的数量和下面介绍的副本数量都是可以通过创建索引时的 Settings 来配置，ES 默认为一个索引创建 5 个主分片, 并分别为每个分片创建一个副本。

## 1.5. 副本

副本就是对分片的 Copy，每个主分片都有一个或多个副本分片，当主分片异常时，副本可以提供数据的查询等操作。主分片和对应的副本分片是不会在同一个节点上的，所以副本分片数的最大值是 N-1（其中 N 为节点数）。对文档的新建、索引和删除请求都是写操作，必须在主分片上面完成之后才能被复制到相关的副本分片。

ES 为了提高写入的能力这个过程是并发写的，同时为了解决并发写的过程中数据冲突的问题，ES 通过乐观锁的方式控制，每个文档都有一个 `_version`（版本）号，当文档被修改时版本号递增。一旦所有的副本分片都报告写成功才会向协调节点报告成功，协调节点向客户端报告成功。

![image.png](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202309101908800.png)

当原主分片故障后，副本分片会被提升为主分片，这个提升主分片的过程是瞬间发生的。此时集群的状态将会为 Yellow。当该分片恢复时，如果期间有更改的数据只需要从主分片上复制修改的数据文件即可。

# 2. 数组存储原理

## 2.1. 索引写入原理

下图描述了 3 个节点的集群，共拥有 12 个分片，其中有 4 个主分片（S0、S1、S2、S3）和 8 个副本分片（R0、R1、R2、R3），每个主分片对应两个副本分片，节点 1 是主节点（Master 节点）负责整个集群的状态。

![image.png](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202309101911565.png)

写索引是只能写在主分片上，然后同步到副本分片。这里有四个主分片，一条数据 ES 是根据什么规则写到特定分片上的呢？

$$
shard=hash(routing) \% number\_of\_primary\_shards
$$

Routing 是一个可变值，默认是文档的 `_id`，也可以设置成一个自定义的值。Routing 通过 Hash 函数生成一个数字，然后这个数字再除以 `number_of_primary_shards`（主分片的数量）后得到余数。

因此**在创建索引的时候就需要确定主分片的数量并且永远无法改变这个数量**，因为如果数量变化了，那么所有之前路由的值都会无效，文档也再也找不到了。

由于在 ES 集群中每个节点通过上面的计算公式都知道集群中的文档的存放位置，所以每个节点都有处理读写请求的能力。在一个写请求被发送到某个节点后，协调节点会根据路由公式计算出需要写到哪个分片上，再将请求转发到该分片的主分片节点上。

![image.png](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202309101915606.png)

## 2.2. 数据存储原理

### 2.2.1. 分段写

索引文档以[[数据密集型系统设计1：引入#1.2.2. SSTables (SortStringTables)|段的形式]]存储在磁盘上，何为段？索引文件被拆分为多个子文件，则每个子文件叫作段，每一个段本身都是一个倒排索引，并且段具有不变性，一旦索引的数据被写入硬盘，就不可再修改。在底层采用了分段的存储模式，使它在读写时几乎完全避免了锁的出现，大大提升了读写性能 （在并发更新文档的场景下，ES 是采用乐观锁版本号的方式来实现并发控制）。

段被写入到磁盘后会生成一个提交点，提交点是一个用来记录所有提交后段信息的文件。一个段一旦拥有了提交点，就说明这个段只有读的权限，失去了写的权限。相反，当段在内存中时，就只有写的权限，而不具备读数据的权限，意味着不能被检索。

![image.png](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202309102050159.png)

每当有新 Document 要写入时，会进行以下操作：

1. 文档会先写入 Index Buffer，作为缓冲区
2. 将 Index Buffer 写入 Segment（内存），然后文档就可以被查询到了
3. 将 Index Buffer 同时写入 Transaction Log（WAL 机制），用于断电恢复数据
4. 将 Segments 落盘

其中第 2、3 两步合并成为 Refresh，默认 1 秒（`index.refresh_interval`）执行一次，这也就是为什么 ES 是近实时搜索引擎的原因（高版本的 ES 默认落盘）；另外 Index Buffer 被写满时也会触发，默认大小是 JVM 内存的 10%

其中第 4 步，其实是归属于 Flush 操作的一个步骤。Flush 默认 30 分钟执行一次，或者 Transaction Log 满（默认 512MB）也会触发。Flush 执行包含的流程有

1. 调用 Refresh
2. 调用 fsync，将 Segments 落盘
3. 清空 Transaction Log

段的概念提出主要是因为：在早期全文检索中为整个文档集合建立了一个很大的倒排索引，并将其写入磁盘中。如果索引有更新，就需要重新全量创建一个索引来替换原来的索引。这种方式在数据量很大时效率很低，并且由于创建一次索引的成本很高，所以对数据的更新不能过于频繁，也就不能保证时效性。

索引文件分段存储并且不可修改，那么新增、更新和删除如何处理呢？

- 新增
    由于数据是新的，所以只需要对当前文档新增一个段就可以了。
- 删除
    由于不可修改，所以对于删除操作，不会把文档从旧的段中移除而是通过新增一个 `.del` 文件，文件中会列出这些被删除文档的段信息。这个被标记删除的文档仍然可以被查询匹配到，但它会在最终结果被返回前从结果集中移除。
- 更新
    不能修改旧的段来进行反映文档的更新，其实更新相当于是删除和新增这两个动作组成。会将旧的文档在 `.del` 文件中标记删除，然后文档的新版本被索引到一个新的段中。可能两个版本的文档都会被一个查询匹配到，但被删除的那个旧版本文档在结果集返回前就会被移除。

段被设定为不可修改具有一定的优势也有一定的缺点，优势主要表现在：

- 不需要锁：如果你从来不更新索引，你就不需要担心多进程同时修改数据的问题。
- 一旦索引被读入内核的文件系统缓存，便会留在哪里，由于其不变性。只要文件系统缓存中还有足够的空间，那么大部分读请求会直接请求内存，而不会命中磁盘。这提供了很大的性能提升。
- 其它缓存 (像 Filter 缓存)，在索引的生命周期内始终有效。它们不需要在每次数据改变时被重建，因为数据不会变化。
- 写入单个大的倒排索引允许数据被压缩，减少磁盘 I/O 和需要被缓存到内存的索引的使用量。

段的不变性的缺点如下：

- 当对旧数据进行删除时，旧数据不会马上被删除，而是在 `.del` 文件中被标记为删除。而旧数据只能等到段更新时才能被移除，这样会造成大量的空间浪费。
- 若有一条数据频繁的更新，每次更新都是新增新的标记旧的，则会有大量的空间浪费。
- 每次新增数据时都需要新增一个段来存储数据。当段的数量太多时，对服务器的资源例如文件句柄的消耗会非常大。
- 在查询的结果中包含所有的结果集，需要排除被标记删除的旧数据，这增加了查询的负担。

### 2.2.2. 延迟写策略

为了提升写的性能，ES 并没有每新增一条数据就增加一个段到磁盘上，而是采用延迟写的策略。每当有新增的数据时，就将其先写入到内存中，在内存和磁盘之间是文件系统缓存。

在 Elasticsearch 中，写入和打开一个新段的轻量的过程叫做 Refresh （即内存刷新到文件缓存系统）。当达到默认的时间（1 秒钟）或者内存的数据达到一定量时，会触发一次刷新（Refresh），将内存中的数据生成到一个新的段上并缓存到文件缓存系统上，稍后再被刷新到磁盘中并生成提交点。这里的内存使用的是 ES 的 JVM 内存，而文件缓存系统使用的是操作系统的内存。

新的数据会继续的被写入内存，但内存中的数据并不是以段的形式存储的，因此不能提供检索功能。这就是为什么我们说 Elasticsearch 是近实时搜索，因为文档的变化并不是立即对搜索可见，但会在一秒之内变为可见。由内存刷新到文件缓存系统的时候会生成新的段，并将段打开以供搜索使用，而不需要等到被刷新到磁盘。

我们也可以手动触发 Refresh，`POST /_refresh` 刷新所有索引，`POST /nba/_refresh` 刷新指定的索引。

> [!tip] 刷新时机
> 
> 尽管刷新是比提交轻量很多的操作，它还是会有性能开销。当写测试的时候，手动刷新很有用，但是不要在生产环境下每次索引一个文档都去手动刷新。而且并不是所有的情况都需要每秒刷新。
> 
> 可能你正在使用 Elasticsearch 索引大量的日志文件，你可能想优化索引速度而不是近实时搜索。这时可以在创建索引时在 Settings 中通过调大 `refresh_interval = "30s"` 的值，降低每个索引的刷新频率，设值时需要注意后面带上时间单位，否则默认是毫秒。当 `refresh_interval=-1` 时表示关闭索引的自动刷新。

虽然通过延时写的策略可以减少数据往磁盘上写的次数提升了整体的写入能力，但是我们知道文件缓存系统也是内存空间，属于操作系统的内存，只要是内存都存在断电或异常情况下丢失数据的危险。为了避免丢失数据，Elasticsearch 添加了事务日志（Translog，一种预写日志），事务日志记录了所有还没有持久化到磁盘的数据。

![image.png](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202309101924322.png)

添加了事务日志后整个写索引的流程如上图所示：
1. 新文档写入内存
    一个新文档被索引之后，先被写入到内存中，但是为了防止数据的丢失，会追加一份数据到事务日志中。
    
    不断有新的文档被写入到内存，同时也都会记录到事务日志中。这时新数据还不能被检索和查询。
2. 内存数据刷新到磁盘
    当达到默认的刷新时间或内存中的数据达到一定量后，会触发一次 Refresh，将内存中的数据以一个新段形式刷新到文件缓存系统中并清空内存。这时虽然新段未被提交到磁盘，但是可以提供文档的检索功能且不能被修改。
3. 事务日志刷新
    随着新文档索引不断被写入，当日志数据大小超过 512M 或者时间超过 30 分钟时，会触发一次 Flush。内存中的数据被写入到一个新段同时被写入到文件缓存系统，文件系统缓存中数据通过 Fsync 刷新到磁盘中，生成提交点，日志文件被删除，创建一个空的新日志。

通过这种方式当断电或需要重启时，ES 不仅要根据提交点去加载已经持久化过的段，还需要根据 Translog 里的记录，把未持久化的数据重新持久化到磁盘上，避免了数据丢失的可能。

### 2.2.3. 段合并

由于自动刷新流程每秒会创建一个新的段，这样会导致短时间内的段数量暴增。而段数目太多会带来较大的麻烦。每一个段都会消耗文件句柄、内存和 CPU 运行周期。更重要的是，每个搜索请求都必须轮流检查每个段然后合并查询结果，所以段越多，搜索也就越慢。

Elasticsearch 通过在后台定期进行[[数据密集型系统设计1：引入#1.2.2.2. SSTables 写入与维护|段合并]]来解决这个问题。小的段被合并到大的段，然后这些大的段再被合并到更大的段。

段合并的时候会将那些旧的已删除文档从文件系统中清除。被删除的文档不会被拷贝到新的大段中。合并的过程中不会中断索引和搜索。由于自动刷新流程每秒会创建一个新的段，这样会导致短时间内的段数量暴增。而段数目太多会带来较大的麻烦。

段合并在进行索引和搜索时会自动进行，合并进程选择一小部分大小相似的段，并且在后台将它们合并到更大的段中，这些段既可以是未提交的也可以是已提交的。合并结束后老的段会被删除，新的段被 Flush 到磁盘，同时写入一个包含新段且排除旧的和较小的段的新提交点，新的段被打开可以用来搜索。

段合并的计算量庞大，而且还要吃掉大量磁盘 I/O，段合并会拖累写入速率，如果任其发展会影响搜索性能。Elasticsearch 在默认情况下会对合并流程进行资源限制，所以搜索仍然有足够的资源很好地执行。

# 3. ES 的性能优化

## 3.1. 集群索引评估准则

索引配置的评估可根据下面几点准则进行评估：

- 单个分片大小控制在 30-50GB。
- 集群总分片数量控制在 3w 以内。
- 1GB 的内存空间支持 20-30 个分片为佳。
- 一个节点建议不超过 1000 个分片。
- 索引分片数量建议和节点数量保持一致。
- 集群规模较大时建议设置专用主节点。
- 专用主节点配置建议在 8C16G 以上。
- 如果是时序数据，建议结合冷热分离 + ILM 索引生命周期管理。

## 3.2. 写入性能优化

1. 写入数据不指定 doc_id，让 ES 自动生成。
    如果自定义 doc_id 的话，则 ES 在写入过程中会多一步判断的过程，即先 Get 下该 doc_id 是否已经存在。如果存在的话则执行 Update 操作，不存在则创建新的 doc。
    
    因此如果我们对索引 doc_id 没有特别要求，则建议让 ES 自动生成 doc_id，这样可提升一定的写入性能。
2. 提前创建索引，使用固定的索引 mapping。
    创建索引及新加字段都是更新元数据操作，需要 master 节点将新版本的元数据同步到所有节点。
    
    因此在集群规模比较大，写入 qps 较高的场景下，特别容易出现 master 更新元数据超时的问题，这可导致 master 节点中有大量的 pending_tasks 任务堆积，从而造成集群不可用，甚至出现集群无主的情况。
3. 实时性要求不高的索引增大 refresh_interval 的时间。
    ES 默认的 refresh_interval 是 1s，即 doc 写入 1s 后即可被搜索到。如果业务对数据实时性要求不高的话，如日志场景，可将索引模版的 refresh_interval 设置成 30s，这能够避免过多的小 segment 文件的生成及段合并的操作。
4. 追求写入效率的场景，可先将索引副本数设置为单副本，写入完成后再打开副本。
    例如迁移数据时可先将副本数设置为 0，迁移完毕后再设置回来。
5. 使用 Bulk 批量插入数据时，控制单词 bulk 数量在 10M 左右。
    通常我们建议一次 Bulk 的数据量控制在 10M 以下，一次 Bulk 的 doc 数在 10000 上下浮动。
6. 使用自定义 routing 功能，尽量将请求转发到较少的分片。
    协调节点是异步得将数据发送给所有的分片，但是却需要等待所有的分片响应后才能返回给客户端，因此一次 Bulk 的延迟则取决于响应最慢的那个分片所在的节点。这就是分布式系统的长尾效应。
    
    因此，我们可以自定义 routing 值，这样写入、查询均带有路由字段信息。请求只会发送给部分分片，避免全量分片扫描。这些节点完成查询后将结果返回给请求节点，由请求节点汇聚各个节点的结果返回给客户端。
    
    ![image.png](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202309110033326.png)
    
7. 尽量选择 SSD 磁盘类型。
8. 冻结历史索引，释放内存空间。
    
    ![image.png](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202309110034464.png)
    
    Open 状态的索引由于是通过将倒排索引以 FST 数据结构的方式加载进内存中，因此索引是能够被快速搜索的，且搜索速度也是最快的。但是需要消耗大量的内存空间，且这部分内存为常驻内存，不会被 GC 的。1T 的索引预计需要消耗 2-4 GB 的 JVM 堆内存空间。
    
    > [!info] FST
    > </br>
    > 
    > `fst`（有限状态转换机）是一种数据结构，用于高效地存储和检索有限状态自动机（finite-state automaton）的形式。
    > 
    > 有限状态转换机是一种表示字符串集合的形式，它可以支持高效的前缀搜索、模式匹配和词典查询等操作。它在自然语言处理、信息检索和编译等领域有广泛的应用，特别是在需要高效处理大量字符串数据的场景下。
    > 
    > `fst` 数据结构的特点包括：
    > 1. 紧凑性：`fst` 以紧凑的形式存储字符串集合，通过有效地压缩状态转换和共享相同的前缀来减小存储空间的需求。
    > 2. 高效性：`fst` 支持高效的字符串操作，如前缀搜索、最长前缀匹配和模式匹配。它可以在常量时间内确定输入字符串是否存在于集合中，并快速地找到匹配的前缀或最佳匹配。
    > 3. 可变性：`fst` 可以进行动态更新和修改，允许在运行时插入、删除和修改字符串集合中的元素。
    > 4. 可序列化：`fst` 可以被序列化为二进制格式，以便在不同的系统之间进行存储和传输。
    
    Frozen 状态的索引特点是可被搜索，但是由于它不占用内存，只是存储在磁盘上，因此冻结索引的搜索速度是相对比较慢的。如果我们集群中的数据量比较大，历史数据也不能被删除，则可以考虑使用下面的 API 将历史索引冻结起来，这样便可释放出较多的内存空间。

## 3.3. 集群性能优化

以下性能优化点参考[腾讯的 ES 性能优化](https://mp.weixin.qq.com/s/CNf75yT0A0QPki-Qhw3_8w)一节。

![image.png](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202309110212386.png)

### 3.3.1. 高可用性能优化

高可用可划分为三个维度：

- 系统健壮性
    指 ES 内核自身的健壮性，也是分布式系统面临的共性难题。例如，在异常查询、压力过载下集群的容错能力；在高压力场景下，集群的可扩展性；在集群扩容、节点异常场景下，节点、多硬盘之间的数据均衡能力。
- 容灾方案
    如果通过管控系统建设，保障机房网络故障时快速恢复服务，自然灾害下防止数据丢失，误操作后快速恢复等。
- 系统缺陷
    这在任何系统发展过程中都会持续产生，比如说 Master 节点堵塞、分布式死锁、滚动重启缓慢等。

针对上述问题，下面来介绍我们在高可用方面的解决方案：

- 系统健壮性方面
    通过服务限流，容忍机器网络故障、异常查询等导致的服务不稳定。通过优化集群元数据管控逻辑，提升集群扩展能力一个数量级，支持千级节点集群、百万分片，解决集群可扩展性问题；集群均衡方面，通过优化节点、多硬盘间的分片均衡，保证大规模集群的压力均衡。
- 容灾方案方面
    通过扩展 ES 的插件机制支持备份回档，把 ES 的数据备份回档到廉价存储，保证数据的可恢复；支持跨可用区容灾，用户可以按需部署多个可用区，以容忍单机房故障。垃圾桶机制，保证用户在欠费、误操作等场景下，集群可快速恢复。
- 系统缺陷方面
    修复了滚动重启、Master 阻塞、分布式死锁等一系列 Bug。其中滚动重启优化，可加速节点重启速度 5+倍，具体可参考 PR [ES-46520](https://github.com/elastic/elasticsearch/pull/46520)；

![image.png](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202309110214557.png)

### 3.3.2. 成本优化

硬盘成本方面，由于数据具有明显的冷热特性，首先我们采用冷热分离架构，使用混合存储的方案来平衡成本、性能；其次，既然对历史数据通常都是访问统计信息，那么以通过预计算来换取存储和性能；如果历史数据完全不使用，也可以备份到更廉价的存储系统；其他一些优化方式包含存储裁剪、生命周期管理等。

内存成本方面，很多用户在使用大存储机型时会发现，存储资源才用了百分之二十，内存已经不足。其实基于时序数据的访问特性，我们可以利用 Cache 进行优化。

![image.png](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202309110216043.png)

### 3.3.3. 性能优化

以日志、监控为代表的时序场景，对写入性能要求非常高，写入并发可达 1000w/s。然而我们发现在带主键写入时，ES 性能衰减 1+倍，部分压测场景下，CPU 无法充分利用。以搜索服务为代表的场景，对查询性的要求非常高，要求 20w QPS, 平响 20ms，而且尽量避免 GC、执行计划不优等造成的查询毛刺。

![image.png](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202309110217952.png)

# 4. 集群运维经验

## 4.1. 集群问题排查
ES 集群的健康状态分为三种，分别是 Green、Yellow 和 Red。

- Green (绿色)：全部主&副本分片分配成功；
- Yellow (黄色)：至少有一个副本分片未分配成功；
- Red (红色)：至少有一个主分片未分配成功。

### 4.1.1. 查看集群健康状态

可以通过下面的 API 来查询集群的健康状态及未分配的分片个数：

```bash
GET _cluster/health
{
  "cluster_name": "es-xxxxxxx",
  "status": "yellow",
  "timed_out": false,
  "number_of_nodes": 103,
  "number_of_data_nodes": 100,
  "active_primary_shards": 4610,
  "active_shards": 9212,
  "relocating_shards": 0,
  "initializing_shards": 0,
  "unassigned_shards": 8,
  "delayed_unassigned_shards": 0,
  "number_of_pending_tasks": 0,
  "number_of_in_flight_fetch": 0,
  "task_max_waiting_in_queue_millis": 0,
  "active_shards_percent_as_number": 99.91323210412148
}
```

其中需要重点关注的几个字段有 status、number_of_nodes、unassigned_shards 和 number_of_pending_tasks。number_of_pending_tasks 这个字段如果很高的话，通常是由于 master 节点触发的元数据更新操作，部分节点响应超时导致的大量的任务堆积。我们可以通过下面的 API 来查看具体有那些 task 需要执行：

```bash
GET /_cat/pending_tasks
```

### 4.1.2. 查看分片未分配原因

当集群 Red 时候，我们可以通过下面的 API 来查看分片未分配的原因：

```bash
GET _cluster/allocation/explain
```

其中 index 和 shard 列出了具体哪个索引的哪个分片未分配成功。reason 字段则列出了哪种原因导致的分片未分配。这里也将所有可能的原因列出来：

```text
INDEX_CREATED：由于创建索引的API导致未分配。
CLUSTER_RECOVERED ：由于完全集群恢复导致未分配。
INDEX_REOPENED ：由于打开open或关闭close一个索引导致未分配。
DANGLING_INDEX_IMPORTED ：由于导入dangling索引的结果导致未分配。
NEW_INDEX_RESTORED ：由于恢复到新索引导致未分配。
EXISTING_INDEX_RESTORED ：由于恢复到已关闭的索引导致未分配。
REPLICA_ADDED：由于显式添加副本分片导致未分配。
ALLOCATION_FAILED ：由于分片分配失败导致未分配。
NODE_LEFT ：由于承载该分片的节点离开集群导致未分配。
REINITIALIZED ：由于当分片从开始移动到初始化时导致未分配（例如，使用影子shadow副本分片）。REROUTE_CANCELLED ：作为显式取消重新路由命令的结果取消分配。
REALLOCATED_REPLICA ：确定更好的副本位置被标定使用，导致现有的副本分配被取消，出现未分配。
```

常见分片未分配的原因如下：

1. 磁盘满了
    通常如果磁盘满了，ES 为了保证集群的稳定性，会将该节点上所有的索引设置为只读。ES `7.x` 版本之后当磁盘空间提升后可自动解除，但是 `7.x` 版本之前则需要手动执行下面的 API 来解除只读模式：
    
    ```bash
    PUT index_name/_settings
    {
     "index": {
       "blocks": {
         "read_only_allow_delete": null
        }
      }
    }
    ```
2. 分片数量超过 21 亿条限制
    该限制是分片维度而不是索引维度的。因此出现这种异常，通常是由于我们的索引分片设置的不是很合理。
    
    解决方法：切换写入到新索引，并修改索引模版，合理设置主分片数。
3. 主分片所在节点掉线
    这种情况通常是由于某个节点故障或者由于负载较高导致的掉线。
    
    解决方法：找到节点掉线原因并重新启动节点加入集群，等待分片恢复。
4. 索引所需属性与节点属性不匹配
    解决方法：重新设置索引所需的属性，和节点保持一致。因为如果重新设置节点属性，则需要重启节点，代价较高。
5. 节点长时间掉线后重新加入集群，引入了脏数据
    解决方法：通过 reroute API 来重新分配一个主分片。
6. 未分配分片太多，达到了分片恢复的阈值，其他分片排队等待
    这种情况通常出现在集群重启，或者某一个节点重启后。且由于设置的分片并发恢复的值较低导致。为了尽快恢复集群健康状态，可以通过调用下面的 API 来提升分片恢复的速度和并发度：
    
    ```bash
    PUT /_cluster/settings
    {
        "transient" : {
            "cluster.routing.allocation.node_concurrent_recoveries": "20",
            "indices.recovery.max_bytes_per_sec": "100mb"
        }
    }
    ```

## 4.2. 索引声明周期管理

在生产环境使用 ES 要面对的第一个问题通常是索引容量的规划，不合理的分片数，副本数和分片大小会对索引的性能产生直接的影响。查询和写入的性能与索引的大小是正相关的，所以要保证高性能，一定要限制索引的大小，具体来说是就限制分片数量和单个分片的大小。

以下介绍几种管理索引容量的方法：

### 4.2.1. 使用 Rollover 管理索引

Rollover 的原理是使用一个别名指向真正的索引，当指向的索引满足一定条件（文档数或时间或索引大小）更新实际指向的索引。

1. 创建索引并使用别名
     索引名称的格式为 {.\*}-d 这种格式的，数字默认是 6 位：
    ```bash
    PUT myro-000001
    {
      "aliases": {
        "myro_write_alias":{}
      }
    }
    ```
2. 通过别名写入数据
    例如 `POST /myro_write_alias/_bulk`
3. 执行 rollover 操作
    
    rollover 的 3 个条件是或关系，任意一个条件满足就会发生 rollover：
    
    ```bash
    POST /myro_write_alias/_rollover
    {
      "conditions": {
        "max_age":   "7d",
        "max_docs":  3,
        "max_size": "5gb"
      }
    }
    ```

使用 Rollover 的缺点：

- 必须明确执行了 rollover 指令才会更新 rollover 的别名对应的索引。
- 通常可以在写入数据之后再执行一下 rollover 命令，或者采用配置系统 cron 脚本的方式。
- 增加了使用的 rollover 的成本，对于开发者来说不够自动化。

### 4.2.2. 使用 ILM（Index Lifecycle Management ） 管理索引

ES 一直在索引管理这块进行优化迭代，从 6.7 版本推出了索引生命周期管理（Index Lifecycle Management ，简称 ILM)机制，是目前官方提供的比较完善的索引管理方法。所谓 Lifecycle (生命周期)是把索引定义了四个阶段：

![image.png](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202309110159297.png)

- Hot：索引可写入，也可查询，也就是我们通常说的热数据，为保证性能数据通常都是在内存中的。
- Warm：索引不可写入，但可查询，介于热和冷之间，数据可以是全内存的，也可以是在 SSD 的硬盘上的。
- Cold：索引不可写入，但很少被查询，查询的慢点也可接受，基本不再使用的数据，数据通常在大容量的磁盘上。
- Delete：索引可被安全的删除。

这 4 个阶段是 ES 定义的一个索引从生到死的过程, Hot -> Warm -> Cold -> Delete 4 个阶段只有 Hot 阶段是必须的，其他 3 个阶段根据业务的需求可选。

通过 ILM 按以下方式管理索引：

1. 建立 Lifecycle 策略
    
    ![image.png](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/202309110202931.png)
    
2. 建立索引模版
    
    ```bash
    PUT /_template/myes_template
    {
      "index_patterns": [
        "myes-*"
      ],
      "aliases": {
        "myes_reade_alias": {}
      },
      "settings": {
        "index": {
          "lifecycle": {
            "name": "myes-lifecycle",
            "rollover_alias": "myes_write_alias"
          },
          "refresh_interval": "30s",
          "number_of_shards": "12",
          "number_of_replicas": "1"
        }
      },
      "mappings": {
        "properties": {
          "name": {
            "type": "keyword"
          }
        }
      }
    }
    ```
    
    注意：
    - 模版匹配以索引名称 myes- 开头的索引。
    - 所有使用此模版创建的索引都有一个别名 myes_reade_alias 用于方便查询数据；
    - 模版绑定了上面创建的 Lifecycle 策略，并且用于 rollover 的别名是 myes_write_alias。
3. 创建索引
    
    ```bash
    PUT /myes-testindex-000001
    {
      "aliases": {
        "myes_write_alias":{}
      }
    }
    ```
    
    注意:
    - 索引的名称是 .\*-d 的形式；
    - 索引的别名用于 lifecycle 做 rollover。
4. 查看索引配置
    `GET /myes-testindex-000001`
5. 写入数据
    `POST /myes_write_alias/_bulk`
6. 配置 Lifecycle 自动 Rollover 的时间间隔
    由于 ES 是一个准实时系统，很多操作都不能实时生效。Lifecycle 的 rollover 之所以不用每次手动执行 rollover 操作是因为 ES 会隔一段时间判断一次索引是否满足 rollover 的条件，ES 检测 ILM 策略的时间默认为 10min。如果需要手动修改这个时间间隔的话可以修改 Lifecycle 配置：
    
    ```bash
    PUT _cluster/settings
    {
      "transient": {
        "indices.lifecycle.poll_interval": "3s"
      }
    }
    ```

