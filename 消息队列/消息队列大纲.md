#知识大纲 #消息队列 

# 1. 使用篇

## 1.1. 消息队列的使用场景

### 1.1.1. 使用场景

- 异步化，提升并行能力。
- 增加缓冲，提升系统可用性。
- 流量控制。
- 解耦。
- 生产者/消费者模型。
- 广播消息。
- 流数据处理。

### 1.1.2. 弊端

同时我们也要认识到，消息队列也有它自身的一些问题和局限性，包括：
-   引入消息队列带来的延迟问题。
-   增加了系统的复杂度。
-   可能产生数据不一致的问题。如果需要强一致需要高代价补偿，如分布式事务，对账。

## 1.2. 基于消息队列支持分布式事务

在实际应用中，比较常见的分布式事务实现有 2PC、TCC 和事务消息。每一种实现都有其特定的使用场景，也有各自的问题，都不是完美的解决方案。

事务消息需要消息队列提供相应的功能才能实现，Kafka 和 RocketMQ 都提供了事务相关功能：

![](https://r2.129870.xyz/img/20220530110727.png)

对于第 4 步的消息提交操作，如果出现失败，Kafka 和 RocketMQ 有不同的处理策略：

- Kafka：直接抛出异常，由用户进行处理。

- RocketMQ：增加了事务反查的机制。

	在提交或者回滚事务消息时发生网络异常，RocketMQ 的 Broker 没有收到提交或者回滚的请求，Broker 会定期去 Producer 上反查这个事务对应的本地事务的状态，然后根据反查结果决定提交或者回滚这个事务。
	
	为了支撑这个事务反查机制，业务代码需要实现一个反查本地事务状态的接口，告知 RocketMQ 本地事务是成功还是失败。

	![](https://r2.129870.xyz/img/20220530111046.png)

# 2. 基础原理

## 2.1. 消息队列设计考虑点

1. 基于异步的机制，减少同步阻塞。

2. 高效的网络 IO 模型，如 Netty/NIO。

3. 序列化与反序列化机制，需要在序列化复杂度，易读性，序列化速度以及序列化的大小之间作出平衡。

4. 数据的传输问题，需要处理数据的收发 (毡包拆包问题：固定长度，分隔符，前置长度)，数据的单双工信道通信问题。

## 2.2. 消息队列的模型

### 2.2.1. 队列模型

队列模型在消息入队出队过程中，需要保证这些消息严格有序，按照什么顺序写进队列，必须按照同样的顺序从队列中读出来。

如果有多个生产者往同一个队列里面发送消息，这个队列中可以消费到的消息，就是这些生产者生产的所有消息的合集。消息的顺序就是这些生产者发送消息的自然顺序。如果有多个消费者接收同一个队列的消息，这些消费者之间实际上是竞争的关系，每个消费者只能收到队列中的一部分消息，也就是说任何一条消息只能被其中的一个消费者收到。

如果需要将一份消息数据分发给多个消费者，要求每个消费者都能收到全量的消息，这个时候，单个队列就满足不了需求。一个可行的解决方式是，为每个消费者创建一个单独的队列，让生产者发送多份。

![](https://r2.129870.xyz/img/20220527120014.png)

RabbitMQ 使用的就是队列模型，由 Exchange 决定将消息发送给哪些队列：

![](https://r2.129870.xyz/img/20220527120229.png)

### 2.2.2. 发布订阅模型

#### 2.2.2.1. 发布订阅模型机制

在发布 - 订阅模型中，消息的发送方称为发布者（Publisher），消息的接收方称为订阅者（Subscriber），服务端存放消息的容器称为主题（Topic）。发布者将消息发送到主题中，订阅者在接收消息之前需要先“订阅主题”。“订阅”在这里既是一个动作，同时还可以认为是主题在消费时的一个逻辑副本，每份订阅中，订阅者都可以接收到主题的所有消息。

![](https://r2.129870.xyz/img/20220527120027.png)

#### 2.2.2.2. 发布订阅模型的消费组

![](https://r2.129870.xyz/img/20220527122610.png)

RocketMQ 中，订阅者的概念是通过消费组（Consumer Group）来体现的。每个消费组都消费主题中一份完整的消息，不同消费组之间消费进度彼此不受影响。也就是说，一条消息被 Consumer Group1 消费过，也会再给 Consumer Group2 消费。

消费组中包含多个消费者，同一个组内的消费者是竞争消费的关系，每个消费者负责消费组内的一部分消息。如果一条消息被消费者 Consumer1 消费了，那同组的其他消费者就不会再收到这条消息。

在 Topic 的消费过程中，由于消息需要被不同的组进行多次消费，所以消费完的消息并不会立即被删除，这就需要 RocketMQ 为每个消费组在每个队列上维护一个消费位置（Consumer Offset），这个位置之前的消息都被消费过，之后的消息都没有被消费过，每成功消费一条消息，消费位置就加一。这个消费位置是非常重要的概念，我们在使用消息队列的时候，丢消息的原因大多是由于消费位置处理不当导致的。

Kafka 的消息模型和 RocketMQ 是完全一样的，RocketMQ 中对应的概念和生产消费过程中的确认机制，都完全适用于 Kafka。唯一的区别是，在 Kafka 中，队列这个概念的名称不一样，Kafka 中对应的名称是“分区（Partition）”，含义和功能是没有任何区别的。

### 2.2.3. 队列模型与发布订阅模型对比

生产者就是发布者，消费者就是订阅者，队列就是主题，并没有本质的区别。它们最大的区别其实就是，**一份消息数据能不能被消费多次的问题**。实际上，在发布 - 订阅模型中，如果只有一个订阅者，那它和队列模型就基本是一样的了。也就是说，发布 - 订阅模型在功能层面上是可以兼容队列模型的。现代的消息队列产品使用的消息模型大多是这种发布 - 订阅模型，当然也有例外。

## 2.3. Pulsar 的架构

![](https://r2.129870.xyz/img/20220531182328.png)

### 2.3.1. Pulsar 架构分析

- Pulsar 采用存储与计算分离的架构，对于 Broker 节点，负责消息读写请求的转发，消息数据存储在 BookKeeper (一个分布式存储系统) 中，而元数据信息存放在 Zookeeper 中。
- Broker 与 BookKeeper 通过 ledger (WAL) 进行交互，每个 ledger 只能由一个进程读写，避免了写的冲突。
- Broker 采取无状态设计，因此可以快速高效的支持扩缩容，对于节点宕机与更换情况也可以快速切换节点。
- Load Balancer 负责动态的分配，哪些 Broker 管理哪些主题分区。
- Managed Ledger 这个模块负责管理本节点需要用到的那些 Ledger，这些 Ledger 都是保存在 BookKeeper 集群中的。为了提升性能，Pulsar 采用了一个 Cache 模块，来缓存一部分 Ledger。

### 2.3.2. Pulsar 数据流转过程

Pulsar 的客户端要读写某个主题分区上的数据之前，依然要在元数据中找到分区当前所在的那个 Broker，这一点是和其他消息队列的实现是一样的。不一样的地方是，其他的消息队列，分区与 Broker 的对应关系是相对稳定的，只要不发生故障，这个关系是不会变的。而在 Pulsar 中，这个对应关系是动态的，它可以根据 Broker 的负载情况进行动态调整，而且由于 Broker 是无状态的，分区可以调整到集群中任意一个 Broker 上，这个负载均衡策略就可以做得非常简单并且灵活。如果某一个 Broker 发生故障，可以立即用任何一个 Broker 来替代它。

客户端在收发消息之前，需要先连接 Service Discovery 模块，获取当前主题分区与 Broker 的对应关系，然后再连接到相应 Broker 上进行消息收发。客户端收发消息的整体流程，和其他的消息队列是差不多的。比较显著的一个区别就是，消息是保存在 BookKeeper 集群中的，而不是本机上。数据的可靠性保证也是 BookKeeper 集群提供的，所以 Broker 就不需要再往其他的 Broker 上复制消息了。

图中的 Global replicators 模块虽然也会复制消息，但是复制的目的是为了在不同的集群之间共享数据，而不是为了保证数据的可靠性。集群间数据复制是 Pulsar 提供的一个特色功能，具体可以看一下 Pulsar 文档中的 geo-replication 这部分。

### 2.3.3. Pulsar 的优劣势

- 在 Pulsar 这种架构下，消息数据保存在 BookKeeper 中，元数据保存在 ZooKeeper 中，Broker 的数据存储的职责被完全被剥离出去，只保留了处理收发消息等计算的职责，这是一个非常典型的“存储计算分离”的设计。

- 对于计算节点来说，它不需要存储数据，节点就变成了无状态的（Stateless）节点。一个由无状态节点组成的集群，管理、调度都变得非常简单了。集群中每个节点都是一样的，天然就支持水平扩展。任意一个请求都可以路由到集群中任意一个节点上，负载均衡策略可以做得非常灵活，可以随机分配，可以轮询，也可以根据节点负载动态分配等等。故障转移（Failover）也更加简单快速，如果某个节点故障了，直接把请求分配给其他节点就可以了。

- 存储计算分离之后，原来一个集群变成了两个集群，整个系统变得更加复杂了。

- 原本可以在本地进行读写的数据，现在需要从另一个集群读写。这受限于网络带宽与传输成本。当涉及到大量数据读写时，网络开销会更明显。**因此这种架构吞吐量没有传统的 MQ 架构高。**

# 3. 高可靠

## 3.1. 消息不丢失保证

![](https://r2.129870.xyz/img/20220530113439.png)

1. 生产阶段
	通过请求确认机制，当生产者发送消息后，需要等到消息队列返回确认发送成功的标识后才可认为消息发送成功。否则超时未收到消息或者发送失败时需要进行重试重新发送。如果由于网络抖动导致的确认丢失出现消息重复的情况，**消费者需要有能力处理消息重复的场景**。
2. 存储阶段
	如果需要保证消息百分百不丢失，则需要消息队列在收到消息后立马持久化到磁盘中。只有持久化成功后给生产者返回 ack 信号。

	消息队列在收到消息后不一定会立马持久化到磁盘中，可能仅仅写入 PageCache 中，所以为了保证消息不丢失，可以设置消息写入时直接持久化到磁盘，但这样会影响写入性能。同时为了避免节点崩溃，还需要将消息同步到其它备用节点，当备用节点确认消息存储后返回 ack 信号，以此保证消息的备份。备用节点的数量越多，容灾能力自然更好，但写入速度也会因此下降。
	
3. 消费阶段
	消费者也采用请求确认机制，只有当消费成功后才给队列返回成功的 ack，若消费失败则不会删除消息以备重新消费。为了确认能做到这点，**消息队列通常在同一时刻会被某个消费者锁定，在锁定这段期间，其他消费者无法消费。**

## 3.2. 消息幂等性保证

在 MQTT 协议中，给出了三种传递消息时能够提供的服务质量标准，这三种服务质量从低到高依次是：

- At most once: 至多一次。消息在传递时，最多会被送达一次。换一个说法就是，没什么消息可靠性保证，允许丢消息。一般都是一些对消息可靠性要求不太高的监控场景使用，比如每分钟上报一次机房温度数据，可以接受数据少量丢失。

- At least once: 至少一次。消息在传递时，至少会被送达一次。也就是说，不允许丢消息，但是允许有少量重复消息出现。

- Exactly once：恰好一次。消息在传递时，只会被送达一次，不允许丢失也不允许重复，这个是最高的等级。这个服务质量标准不仅适用于 MQTT，对所有的消息队列都是适用的。

我们现在常用的绝大部分消息队列提供的服务质量都是 At least once，包括 RocketMQ、RabbitMQ 和 Kafka 都是这样。也就是说，消息队列很难保证消息不重复。

### 3.2.1. 幂等性解决方案

对于天然幂等性的操作，就算不进行额外处理也能保证数据准确性。如删除数据，将删除标记为置为 1，不管执行多少次都是一样的。

而对于非天然幂等性的，就需要在业务代码中进行额外处理，通常有以下几种解决措施：

1. 利用数据库的唯一约束实现幂等。
2. 为更新的数据设置前置条件，如版本号机制。
3. 记录并检查操作，可以利用全局唯一 ID 方式进行检查。
    - 全局唯一 ID 生成需要满足简单，高可用和高性能条件，这或多或少都会对性能做出一些牺牲。
    - 获取 + 检查 + 记录这三个操作必须是原子性的，否则会出现检查失效的情况。这就意味着要么将这三个操作用事务来完成，要么需要锁机制。这也会对性能造成一定影响。

## 3.3. 消息积压处理

### 3.3.1. 消息积压原因

- 生产者发送过快。

- 消费者消费过慢，除了正常消费慢之外，还需要考虑是否是消费失败导致重复消费的情况。

### 3.3.2. 消息积压解决措施

- 进行消费端紧急扩容
	需要说明的是，**在队列数量没有变的情况下，单纯增加消费者数量是没有效果的**。因为一个队列在同一时刻对于一个消费组来说是只会有一个消费者的，消费组中消费者再多也没意义。因此要么是在生产端把消息发往多个队列，达到并行消费的目的。要么是从该队列中取出消息，再将它分发个多个队列，从而达到并行消费的目的。
- 将消息临时写入其他系统，如数据库中。先将消息接下来，后续再慢慢消费回去。
- 服务降级，保证系统正常运转，防止压垮系统。
- 若是消息生产速率过快，排查过快原因，减少消息生产。

## 3.4. 消息复制与备份

### 3.4.1. RocketMQ 的复制方式

#### 3.4.1.1. 异步复制

消息先发送到主节点上，就返回“写入成功”，然后消息再异步复制到从节点上。此种模式下主从配置不可更改，不采用选举模式。这意味着如果主节点宕机，则无法进行数据写入。

#### 3.4.1.2. 同步复制

Dledger 在写入消息的时候，要求至少消息复制到半数以上的节点之后，才给客户端返回写入成功，并且它是支持通过选举来动态切换主节点的。

### 3.4.2. Kafka 的复制方式

Kafka 中，复制的基本单位是分区。每个分区的几个副本之间，构成一个小的复制集群，Broker 只是这些分区副本的容器，所以 Kafka 的 Broker 是不分主从的。分区的多个副本中也是采用一主多从的方式。Kafka 在写入消息的时候，采用的也是异步复制的方式。消息在写入到主节点之后，并不会马上返回写入成功，而是等待足够多的节点都复制成功后再返回。在 Kafka 中这个“足够多”是多少呢？Kafka 的设计哲学是，让用户自己来决定。

Kafka 为这个“足够多”创造了一个专有名词：ISR（In Sync Replicas)，翻译过来就是“保持数据同步的副本”。ISR 的数量是可配的，但需要注意的是，这个 ISR 中是包含主节点的。Kafka 使用 ZooKeeper 来监控每个分区的多个节点，如果发现某个分区的主节点宕机了，Kafka 会利用 ZooKeeper 来选出一个新的主节点，这样解决了可用性的问题。

### 3.4.3. Kafka 的组织关系图

Kafka 使用 Zookeeper 来保存节点和集群的关系。

![](https://r2.129870.xyz/img/20220531152458.png)

左侧这棵树保存的是 Kafka 的 Broker 信息，/brokers/ids/\[0…N]，每个临时节点对应着一个在线的 Broker，Broker 启动后会创建一个临时节点，代表 Broker 已经加入集群可以提供服务了，节点名称就是 BrokerID，节点内保存了包括 Broker 的地址、版本号、启动时间等等一些 Broker 的基本信息。如果 Broker 宕机或者与 ZooKeeper 集群失联了，这个临时节点也会随之消失。

右侧部分的这棵树保存的就是主题和分区的信息。/brokers/topics/ 节点下面的每个子节点都是一个主题，节点的名称就是主题名称。每个主题节点下面都包含一个固定的 partitions 节点，pattitions 节点的子节点就是主题下的所有分区，节点名称就是分区编号。每个分区节点下面是一个名为 state 的临时节点，节点中保存着分区当前的 leader 和所有的 ISR 的 BrokerID。这个 state 临时节点是由这个分区当前的 Leader Broker 创建的。如果这个分区的 Leader Broker 宕机了，对应的这个 state 临时节点也会消失，直到新的 Leader 被选举出来，再次创建 state 临时节点。

# 4. 高性能

## 4.1. Kafka 的高性能设计

1. 批量处理技术
    当调用 send () 方法发送一条消息之后，无论是同步发送还是异步发送，Kafka 都不会立即就把这条消息发送出去。它会先把这条消息，存放在内存中缓存起来，然后选择合适的时机把缓存中的所有消息组成一批，一次性发给 Broker。
    
    简单地说，就是攒一波一起发。在 Kafka 的服务端，Kafka 不会把一批消息再还原成多条消息，再一条一条地处理，这样太慢了。Kafka 这块儿处理的非常聪明，每批消息都会被当做一个“批消息”来处理。也就是说，在 Broker 整个处理流程中，无论是写入磁盘、从磁盘读出来、还是复制到其他副本这些流程中，批消息都不会被解开，一直是作为一条“批消息”来进行处理的。在消费时，消息同样是以批为单位进行传递的，Consumer 从 Broker 拉到一批消息后，在客户端把批消息解开，再一条一条交给用户代码处理。
    
    构建批消息和解开批消息分别在发送端和消费端的客户端完成，不仅减轻了 Broker 的压力，最重要的是减少了 Broker 处理请求的次数，提升了总体的处理能力。
    
    如果开启了压缩模式的话，那么压缩与解压的过程也是在发送端与客户端完成的，这也利用了批处理的优势。
1. 使用顺序读写提升磁盘 IO 性能
    Kafka 充分利用了磁盘顺序读写快于随机读写的特性。它的存储设计非常简单，对于每个分区，它把从 Producer 收到的消息，顺序地写入对应的 log 文件中，一个文件写满了，就开启一个新的文件这样顺序写下去。消费的时候，也是从某个全局的位置开始，也就是某一个 log 文件中的某个位置开始，顺序地把消息读出来。这样一个简单的设计，充分利用了顺序读写这个特性，极大提升了 Kafka 在使用磁盘时的 IO 性能。
1. 利用 PageCache 加速消息读写
    Kafka 在读写消息文件的时候，充分利用了 PageCache 的特性。一般来说，消息刚刚写入到服务端就会被消费，按照 LRU 的“优先清除最近最少使用的页”这种策略，读取的时候，对于这种刚刚写入的 PageCache，命中的几率会非常高。
    
    也就是说，大部分情况下，消费读消息都会命中 PageCache，带来的好处有两个：一个是读取的速度会非常快，另外一个是，给写入消息让出磁盘的 IO 资源，间接也提升了写入的性能。
1. ZeroCopy：零拷贝技术
    在服务端，处理消费的大致逻辑是这样的：
    1. 首先，从文件中找到消息数据，读到内存中。
    2. 然后，把消息通过网络发给客户端。
    
    这个过程中，数据实际上做了 2 次或者 3 次复制：
    
    1. 从文件复制数据到 PageCache 中，如果命中 PageCache，这一步可以省掉。
    2. 从 PageCache 复制到应用程序的内存空间中，也就是我们可以操作的对象所在的内存。
    3. 从应用程序的内存空间复制到 Socket 的缓冲区，这个过程就是我们调用网络应用框架的 API 发送数据的过程。
    
    Kafka 使用零拷贝技术可以把这个复制次数减少一次，上面的 2、3 步骤两次复制合并成一次复制。直接从 PageCache 中把数据复制到 Socket 缓冲区中，这样不仅减少一次数据复制，更重要的是，由于不用把数据复制到用户内存空间，DMA 控制器可以直接完成数据复制，不需要 CPU 参与，速度更快。
    
    如果你遇到这种从文件读出数据后再通过网络发送出去的场景，并且这个过程中你不需要对这些数据进行处理，那一定要使用这个零拷贝的方法，可以有效地提升性能。

- 消息队列如何做到高可用？
    - Rabbit 的单机模式、普通集群模式 (多机器消费一台机器的 queue)、镜像模式 (每台机器同步 queue)
    - Kafka 的模式：一个节点是一个 broker，一个 topic 划分成多个 partion 同步到不同的 broker 上，形成 replica 副本。Replica 选举 leader，读写都与 leader 进行。当消息成功写到所有 replica 副本，消息才可被读到。
- 消息的有序性如何保证？
- 如何设计一个 MQ 系统？
    - 伸缩性方面：可快速扩容，如 Kafka 的 broker -> topic -> partition 的模式
    - 持久化方面：数据需要写入磁盘；生产者与消费者的消息确认机制。
    - 可用性方面：部分节点宕机可快速切换与恢复