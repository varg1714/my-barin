# Alibaba-Canal接入记录

## Canal原理

### 基于主从复制协议

基于Mysql的主从复制协议。

- Mysql主从复制实现

  

![image-20201015180818351](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20201015180818.png)

复制遵循三步过程：

1. 主服务器将更改记录到binlog中（这些记录称为binlog事件，可以通过来查看`show binary events`）
2. 从服务器将主服务器的二进制日志事件复制到其中继日志。
3. 中继日志中的从服务器重做事件将随后更新其旧数据。

- Canal将自己伪装成Slave与Mysql进行交互，订阅Mysql的binlog日志记录。

![image-20201015180941023](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20201015180941.png)

1. Canal模拟MySQL从站的交互协议，伪装成MySQL从站，然后将转储协议发送到MySQL主服务器。
2. MySQL Master接收到转储请求，并开始将二进制日志推送到slave（即Canal）。
3. Canal将二进制日志对象解析为其自己的数据类型（最初为字节流）

### 数据的同步

#### 为何采取binlog作为数据同步的方式？

如果在业务中存在多个数据源需要同步数据的话，可以有以下几种方式：

- ES与其它数据源数据的双写

  在数据更新的业务代码中同步更新数据到其它数据源中。这样虽然能做到数据的同步，但在业务代码中耦合了数据同步的非业务逻辑，因此这种做法并非上策。

- 利用LogStash定时同步

  可以利用LogStash定时获取Mysql中的数据将其刷新到其它数据源中，但这样的问题在于这个同步时间的确定。如果时间设定过长，那么数据同步的及时性得不到保证。如果设定的过短，那么对数据库的访问压力就会增大。因此这种做法亦非上策。

- 利用binlog日志文件同步

  binlog日志记录了Mysql数据库的数据更新操作，因此如果我们能获取到binlog数据的话，就能获取到其中变更的数据记录，然后将其处理更新到其它数据源。采取这样的方式，可以将数据的更新操作解耦出来，不影响业务代码的逻辑。

#### [binlog日志格式](https://blog.csdn.net/wwwdc1012/article/details/88373440)

| 错误日志              | 记录在启动，运行或停止mysqld时遇到的问题                     |
| --------------------- | ------------------------------------------------------------ |
| 通用查询日志          | 记录建立的客户端连接和执行的语句                             |
| 二进制日志            | 记录更改数据的语句                                           |
| 中继日志              | 从复制主服务器接收的数据更改                                 |
| 慢查询日志            | 记录所有执行时间超过 `long_query_time` 秒的所有查询或不使用索引的查询 |
| DDL日志（元数据日志） | 元数据操作由DDL语句执行                                      |

- binlog结构

  ![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20201015181356.png)

  binlog中主要包含以下部分：

  - 日志名

  - 位点

  - 事件名

  - Mysql Server Id

  - Info

    Info中对于表的变动会产生一条binlog日志，这条日志里记录了变动的表的id，即table_id。这个table_id是存储在内存中的，变更的方式是每次自增+1。这个table_id会映射到表结构的快照(table_map)。对于表结构的变动(DML)以及表数据的变动，都会产生这样一条日志。

    上面说到的表结构的快照是存储在内存中，因此有容量限制，Mysql采取了置换算法(LRU最久未用淘汰算法),会淘汰旧的快照记录。

    [table_id参考1](https://blog.csdn.net/iteye_7245/article/details/82477336)

    [table_id参考2](http://blog.chinaunix.net/uid-26896862-id-3329896.html)

#### Canal获取binlog数据

了解binlog日志以后可以发现binlog中不会存储列名的信息，只会存储变动的数据以及表名等基本信息([Column结构](https://dev.mysql.com/doc/refman/5.6/en/information-schema-columns-table.html))。因此单纯的从binlog日志中读取到的数据其实并不是完整的，如果在处理这些数据时如果需要列名等其它信息，需要我们进行其它额外的操作。

如何获取我们想要的binlog data对应的列名等其它数据呢？这里给出了[Canal实现这方面所采取的方案](https://github.com/alibaba/canal/wiki/TableMetaTSDB)：

- **思考一**

  解决这个问题，第一个最直接的思考：canal在订阅binlog时，尽可能保持准实时，不做延迟回溯消费. 这样的方式会有对应的优点和缺点：
  - canal要做准实时解析，业务上可能有failover的需求，假如在业务处理离线时，原本canal基于内存ringBuffer的模型，会出现延迟解析，如果要解决这个问题，必须在canal store上支持了持久化存储的能力，比如实现或者转存到kafka/rocketmq等.
  - canal准实时解析，如果遇到canal本身的failover，比如zookeeper挂、网络异常，出现分钟级别以上的延迟，DDL变化的概率会比较高，此时就会陷入之前一样的表结构一致性的问题

  整个方案上，基本是想避开表结构的问题，在遇到一些容灾场景下一定也会遇上，不是一个技术解决的方案，废弃.

- **思考二**

  经过了第一轮辩证的思考，基本确定想通过迂回的方式，简单绕过一致性的问题不是正解，所以这次的思考主要就是如何正面解决一致性的问题. 基本思路：基于binlog中DDL的变化，来动态维护一份表结构，比如DDL中增加一个列，在本地表结构中也动态增加一列，解析binlog时都从本地表结构中获取
  - 本地表结构的维护，每个canal进程可以带着一个二进制的MySQL版本，把收到的每条DDL，在本地MySQL中进行重放，从而维护一个本地的MySQL表结构
  - 每个canal第一次订阅或者回滚到指定位点，刚启动时需要拉取一份表结构基线，存入本地表结构MySQL库，然后在步骤1的方案上维护一个增量DDL.

  整个方案上，可以绝大部分的解决DDL的问题，但也存在一些缺点：
  - 每个canal进程，维护一个隔离的MySQL实例。不论是资源成本、运维成本上都有一些瑕疵，更像是一个工程的解决方案，不是一个开源+技术产品的解决方案
  - 位点如果存在相对高频的位点回溯，每次都需要重新做表结构基线，做表结构基线也会概率遇上表结构一致性问题

- **思考三**

  有了之前的两次思考，思路基本明确了，在一次偶然的机会中和alibaba Druid的作者高铁，交流中得到了一些灵感，是否可以基于Druid对DDL的支持能力，来构建一份动态的表结构.
  - 首先准备一份表结构基线数据，每条建表语句传入druid的SchemaRepository.console()，构建一份druid的初始表结构
  - 之后在收到每条DDL变更时，把alter table add/drop column等，全部传递给druid，由druid识别ddl语句并在内存里执行具体的add/drop column的行为，维护一份最终的表结构
  - 定时把druid的内存表结构，做一份checkpoint，之后的位点回溯，可以是checkpoint + 增量DDL重放的方式来快速构建任意时间点的表结构

最终实现的方案如下：

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20201015184803.png)

#### Canal数据处理方式

Canal订阅binlog日志获取到数据后，将数据经过一定处理得到数据与表结构。数据处理可以有几种处理方式，可以本地存储或者将其送到MQ中，这取决与我们具体采用的策略。

- 存放在本地的话，不可避免的问题就是存储数据的容量。当数据量小的时候，存放在本地尚可，但一旦数据量过大的话，对机器内存就产生了一定影响。Canal将数据在本地存储的方式实现为ring-buffer(环形存储)，因此对于数据的存储是作出了限制，它会制定一个阈值。并且对于这些数据的获取采取了生产者-消费者类型的阻塞方式，PV操作条件无法满足时将会阻塞直至缓存可用。因此这就解决了数据存储的问题。

- 如果发送到MQ的话，那么要考虑将数据发送到MQ与客户端消费MQ消息的正确性。

  binlog日志本身是有序的，当保存到MQ时也要保证MQ里消息也是有序的，如果不一致的话将会产生数据一致性问题。针对MQ的有序性，阿里做了这样一些[考虑](https://github.com/alibaba/canal/wiki/Canal-Kafka-RocketMQ-QuickStart)，如果消息队列采取的是Kafka的话，那么单topic单分区天然就能保证消息的顺序性。但是如果是多topic多分区的话，那么这个有序性问题就需要我们解决。更为复杂的情况是，当部署多个Canal Server，并且每个Server订阅一张表的时候，那么保证这两张表的数据顺序性写入问题将会更加复杂。具体的问题以及对应措施考虑可以参考[有赞项目中关于这部分的设计](https://www.infoq.cn/article/QxFxSOt5UuH1-WqJwMQg)。

## Canal使用

### Canal同步数据到RDB

可以将数据同步到RDB数据库，用于数据备份与迁移

### Canal同步数据到NoSQL数据库

如同步Redis中用于缓存。利用Canal来做缓存更新的话，就不需要在业务代码中耦合缓存更新的代码了，即以前缓存的双写策略可以替换为binlog的同步更新。

### Canal同步数据到ES

对于Mysql来说，搜索数据能力并不是非常的优秀，特别是在数据量大的情况下，如果扫描全表搜索数据的话，那么效率将会更低。ES可以以优秀的搜索能力来满足我们的搜索需求。

