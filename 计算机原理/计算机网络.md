#知识大纲 #计算机原理 #网络

# 1. 基础大纲

## 1.1. TCP 与 UDP 的区别

- TCP 面向连接的，而 UDP 是无连接的
- TCP 提供可靠的传输服务
	- 无差错
	- 不丢失
	- 不重复
	- 按序到达
	- TCP 通过校验和，重传机制，序号标识，滑动窗口，确认应答实现可靠传输。
- UDP 尽最大努力交付
	- UDP 不保证可靠交付，也不保证有序性和准确性
	- UDP 面向报文，对应用层交下来的报文不合并，不拆分，保留报文边界
	- UDP 没有拥塞机制，即使网络阻塞也不会降低发送速率
	- UDP 实时性更好，可以一对一，一对多，多对一和多对多
	- TCP 对资源消耗多，UDP 资源消耗少
	- DNS、DHCP、TFTP、SNMP 等都基于 UDP 协议

## 1.2. HTTP 与 HTTPS 区别

- HTTP 使用 80 端口，HTTPS 使用 443 端口
- HTTPS 采用 SSL 认证，通信需要证书

## 1.3. HTTP、TCP、Socket 的区别

HTTP 是应用层协议，底层采用 TCP 协议实现。使用 TCP 时通过 SOCKET 进行连接建立与通信，SOCKET 是一个门面模式，将 TCP 的协议实现内容隐藏在内部。

## 1.4. HTTP 版本

- HTTP1.0
	短连接使用一次就关闭连接，重新请求又会重新建立连接
- HTTP1.1
	- 长连接复用连接，数据传输后连接保持。使用 Keep-Alive 控制，有一个保存时间。
	- 请求管道化：请求可以批量发送，但是必须要按顺序返回。
	- 增加 Host 字段，支持断点续传。
- HTTP2.0
	- 二进制分帧
		- 将数据拆分为一个个帧，每个帧归属于一个流，复用 TCP 连接。多个所属的帧在多个流中并发，达到了真正并行的意义。
		- 可以设置特定的优先级，保证优先级高的请求帧优先传输。**有优先级就会有饿死问题。**
		- 由于在同一个 TCP 连接中，因此丢包时 TCP 会阻塞等待丢失的包重发，因此丢包率高的情况下性能还不如 HTTP1.1。
	- 连接共享：可以复用连接。
	- 头部压缩：将原有的文本格式的 http 请求头进行了压缩。
	- 服务器推送：服务器可主动推送数据。

## 1.5. TCP 三次握手四次挥手协议

![](https://r2.129870.xyz/img/20220507111819.png)

### 1.5.1. 三次握手原因

**确保服务端与客户端都有收发数据的能力**

1. 第一次握手，服务端收到客户端的请求。
	- 服务端确认：客户端有发送数据的能力，自己有接收数据的能力。
	-  客户端确认：无法确认任何信息。
2. 第二次握手，客户端收到服务端的响应。
	- 服务端确认：客户端有发送数据的能力，自己有接收数据的能力。
	- 客户端确认：自己有发送接收和数据的能力。服务端有发送和接收数据的能力。
 
	此时服务端会将此连接的请求放到半连接队列中，第三次握手失败会重试。重试超过一定次数丢弃连接。
	
3. 第三次握手，服务端收到客户端的响应。
	- 这一步确认了客户端有接收数据的能力，服务端有发送数据的能力。至此服务端与客户端双方的收发数据能力都确认完毕，可以开始数据传输。
	- 此时该连接进入全连接队列，客户端与服务端开始通信。第三次握手时客户端是可以携带数据，前两次不可以。

### 1.5.2. 四次挥手原因

**将服务端与客户端的关闭连接时间分开**

1. 前两次挥手
	客户端发起关闭连接给服务端，服务端确认连接关闭
2. 后两次挥手
	服务端关闭连接，客户端确认连接关闭
> [!question] 客户端第四次挥手等待时间为何是 2MSL?
> - 确保服务端收到了自己的确认通知，而不至于导致在服务端没收到通知的情况下连接被关闭。两倍时延为客户端响应一次的时延与服务端重发一次断开请求的时延。
> - 只在 TIME_WAIT 阶段进行 2MSL 等待，第二次挥手时候不会等待的原因是当前连接没有真正断开，包数据丢失仍可以重新发送。可第四次挥手不等待的话连接被关闭就没有重新发送的机会了！
> - 2MSL 的时间同样可以保证之前的所有数据都在网络中消失，这样下一个连接就不会有旧的数据包了。

## 1.6. TCP 粘包与拆包

在 socket 通讯过程中，如果通讯的一端一次性连续发送多条数据包，tcp 协议会将多个数据包打包成一个 tcp 报文发送出去，这就是所谓的粘包。而如果通讯的一端发送的数据包超过一次 tcp 报文所能传输的最大值时，就会将一个数据包拆成多个最大 tcp 长度的 tcp 报文分开传输，这就叫做拆包。

- 出现的原因
	- 发送的数据包太小，都放在缓冲区，TCP 将缓冲区数据一起发出产生粘包。
	- 发送数据过快，或者接收端接收过慢，导致缓冲区数据堆积产生粘包。
	- 数据包过大超过缓冲区空间或者最大报文长度发生拆包。
- 解决措施
	由上可以看出，其实粘包与拆包和数据大小有关，这个问题本质和应用层有关，而只是在缓冲区体现出来这个现象而已。
	
	所以一些解决都措施都需要在应用层进行，如发送端增加数据长度属性，接收端根据此判断；固定数据包长度进行接收处理；发送端给数据包增加分隔符使用分隔符处理等，这些都是在应用层做的操作。

## 1.7. OSI 与 TCP/IP 模型层次

OSI (Open-System-Interconnect) 模型：
- 应用层 ：HTTP/HTTPS/FTP 等协议
- 表示层：进行数据加解密，压缩转换等工作。如 LPP 轻量级表示协议。
- 会话层：负责连接管理，如 SSL 套接字协议，TLS 安全传输协议，RPC 调用协议。
- 传输层：进行数据传输。如 TCP/UDP 协议。
- 网络层：网络寻址转发，逻辑编址与子网划分等。如 IP 协议
- 数据链路层：物理寻址，物理信号与逻辑信号转好控制。如 XTP 协议，PPTP 协议。
- 物理层：实际物理线路相关规范。

TCP/IP 协议：
- 应用层：负责应用数据处理，数据加解密转换，格式转换等。如 HTTP、IAMP、FTP、DHCP、SSH 等。
- 传输层：进行数据传输。如 TCP/UDP 协议。
- 网络层：网络寻址转发，逻辑编址与子网划分等。如 IP 协议
- 物理层：实际物理线路相关规范。
- 数据链路层：物理寻址，物理信号与逻辑信号转好控制。如 XTP 协议，PPTP 协议。

区别：可以看到，OSI 与 TCP/IP 相比最大的区别就是将应用传，表示层，会话层进行了合并。

![](https://r2.129870.xyz/img/20220507224631.png)

## 1.8. TCP 的可靠传输原理

TCP 保证传输的有序性、不丢失、无差错，主要通过以下几点实现：
- 确认机制
	当收到 seq 为 x 的数据后，会返回 $ack=x+1$，期待下一个收到的数据是 $x+1$。发送方此时发送了 $x+1$, $x+2$, $x+3$ 三个数据，若 $x+1$ 数据丢失了，接收方仍会返回 $ack=x+1$，$x+2$, $x+3$ 会暂存在缓冲区。等到 $x+1$ 超时重传了，此时接收方累计确认，$ack=x+4$。
- 流量控制
	1. 接收方会维护一个接收窗口大小，用于告诉发送方可接收的最大数据量。计算规则为：$窗口大小=总缓存大小-已接收未读数据大小+已接收可删除数据大小$。
	2. 当发送方发送数据时，接收方会返回此窗口大小。发送方根据窗口大小动态调整发送数据，称之为滑动窗口。
	3. 当发送方记录的滑动窗口为 0，接收方有空间了尝试更新窗口值，会给发送方通知。为了避免该通知丢失，接收方维持了一个定时器。该定时器自动探测接收方窗口是否变化。
- 拥塞避免
	为了避免网络原因导致的大批数据发送而压垮接收方，因此引入了拥塞避免机制。拥塞避免机制根据接收方网络情况动态调整发送的数据量大小。
	
	拥塞控制有以下几个核心参数：cwnd (congestion window)：当前窗口大小；ssthresh：慢开始门限值，初始值 16。
	1. 慢开始
		1. 首先发送方从 $cwnd=1$ 开始发送数据，收到 ack 后 $cwnd = cwnd \times 2$，直到到达 ssthresh 门限值。
		2. 到达门限制后转为拥塞避免算法。
	2. 拥塞避免
		Reno 算法：从 ssthresh 值开始，cwnd 窗口改为加法递增，每次+1；当发生网络阻塞丢包时，ssthresh 变为 cwnd 的一半。
		
		实际上 Reno 算法并不是 TCP 协议中的真正实现，因为骤降一半虽然可以避免大量数据涌入网络，但非平滑的过渡仍非常影响性能。特别是在包偶尔丢失的情况下。许多算法对此进行了[改进](https://www.jiqizhixin.com/articles/2020-06-01-12)，这些算法寻求一个平稳的过渡。
	3. 快重传
		快重传要求接收方一旦收到一个失序报文后就立马发出确认，使发送方知道数据发生丢失。
		
		接收方存在数据缓存的情况，有可能不会立马发出 ack 通知，而是将通知等到接收方给发送方发通知的时候捎带过去。快重传避免这种情况。
	4. 快恢复
		在连续收到三个 ack 后，发送方知道数据丢失，因此重新发送数据。数据丢失可能是网络拥堵，因此发送方会将 ssthresh 降低为原始一半，然后开始加法递增。
- 超时时间的确定
	- $RTTs_n (平滑往返时间) = (1 - \alpha) \times RTTs_{n-1} + \alpha × 新的 RTT 样本$，(RFC6298 标准推荐 $\alpha$ 值为 0.125)
	- $RTO (超时重传时间) = RTTs_n + 4 \times RTTd_n$
	- $新的 RTTd_n = (1 - \beta) \times RTTd_{n-1} + \beta × (RTTs_n - 新的 RTT 样本)$，(RFC6298 标准推荐 $\beta$ 值为 0.25)，第一个报文段的 $RTTd_{1}$ 值：$RTTd_{1} = RTT_1 \div 2$ 

![](https://r2.129870.xyz/img/20220507194831.png)
![](https://r2.129870.xyz/img/20220507194925.png)

## 1.9. ARQ (Automatic Repeat-Request) 协议与滑动窗口协议

### 1.9.1. 停止等待协议 ARQ 协议

每发送完一个分组就停止发送，直到确认后发送下一个。失败则重传。需要处理以下几类问题：
- 无差错情况
- 出现差错情况：超时重传
- 确认丢失：超时重传
- 确认延迟：延迟后已经重传数据，对于延迟到达的确认接收不处理

### 1.9.2. 连续 ARQ 协议

连续发送多个分组，采用累计确认的形式。此过程可用滑动窗口记录。当出现数据需要重传的情况，需要回退到上一个确认的分组号进行重发。

滑动窗口协议：在发送方和接收方各自维护一个滑动窗口。
- 发送方窗口
	- 已发送并收到确认的数据（不在发送窗口和发送缓冲区之内）
	- 已发送但未收到确认的数据（位于发送窗口之内）
	- 允许发送但尚未发送的数据（位于发送窗口之内）
	- 发送窗口之外的缓冲区内暂时不允许发送的数据
- 接收方窗口
	- 已发送确认并交付主机的数据（不在接收窗口和接收缓冲区之内）
	- 未按序收到的数据（位于接收窗口之内）
	- 允许的数据（位于接收窗口之内）
	- 不允许接收的数据（位于发送窗口之内）
 
规则：
- 凡是已经发送过的数据，在未收到确认之前，都必须暂时保留，以便在超时重传时使用。
- 只有当发送方 A 收到了接收方的确认报文段时，发送方窗口才可以向前滑动几个序号。
- 当发送方 A 发送的数据经过一段时间没有收到确认（由超时计时器控制），就要使用回退 N 步协议，回到最后接收到确认号的地方，重新发送这部分数据。

# 2. IO 多路复用

## 2.1. IO 模型分类

IO 模型可以分为以下几类：
- BIO（Blocking I/O）：同步阻塞 I/O，传统的 I/O 模型。在进行 I/O 操作时，必须等待数据读取或写入完成后才能进行下一步操作。
- NIO（Non-blocking I/O）：同步非阻塞 I/O，是一种事件驱动的 I/O 模型。在进行 I/O 操作时，不需要等待操作完成，可以进行其他操作。
- AIO（Asynchronous I/O）：异步非阻塞 I/O，是一种更高级别的 I/O 模型。在进行 I/O 操作时，不需要等待操作完成，就可继续进行其他操作，当操作完成后会自动回调通知。

**阻塞和非阻塞，数据就绪前是否等待：**
- 阻塞：数据未就绪，读阻塞直到有数据，缓冲区满时，写操作也会阻塞等待。本质上是线程挂起，不能做其他事。
- 非阻塞：请求直接返回，本质上线程活跃，可以处理其他事情。

**同步与异步，数据就绪，是操作系统送过去，还是应用程序自己读取：**
- 同步：数据就绪后应用程序自己读取。
- 异步：数据就绪后操作系统直接回调应用程序。

## 2.2. Linux 如何处理网络请求？

### 2.2.1. 阻塞 IO

如果客户端想向 Linux 服务器发送一段数据，C 语言的实现方式是：

```c
int main()  
{  
     int fd = socket();      // 创建一个网络通信的socket结构体  
     connect(fd, ...);       // 通过三次握手跟服务器建立TCP连接  
     send(fd, ...);          // 写入数据到TCP连接  
     close(fd);              // 关闭TCP连接  
}
```

服务端通过如下 C 代码接收客户端的连接和发送的数据：

```c
int main()  
{  
     fd = socket(...);        // 创建一个网络通信的socket结构体  
     bind(fd, ...);           // 绑定通信端口  
     listen(fd, 128);         // 监听通信端口，判断TCP连接是否可以建立  
     while(1) {  
         connfd = accept(fd, ...);              // 阻塞建立连接  
         int n = recv(connfd, buf, ...);        // 阻塞读数据  
         doSomeThing(buf);                      // 利用读到的数据做些什么  
         close(connfd);                         // 关闭连接，循环等待下一个连接  
    }  
}
```

把服务端处理请求的细节展开，得到如下图所示的同步阻塞网络 IO 的数据接收流程：

![image.png](https://r2.129870.xyz/img/202308072322663.png)

主要步骤是：

1. 创建 Socket 对象
    服务端通过 `socket ()` 函数陷入内核态进行 socket 系统调用，该内核函数会创建 **socket 内核对象**，主要有两个重要的结构体，**（进程）等待队列**，和 **（数据）接收队列**，为了方便理解，等待队列前可以加上进程二字，其实不加更准确，接收队列同样。
    
    进程等待队列，存放了服务端的用户进程 A 的进程描述符和回调函数；socket 的数据接收队列，存放网卡接收到的该 socket 要处理的数据。
2. 进程 A 调用 recv () 函数接收数据
    此时会进入到 **recvfrom () 系统调用函数**，发现 socket 的数据等待队列没有它要接收的数据到达时，进程 A 会让出 CPU，进入阻塞状态。进程 A 的进程描述符和它被唤醒用到的回调函数 callback func 会组成一个结构体叫等待队列项，放入 socket 的进程等待队列。
3. 客户端的发送数据到达服务端的网卡
4. 网卡将网络传输过来的数据通过 DMA 控制程序复制到内存环形缓冲区 RingBuffer 中
5. 网卡向 CPU 发出**硬中断**
6. CPU 接受硬中断信号触发软中断处理网络设备请求逻辑
    为了避免过度占用 CPU 处理网络设备请求导致其他设备如鼠标和键盘的消息无法被处理，会调用网络驱动注册的中断处理函数，进行简单快速处理后向内核中断进程 ksoftirqd 发出**软中断**，就释放 CPU，由软中断进程处理复杂耗时的网络设备请求逻辑。
7. 复制缓冲区数据到 Socket 的接收队列
    **内核中断进程 ksoftirqd** 收到软中断信号后，会将网卡复制到内存的数据，根据数据报文的 IP 和端口号，将其拷贝到对应 socket 的接收队列。
8. 唤醒进程
    内核中断进程 ksoftirqd 根据 socket 的数据接收队列的数据，通过进程等待队列中的回调函数，唤醒要处理该数据的进程 A。进程 A 会进入 CPU 的运行队列，等待获取 CPU 执行数据处理逻辑。
9. 进程读取数据
    进程 A 获取 CPU 后，会回到之前调用 **recvfrom () 函数**时阻塞的位置继续执行，这时发现 socket 内核空间的等待队列上有数据，会在**内核态将内核空间的 socket 等待队列的数据拷贝到用户空间，然后才会回到用户态执行进程的用户程序，从而真的解除阻塞**。

用户进程 A 在调用 recvfrom () 系统函数时，有两个阶段都是等待的：
1. 等待 socket 接收数据
    在数据没有准备好的时候，进程 A 等待内核 socket 准备好数据。
2. 等待数据拷贝到用户缓冲区
    内核准备好数据后，进程 A 继续等待内核将 socket 等待队列的数据拷贝到自己的用户缓冲区。在内核完成数据拷贝到用户缓冲区后，进程 A 才会从 recvfrom () 系统调用中返回，并解除阻塞状态。

整体流程如下：

![image.png](https://r2.129870.xyz/img/202308072332292.png)

在 IO 阻塞逻辑中，存在下面三个问题：
1. 进程在 recv 的时候大概率会被阻塞掉，导致一次进程切换。
2. 当 TCP 连接上的数据到达服务端的网卡、并从网卡复制到内核空间 socket 的数据等待队列时，进程会被唤醒，又是一次进程切换；并且，在用户进程继续执行完 recvfrom () 函数系统调用，将内核空间的数据拷贝到了用户缓冲区后，用户进程才会真正拿到所需的数据进行处理。
3. 一个进程同时只能等待一条连接，如果有很多并发，则需要很多进程。

总结：一次数据到达会进行两次进程切换，一次数据读取有两处阻塞，单进程对单连接。

### 2.2.2. 非阻塞 IO

为了解决同步阻塞 IO 的问题，操作系统提供了非阻塞的 recv () 函数，这个函数的效果是：如果没有数据从网卡到达内核 socket 的等待队列时，系统调用会直接返回，而不是阻塞的等待。

如果我们要产生一个非阻塞的 socket，在 C 语言中如下代码所示：

```c
// 创建socket  
int sock_fd = socket(AF_INET, SOCK_STREAM, 0);  
...  
// 更改socket为nonblock  
fcntl(sock_fd, F_SETFL, fdflags | O_NONBLOCK);  
// connect  
....  
while(1)  {  
    int recvlen = recv(sock_fd, recvbuf, RECV_BUF_SIZE) ;  
    ......  
}
```

非阻塞 IO 模型如下图所示：

![image.png](https://r2.129870.xyz/img/202308072337827.png)

非阻塞 IO，是将等待数据从网卡到达 socket 内核空间这一部分变成了非阻塞的，用户进程调用 recvfrom () 会重复发送请求检查数据是否到达内核空间，如果没有到，则立即返回，不会阻塞。不过，当数据已经到达内核空间的 socket 的等待队列后，用户进程依然要等待 recvfrom () 函数将数据从内核空间拷贝到用户空间，才会从 recvfrom () 系统调用函数中返回。

非阻塞 IO 模型解决了**两次进程切换，两处阻塞，单进程对单连接**中的**两处阻塞**问题，将**两处阻塞**变成了**一处阻塞**，但依然存在**两次进程切换，一处阻塞，单进程对单连接**的问题。

### 2.2.3. IO 多路复用

要解决**两次进程切换，单进程对单连接**的问题，服务器引入了 IO 多路复用技术，通过一个进程处理多个 TCP 连接，不仅降低了服务器处理网络请求的进程数，而且**不用在每个连接的数据到达时就进行进程切换，进程可以一直运行并只处理有数据到达的连接**，当然，如果要监听的所有连接都没有数据到达，进程还是会进入阻塞状态，直到某个连接有数据到达时被回调函数唤醒。

IO 多路复用模型如下图所示：

![image.png](https://r2.129870.xyz/img/202308072339504.png)

从上图可知，系统调用 select 函数阻塞执行并返回数据就绪的连接个数，然后调用 recvfrom () 函数将到达内核空间的数据拷贝到用户空间，尽管这两个阶段都是阻塞的，但是由于只会处理有数据到达的连接，整体效率会有极大的提升。

到这里，阻塞 IO 模型的**两次进程切换，两处阻塞，单进程对单连接**问题，通过非阻塞 IO 和多路复用技术，就只剩下了**一处阻塞**这个问题，即 Linux 服务器上用户进程一定要等待数据从内核空间拷贝到用户空间，如果这个步骤也变成非阻塞的，也就是进程调用 recvfrom 后立刻返回，内核自行去准备好数据并将数据从内核空间拷贝到用户空间、再 notify 通知用户进程去读取数据，那就是 **IO 异步调用**，不过，Linux 没有提供异步 IO 的实现，真正意义上的网络异步 IO 是 Windows 下的 IOCP（IO 完成端口）模型，这里就不探讨了。

## 2.3. select、poll、epoll 的实现

### 2.3.1. select 的实现原理

#### 2.3.1.1. select 的数据结构

Linux 提供的 select 函数的定义如下：

```c
int select(  
    int nfds,                     // 监控的文件描述符集里最大文件描述符加1  
    fd_set *readfds,              // 监控有读数据到达文件描述符集合，引用类型的参数  
    fd_set *writefds,             // 监控写数据到达文件描述符集合，引用类型的参数  
    fd_set *exceptfds,            // 监控异常发生达文件描述符集合，引用类型的参数  
    struct timeval *timeout);     // 定时阻塞监控时间
```

readfds、writefds、errorfds 是三个文件描述符集合。select 会遍历每个集合的前 nfds 个描述符，分别找到**可以读取、可以写入、发生错误的描述符，统称为 “就绪” 的描述符**。然后用找到的子集替换这三个引用参数中的对应集合，返回所有就绪描述符的数量。

timeout 参数表示调用 select 时的阻塞时长。如果所有 fd 文件描述符都未就绪，就阻塞调用进程，直到某个描述符就绪，或者阻塞超过设置的 timeout 后，返回。如果 timeout 参数设为 NULL，会无限阻塞直到某个描述符就绪；如果 timeout 参数设为 0，会立即返回，不阻塞。

- **文件描述符 fd**
    文件描述符（file descriptor）是一个非负整数，从 0 开始。进程使用文件描述符来标识一个打开的文件。Linux 中一切皆文件。
    
    系统为每一个进程维护了一个文件描述符表，表示该进程打开文件的记录表，而**文件描述符实际上就是这张表的索引**。每个进程默认都有 3 个文件描述符：0 (stdin)、1 (stdout)、2 (stderr)。
- **socket**
    socket 可以用于同一台主机的不同进程间的通信，也可以用于不同主机间的通信。操作系统将 socket 映射到进程的一个文件描述符上，进程就可以通过读写这个文件描述符来和远程主机通信。
    
    socket 是进程间通信规则的高层抽象，而 fd 提供的是底层的具体实现。socket 与 fd 是一一对应的。通过 socket 通信，实际上就是通过文件描述符 fd 读写文件。
    
    本文中，为了方便理解，可以认为 socket 内核对象 ≈ fd 文件描述符 ≈ TCP 连接。
- **fd_set 文件描述符集合**
    select 函数参数中的 fd_set 类型表示文件描述符的集合。
    
    由于文件描述符 fd 是一个从 0 开始的无符号整数，所以可以使用 fd_set 的二进制每一位来表示一个文件描述符。某一位为 1，表示对应的文件描述符已就绪。比如比如设 fd_set 长度为 1 字节，则一个 fd_set 变量最大可以表示 8 个文件描述符。当 select 返回 fd_set = 00010011 时，表示文件描述符 1、2、5 已经就绪。
- **fd_set 的 API**
    
    fd_set 的使用涉及以下几个 API：
    
    ```c
    #include <sys/select.h>  
    int FD_ZERO(int fd, fd_set *fdset);  // 将 fd_set 所有位置 0  
    int FD_CLR(int fd, fd_set *fdset);   // 将 fd_set 某一位置 0  
    int FD_SET(int fd, fd_set *fd_set);  // 将 fd_set 某一位置 1  
    int FD_ISSET(int fd, fd_set *fdset); // 检测 fd_set 某一位是否为 1
    ```

#### 2.3.1.2. select 监听多个连接的用法

服务端使用 select 监控多个连接的 C 代码是：

```c
#define MAXCLINE 5       // 连接队列中的个数  
int fd[MAXCLINE];        // 连接的文件描述符队列  
  
int main(void)  
{  
      sock_fd = socket(AF_INET,SOCK_STREAM,0)          // 建立主机间通信的 socket 结构体  
      .....  
      bind(sock_fd, (struct sockaddr *)&server_addr, sizeof(server_addr);         // 绑定socket到当前服务器  
      listen(sock_fd, 5);  // 监听 5 个TCP连接  
  
      fd_set fdsr;         // bitmap类型的文件描述符集合，01100 表示第1、2位有数据到达  
      int max;  
  
      for(i = 0; i < 5; i++)  
      {  
          .....  
          fd[i] = accept(sock_fd, (struct sockaddr *)&client_addr, &sin_size);   // 跟 5 个客户端依次建立 TCP 连接，并将连接放入 fd 文件描述符队列  
      }  
  
      while(1)               // 循环监听连接上的数据是否到达  
      {  
        FD_ZERO(&fdsr);      // 对 fd_set 即 bitmap 类型进行复位，即全部重置为0  
  
        for(i = 0; i < 5; i++)  
        {  
             FD_SET(fd[i], &fdsr);      // 将要监听的TCP连接对应的文件描述符所在的bitmap的位置置1，比如 0110010110 表示需要监听第 1、2、5、7、8个文件描述符对应的 TCP 连接  
        }  
  
        ret = select(max + 1, &fdsr, NULL, NULL, NULL);  // 调用select系统函数进入内核检查哪个连接的数据到达  
  
        for(i=0;i<5;i++)  
        {  
            if(FD_ISSET(fd[i], &fdsr))      // fd_set中为1的位置表示的连接，意味着有数据到达，可以让用户进程读取  
            {  
                ret = recv(fd[i], buf,sizeof(buf), 0);  
                ......  
            }  
        }  
  }
```

在一个进程中使用 select 监控多个连接的主要步骤是：
1. 调用 socket () 函数建立主机间通信的 socket 结构体，bind () 绑定 socket 到当前服务器，listen () 监听五个 TCP 连接
2. 调用 accept () 函数建立和 5 个客户端的 TCP 连接，并把连接的文件描述符放入 fd 文件描述符队列
3. 定义一个 fd_set 类型的变量 fdsr
4. 调用 FD_ZERO，将 fdsr 所有位置 0
5. 调用 FD_SET，将 fdsr 要监听的几个文件描述符的位置 1，表示要监听这几个文件描述符指向的连接
6. 调用 select () 函数，并将 fdsr 参数传递给 select
7. select 会将 fdsr 中就绪的位置 1，未就绪的位置 0，返回就绪的文件描述符的数量
8. 当 select 返回后，调用 FD_ISSET 检测哪些位为 1，表示对应文件描述符对应的连接的数据已经就绪，可以调用 recv 函数读取该连接的数据了。

#### 2.3.1.3. select 的执行过程

在服务器进程 A 启动的时候，要监听的连接的 socket 文件描述符是 3、4、5，如果这三个连接均没有数据到达网卡，则进程 A 会让出 CPU，进入阻塞状态，同时会将进程 A 的进程描述符和被唤醒时用到的回调函数组成等待队列项加入到 socket 对象 3、4、5 的进程等待队列中，注意，这时 select 调用时，fdsr 文件描述符集会从用户空间拷贝到内核空间，如下图所示：

![image.png](https://r2.129870.xyz/img/202308080000122.png)

当网卡接收到数据，然后网卡通过中断信号通知 CPU 有数据到达，执行中断程序，中断程序主要做了两件事：
1. 将网络数据写入到对应 socket 的数据接收队列里面
2. 唤醒队列中的等待进程 A，重新将进程 A 放入 CPU 的运行队列中

假设连接 3、5 有数据到达网卡，注意，这时 select 调用结束时，fdsr 文件描述符集会从内核空间拷贝到用户空间：

![image.png](https://r2.129870.xyz/img/202308080001293.png)

#### 2.3.1.4. select 的缺点

从上面两图描述的执行过程，可以发现 select 实现多路复用有以下缺点：
1. 性能开销大
    - fd_set 的双向拷贝
        调用 select 时会陷入内核，这时需要将参数中的 fd_set 从用户空间拷贝到内核空间，select 执行完后，还需要将 fd_set 从内核空间拷贝回用户空间，高并发场景下这样的拷贝会消耗极大资源，（epoll 优化为不拷贝）。
    - 需要重新遍历文件描述符
        进程被唤醒后，不知道哪些连接已就绪即收到了数据，需要遍历传递进来的所有 fd_set 的每一位，不管它们是否就绪，（epoll 优化为异步事件通知）。
    
        select 只返回就绪文件的个数，具体哪个文件可读还需要遍历，（epoll 优化为只返回就绪的文件描述符，无需做无效的遍历）。
2. 同时能够监听的文件描述符数量太少。
    受限于 sizeof (fd_set，位图实现) 的大小，在编译内核时就确定了且无法更改。一般是 32 位操作系统是 1024，64 位是 2048。（poll、epoll 优化为适应链表方式）
    
    > [!info] 限制为 1024 的原因
    > 每次调用 select 都需要将进程加入到所有监视 socket 的等待队列，每次唤醒都需要从每个队列中移除，这里涉及了两次遍历。除此外每次都要将整个 fds 列表传递给内核，有一定的开销。正是因为遍历操作开销大，出于效率的考量，才会规定 select 的最大监视数量，默认只能监视 1024 个 socket。

### 2.3.2. poll 实现原理

和 select 类似，只是描述 fd 集合的方式不同，poll 使用 pollfd 结构而非 select 的 fd_set 结构。

```c
struct pollfd {  
    int fd;           // 要监听的文件描述符  
    short events;     // 要监听的事件  
    short revents;    // 文件描述符fd上实际发生的事件  
};
```

相比 select，poll 在以下方面作出了优化：

- 文件描述符数量不受限制
    select 使用 pollfd 结构体数组，数量只受系统内存限制，没有固定上限。
- 数据结构设计优化
    - select: 使用三个位图（可读、可写、异常）来表示关注的事件，描述符和事件类型是分离的
    - poll: 使用单一的 pollfd 结构体数组，每个结构体包含文件描述符及其关注的事件和返回的事件
- 避免描述符集的重置问题
    select 每次调用需要重新设置描述符集，而 poll 结果会单独记录不需要重新设置。
- 处理稀疏文件描述符更高效
    - select: 需要使用连续的文件描述符空间，对于稀疏的文件描述符（如只有 5、500、900 三个）也需要检查所有中间值
    - poll: 只需将关注的文件描述符放入数组，无需处理中间无用的描述符
- 更精细的事件类型区分
    - select: 只有三种事件类型：可读、可写、异常
    - poll: 提供更细粒度的事件类型，如 POLLIN（普通数据可读）、POLLOUT（可写入数据）、POLLERR（错误事件）、POLLHUP（挂起事件） 等，能更精确地指定和处理各类事件

尽管 poll 做了上述改进，但它与 select 仍然共享以下关键缺点：

- 每次调用都需要将完整的文件描述符列表从用户空间复制到内核空间
- 内核仍然需要遍历所有监视的文件描述符来查找就绪的描述符
- 对于大量文件描述符时，时间复杂度仍为 O(n)

### 2.3.3. epoll 实现原理

epoll 是对 select 和 poll 的改进，解决了 “性能开销大” 和“文件描述符数量少”这两个缺点，是性能最高的多路复用实现方式，能支持的并发量也是最大。

解决方向有：
1. 将 socket 等待队列的维护和进程阻塞进行职责分离，select低效的原因之一是将“维护等待队列”和“阻塞进程”两个步骤合二为一
2. 使用就绪列表维护数据准备好的 socket

epoll 的特点是：
1. 使用[[Excalidraw/数据结构/红黑树|红黑树]]存储**一份**文件描述符集合，每个文件描述符只需在添加时传入一次，无需用户每次都重新传入，解决了 select 中 fd_set 重复拷贝到内核的问题。
2. 通过**异步 IO 事件找到就绪的文件描述符**，而不是通过轮询的方式。
3. 使用**队列存储就绪的文件描述符**，且会按需返回就绪的文件描述符，无须再次遍历。

epoll 的基本用法是：

```c
int main(void)  
{  
      struct epoll_event events[5];  
      int epfd = epoll_create(10);         // 创建一个 epoll 对象  
      ......  
      for(i = 0; i < 5; i++)  
      {  
          static struct epoll_event ev;  
          .....  
          ev.data.fd = accept(sock_fd, (struct sockaddr *)&client_addr, &sin_size);  
          ev.events = EPOLLIN;  
          epoll_ctl(epfd, EPOLL_CTL_ADD, ev.data.fd, &ev);  // 向 epoll 对象中添加要管理的连接  
      }  
  
      while(1)  
      {  
         nfds = epoll_wait(epfd, events, 5, 10000);   // 等待其管理的连接上的 IO 事件  
  
         for(i=0; i<nfds; i++)  
         {  
             ......  
             read(events[i].data.fd, buff, MAXBUF)  
         }  
  }
```

主要涉及到三个函数：

```c
int epoll_create(int size);   // 创建一个 eventpoll 内核对象  
  
int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);   // 将连接到socket对象添加到 eventpoll 对象上，epoll_event是要监听的事件  
  
int epoll_wait(int epfd, struct epoll_event *events,  
               int maxevents, int timeout);      // 等待连接 socket 的数据是否到达
```

#### 2.3.3.1. epoll_create

epoll_create 函数会创建一个 struct eventpoll 的内核对象，类似 socket，把它关联到当前进程的已打开文件列表中。

eventpoll 主要包含三个字段：

```c
struct eventpoll {  
 wait_queue_head_t wq;      // 等待队列链表，存放阻塞的进程  
  
 struct list_head rdllist;  // 数据就绪的文件描述符都会放到这里  
  
 struct rb_root rbr;        // 红黑树，管理用户进程下添加进来的所有 socket 连接  
        ......  
}
```

- wq：等待队列
    如果当前进程没有数据需要处理，会把当前进程描述符和回调函数 default_wake_functon 构造一个等待队列项，放入当前 wq 对待队列，软中断数据就绪的时候会通过 wq 来找到阻塞在 epoll 对象上的用户进程。
- rbr：一棵红黑树
    管理用户进程下添加进来的所有 socket 连接。
- rdllist：就绪的描述符的链表
    当有的连接数据就绪的时候，内核会把就绪的连接放到 rdllist 链表里。这样应用进程只需要判断链表就能找出就绪进程，而不用去遍历整棵树。

eventpoll 的结构如图所示：

![image.png](https://r2.129870.xyz/img/202308080001530.png)

#### 2.3.3.2. epoll_ctl

epoll_ctl 函数主要负责把服务端和客户端建立的 socket 连接注册到 eventpoll 对象里，会做三件事：
1. 创建一个 epitem 对象，主要包含两个字段，分别存放 socket fd 即连接的文件描述符，和所属的 eventpoll 对象的指针
2. 将一个数据到达时用到的回调函数添加到 socket 的进程等待队列中
    注意，和阻塞 IO 模式不同的是，这里添加的 socket 的进程等待队列结构中只有回调函数，没有设置进程描述符。因为**在 epoll 中，进程是放在 eventpoll 的等待队列中**，等待被 epoll_wait 函数唤醒，而不是放在 socket 的进程等待队列中。
3. 将第 1 步创建的 epitem 对象插入红黑树

![image.png](https://r2.129870.xyz/img/202308080001994.png)

#### 2.3.3.3. epoll_wait

epoll_wait 函数的动作比较简单，检查 eventpoll 对象的就绪的连接 rdllist 上是否有数据到达，如果没有就把当前的进程描述符添加到一个等待队列项里，加入到 eventpoll 的进程等待队列里，然后阻塞当前进程，等待数据到达时通过回调函数被唤醒。

当 eventpoll 监控的连接上有数据到达时，通过下面几个步骤唤醒对应的进程处理数据：
1. socket 的数据接收队列有数据到达，会通过进程等待队列的回调函数 ep_poll_callback 唤醒红黑树中的节点 epitem
2. ep_poll_callback 函数将有数据到达的 epitem 添加到 eventpoll 对象的就绪队列 rdllist 中
3. ep_poll_callback 函数检查 eventpoll 对象的进程等待队列上是否有等待项，通过回调函数 default_wake_func 唤醒这个进程，进行数据的处理
4. 当进程醒来后，继续从 epoll_wait 时暂停的代码继续执行，把 rdlist 中就绪的事件返回给用户进程，让用户进程调用 recv 把已经到达内核 socket 等待队列的数据拷贝到用户空间使用。

![image.png](https://r2.129870.xyz/img/202308080001228.png)

### 2.3.4. 总结

从阻塞 IO 到 epoll 的实现中，我们可以看到 **wake up 回调函数机制**被频繁的使用，至少有三处地方：
1. 阻塞 IO 中数据到达 socket 的等待队列时，通过回调函数唤醒进程
2.  epoll 中数据到达 socket 的等待队列时，通过回调函数 ep_poll_callback 找到 eventpoll 中红黑树的 epitem 节点，并将其加入就绪列队 rdllist
3. 通过回调函数 default_wake_func 唤醒用户进程，并将 rdllist 传递给用户进程，让用户进程准确读取数据。

这种回调机制能够定向准确的通知程序要处理的事件，而不需要每次都循环遍历检查数据是否到达以及数据该由哪个进程处理，提高了程序效率，在日常的业务开发中，我们也可以借鉴下这一机制。

