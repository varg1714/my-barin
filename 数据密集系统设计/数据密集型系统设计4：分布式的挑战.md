#数据库 #分布式 

# 1. 基于不可靠的组件构建可靠系统

直观来看，系统的可靠性应该取决于最不可靠的组件（即最薄弱的环节）。然而，对于计算机系统来讲，事实并非如此，在低可靠性部件上构建高可靠的系统一直是计算机领域的惯用手段。例如：

- 纠错码可以在各种通信链路上准确传输数据，包括那些可能偶尔传输出错的情况，例如无线网络出现信号干扰。

- IP (Internet 协议）层本身并不可靠，可能会出现丢包、延迟、重复发送以及乱序等情况。但 TCP (传输控制协议）在 IP 之上提供了更可靠的传输层，可以保证丢失的数据包被重传，消除重复包，包的顺序以发送的顺序重新组合等。

需要指出，**系统整体虽然变得比底层组件更为可靠，但可靠性总有其现实上限**。例如，纠错码只能处理某几个位错误，但如果信号被彻底干扰，就很难保证最终可以正确接受多少数据。而 TCP 虽然能够提供重传、重组等，但是它肯定不能神奇地消除网络传输延迟。

尽管系统并不完美，但这样的高可靠系统仍然非常有用，它可以帮我们处理底层一些棘手的故障，使其他故障更容易理解和进一步处理。我们将在笫 12 章进一步探讨这个问题。

# 2. 不可靠的网络

首先要说明，无共享并不是构建集群系统的唯一方式，但它却是构建互联网服务的主流方式。主要是由于以下几个原因：由于不需要专门的硬件因此成本相对低廉，可以采用通用的商品化硬件，可以采用跨区域的多数据中心来实现高可靠性。

## 2.1. 故障的出现

互联网以及大多数数据中心的内部网络（通常是以太网）都是异步网络。在这种网络中，一个节点可以发送消息（数据包）到另一个节点，但是网络并不保证它什么时候到达，甚至它是否一定到达。发送之后等待响应过程中，有很多事情可能会出错：

1. 请求可能已经丢失（比如有人拔掉了网线）。

2. 请求可能正在某个队列中等待，无法马上发送（也许网络或接收方已经超负荷）。

3. 远程接收节点可能已经失效（例如崩溃或关机）。

4. 远程接收节点可能暂时无法响应（例如正在运行长时间的垃圾回收)。

5. 远程接收节点已经完成了请求处理，但回复却在网络中丢失（例如网络交换机配置错误）。

6. 远程接收节点已经完成了请求处理，但回复却被延迟处理（例如网络或者发送者的机器超出负荷）。

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220620222609.png)

**发送者甚至不清楚数据包是否完成了发送，只能选择让接收者来回复响应消息，但回复也有可能丢失或延迟**。这些问题在一个异步网络中无法明确区分，发送者拥有的唯一信息是，尚未收到响应，但却无法判定具体原因。

处理这个问题通常采用超时机制：在等待一段时间之后，如果仍然没有收到回复则选择放弃，并且认为响应不会到达。但是，即使判定超时，仍然并不清楚远程节点是否收到了请求（一种情况，请求仍然在某个地方排队，即使发送者放弃了，但最终请求会发送到接收者）。

## 2.2. 故障检测

许多系统都需要自动检测节点失效这样的功能。例如：

- 负载均衡器需要避免向已失效的节点继续分发请求。

- 对于主从复制的分布式数据库，如果主节点失败，需要将某个从节点提升为主节点。不过，由于网络的不确定性很难准确判断节点是否确实失效。

- 如果服务进程崩溃（或被管理员杀死），但操作系统仍正常运行，可以通过脚本通知其他节点，以便新节点来快速接管而跳过等待超时。 HBase 采用了这样的方法。

- 假设可以登录节点或者访问数据中心网络交换机，则可以通过管理接口查询是否存在硬件级别故障。不过这通常难以实现。

能快速告之远程节点的关闭状态自然有用，但也不是万能的。例如，即使 TCP 确认一个数据包已经发送到目标节点，但应用程序也可能在处理完成之前发生崩溃。如果你想知道一个请求是否执行成功，就需要应用级别的回复。

总之，如果出现了问题，你可能会在应用堆栈的某个级别拿到了一个关于错误的回复，但最好假定最终收不到任何错误报告。接下来尝试重试 (TCP 重试是透明的，但也可以在应用级别重试），等待超时之后，如果还是没有收到响应，则最终声明节点已经失效。

## 2.3. 超时与无期限的延迟

如果超时是故障检测唯一可行的方法，那么超时应该设多长呢？不幸的是没有标准的答案。
设置较长的超时值意味着更长时间的等待，才能宣告节点失效（在此期间，用户只能等待或者拿到错误信息）。较短的超时设置可以帮助快速检测故障，但可能会出现误判，例如实际上节点只是出现暂时的性能波动（由于节点或网络上的高负载峰值），结果却被错误地宣布为失效。

当一个节点被宣告为失效，其承担的职责要交给到其他节点，这个过程会给其他节点以及网络带来额外负担，特别是如果此时系统已经处于高负荷状态。例如节点只是负载过高而出现了响应缓慢，转移负载到其他节点可能会导致失效扩散，产生级联扩大效应，在极端情况下，所有节点都宣告对方死亡，造成服务处于事实停止状态。

异步网络的延迟理论无限大，多数服务端也无法保证在给定时间内完成请求处理。如果超时设置太小，只需要一个短暂的网络延迟尖峰就会导致包超时进而将系统标记为失效。

## 2.4. 网络拥塞与排队

计算机网络上数据包延的变化根源往往在于排队：数据包通过网络交换机发送可能排队，中途由于 TCP 流量控制会排队，数据包到达了对方操作系统后由于繁忙也可能排队，若处理程序处于虚拟机上还会由于调度而排队。因此排队情况出现在网络交互的各个场景中。

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220620224732.png)

所有以上因素都会造成网络延迟的变化或者不确定性。当系统还有足够的处理能力，排队之后可以快速处理；但当系统接近其最大设计上限时，系统负载过高，队列深度就会显著加大，排队对延迟的影响变得特别明显。

在这种环境下，只能通过实验方式一步步设置超时。先在多台机器上，多次测量网络往返时间，以确定延迟的大概范围；然后结合应用特点，在故陈检测与过早超时风险之间选择一个合适的中间值。

更好的做法是，超时设置并不是一个不变的常量，而是持续测批响应时间及其变化 （抖动），然后根据最新的响应时间分布来自动调整。可以用 Pci Accrual 故障检测器完成，该检测器目前已在 Akka 和 Cassandra 中使用。TCP 超时重传也采用了类似的机制。

# 3. 不可靠的时钟

时钟和计时非常重要。有许多应用程序以各种方式依赖于时钟：

1. 某个请求是否超时了？
2. 某项服务的 99%的响应时间是多少？
3. 在过去的五分钟内，服务平均每秒处理多少个查询？
4. 用户在我们的网站上浏览花了多长时间？
5. 这篇文章什么时候发表？
6. 在什么时间发送提醒邮件？
7. 这个缓存条目何时过期？
8. 日志文件中错误消息的时间戳是多少？

上述 1~4 测量的持续时间（例如请求发送与响应接收之间的时间间隔），而 5~8 所描述的某个时间点（在特定日期，特定时间发生的事件）。

在分布式系统中，时间总是件棘手的问题，由于跨节点通信不可能即时完成，消息经由网络从一台机器到另一台机器总是需要花费时间。收到消息的时间应该晚于发送的时间，但是由于网络的不确定延迟，精确测量面临着很多挑战。这些情况使得多节点通信时很难确定事情发生的先后顺序。

而且，网络上的每台机器都有自己的时钟硬件设备，通常是**石英晶体振荡器**。这些设备并非绝对准确，即每台机器都维护自己本地的时间版本，可能比其他机器稍快或更慢。可以在一定程度上同步机器之间的时钟，最常用的方法是网络时间协议 (Network Time Protocol, NTP) , 它可以根据一组专门的时间服务器来调整本地时间, 时间服务器则从精确更高的时间源（如 GPS 接收机）获取高精度时间。

## 3.1. 单调时钟与墙上时钟

现代计算机内部至少有两种不同的时钟： 一个是墙上时钟（或称钟表时间），一个是单调时钟。

### 3.1.1. 墙上时钟

墙上时钟根据某个日历（也称为墙上时间）返回当前的日期与时间。例如，Linux 的 clock_gettime (CLOCK_REALTIME) 注 5 和 Java 中的 System. currentTimeMillis () 会返回自纪元 1970 年 1 月 1 日 (UTC) 以来的秒数和毫秒数，不含闰秒。而有些系统则使用其他日期作为参考点。

墙上时钟可以与 NTP 同步。但是，如下一节所述，这里还存在一些奇怪问题。特别是，**如果本地时钟远远快于 NTP 服务器，强行重置之后会跳回到先前的某个时间点**。这种跳跃以及经常忽略闰秒，导致其不太适合测量时间间隔。

### 3.1.2. 单调时钟

单调时钟更适合测晕持续时间段（时间间隔），例如超时或服务的响应时间： Linux 上的 clock_gettime (CLOCK_MONOTONIC) 和 Java 中的 System. nanoTime () 返回的即是单调时钟。单调时钟的名字来源于它们保证总是向前（而不会出现墙上时钟的回拨现象）。

注意，单调时钟的绝对值并没有任何意义，它可能电脑启动以后经历的纳秒数或者其他含义。因此**比较不同节点上的单调时钟值毫无意义，它们没有任何相同的基准**。

## 3.2. 时钟同步与准确性

单调时钟不需要同步，但是墙上时钟需要根据 NTP 服务器或其他外部时间源做必要的调整。然而，我们获取时钟的方法并非预想那样可靠或准确，硬件时钟和 NTP 可能会出现一些莫名其妙的现象：

- 计算机中的石英钟不够精确，存在漂移现象。

- 如果时钟与 NTP 服务器的时钟差别太大，可能会出现拒绝同步，或者本地时钟将被强制重置。在重置前后应用程序观察可能会看到时间突然倒退或突然跳跃的现象。

- 由于某些原因，如果与 NTP 服务器链接失败（如防火墙），可能会很长一段时间没有留意到错误配置最终导致同步失败。

- NTP 同步会受限于当时的网络环境特别是延迟，如果网络拥塞、数据包延迟变化不定，则 NTP 同步的准确性会受影响。

- 一些 NTP 服务器本身出现故障、或者配置错误，其报告的时间可能存在数小时的偏差。

- 闰秒会产生一分钟为 59 秒或 61 秒的现象，这会在使一些对闰秒毫无防范的系统出现混乱。

- 在虚拟机中，由于硬件时钟也是被虚拟化的，这对于需要精确计时的应用程序提出了额外的挑战。

### 3.2.1. 依赖同步的时钟

如果应用需要精确同步的时钟，最好仔细监控所有节点上的时钟偏差。如果某个节点的时钟漂移超出上限，应将其宣告为失效，并从集群中移除。这样的监控的目的是确保在造成重大影响之前尽早发现并处理问题。

#### 3.2.1.1. 时间戳与事件顺序

对于一个常见的功能：跨节点的事件排序，如果它高度依赖时钟计时，就存在一定的技术风险。例如，两个客户端同时写入分布式数据库，谁先到达？哪一个操作是最新的呢？

如果使用时间戳进行时间顺序的判断，那么当出现时钟漂移情况时事件的处理顺序可能并不是期待的顺序。例如在[[数据密集系统设计/数据密集型系统设计2：复制与分区#1 6 3 1 最后写入者获胜|最后写入者获胜]]的做法中，若出现此类情况，则会造成数据的一致性问题。

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220620230444.png)

上图给出了这样的危险例子，即多主节点复制的分布式数据库高度依赖于墙上时钟。客户端 A 在节点 1 上写入 X =1, 写入被复制到节点 3; 客户端 B 在节点 3 上增加 X (现在 X = 2)。最后，这两个写入都被复制到节点 2 上。

但是，这样的时间戳却不能正确排序事件： 写入 X= 1 的时间戳为 42.004 秒，写入 x=2 虽然后续发生，时间戳却是 42.003s。当节点 2 收到这两个事件时，会根据时间戳错误地判断 X= 1 是最新值，然后决定丢弃 X= 2, 就给就导致客户端 B 的增批操作丢失。

因此，通过保持最新值并丢弃其他值来解决冲突看似不错，但要注意，最新的定义如果取决于墙上时钟就会引入偏差。对于排序来讲，基于递增计数器而不是振荡石英晶体的逻辑时钟是更可靠的方式。逻辑时钟并不测量一天的某个时间点或时间间隔，而是事件的相对顺序（事件发生的相对前后关系）。与之对应的，墙上时钟和单调时钟都属于物理时钟。

#### 3.2.1.2. 时钟的置信区间

墙上时钟会返回微秒甚至纳秒级别的信息，但是这种精度的测晕值其实并不可信。因此，我们不应该将时钟读数视为一个精确的时间点，而更应该视为带有置信区间的时间范围。例如，系统可能有 95%的置信度认为目前时间介于 10.3 ~10.5 秒之间。**如果我们可完全相信的精度为+/-100 毫秒，那么时间戳中那些微秒级的读数并无实际意义**。

这里有趣的是 Google Spanner 中的 TrueTime API, 它会明确地报告本地时钟的置信区间。当查询当前时间时，你会得到两个值：［不早于，不晚于］分别代表误差的最大偏差范围。基于上述两个时间戳，可以知道实际时间应在其范围之内。该间隔的范围主要取决于本地石英钟最后与高精时钟源同步后所经历的时间长短。

### 3.2.2. 全局快照的同步时钟

在事务的 MVCC 实现[[数据密集系统设计/数据密集型系统设计3：事务#1 2 2 快照隔离级别与可重复读|快照隔离与可重复读]]实现中，常见的快照隔离实现中需要单调递增事务 ID。如果写入发生在快照之后（即写入具有比快照更大的事务 ID) , 那么该写入对于快照不可见。在单节点数据库上，一个简单的计数器足以生成事务 ID。

当数据库分布在多台机器上（可能跨越多个数据中心）时，由于需要复杂的协调以产生全局的、单调递增的事务 ID (跨所有分区）。事务 ID 要求必须反映因果关系：事务 B 如果要读取事务 A 写入的值，则 B 的事务 ID 必须大于 A 的事务 ID, 否则快照将不一致。考虑到大量、频繁的小数据包，在分布式系统中创建事务 ID 通常会引入瓶颈。

能否使用同步后的墙上时钟作为事务 ID 呢？**如果我们能够获得足够可靠的同步时钟，自然它可以符合事务 ID 属性要求**： 后发生的事务具有更大的时间戳。然而问题还是时钟精度的不确定性。

Google Spanner 采用以下思路来实现跨数据中心的快照隔离。它根据 TrueTime API 返回的时钟置信区间，并基于以下观察结果：如果有两个置信区间，每个置信区间都包含最早和最新可能的时间戳 (A = \[ Aearliest, Alatest]和 B = \[ Bearliest , Blatest])，且这两个区间没有重叠（即 Aearliest < Alatest < Bearliest < Blatest) 后。只有发生了重叠， A 和 B 发生顺序才无法明确。

借助时钟同步来处理分布式事务语义是一个非常有趣和活跃的研究领域, 但除了 Google 以外，目前主流数据库中还没有更多的实现。

还有一种分布式序列号生成器，例如 Twitter 的 Snowflake, 它用更为扩展的方式（例如将 ID 空间划分不同的范围，然后分配给不同的节点）未生成近似单调递增的唯一 ID。但通常无法保证[[数据密集型系统设计5：一致性与共识#1.2. 顺序保证|与因果关系一致的顺序]]。

## 3.3. 进程暂停

对于许多基于时间判断的操作，通常是判断某个时间或者时间间隔是否符合条件。一种危险的情况是**条件满足后执行语句时花费过长时间或者进程暂停引起的时间消耗而导致的时间判断失效**。

这种场景出现不止在单机系统中，在分布式系统中更为容易出现，各个节点时钟的不同步以及网络各种问题等加大了这种风险。特别是本身对于时间敏感的操作，如时间跳跃导致的[[数据库/Redis大纲#2 4 3 2 RedLock 失效场景之时钟跳跃|锁失效场景]]。

那么，一个线程可能会暂停这么长时间么？这是可能的，发生这种情况的原因有很多种：

- 编程语言特性决定

	许多编程语言（如 Java 虚拟机）都有垃圾收集器 (GC) , 有时运行期间会暂停所有正在运行的线程。这些 GC 暂停甚至有时会持续数分钟！即使像 `HotSpot JVM CMS` 所谓的并发垃圾收集器也不能完全与应用代码并行运行，需要时不时地停止活动的线程。通过改变分配模式或调整 GC 某些参数可以减少一些暂停, 但是我们还是要防范最差情况以提供可靠的保证。

	现在一个较新的想法是把 GC 暂停视为节点的一个计划内的临时离线，当节点启动垃圾回收时，通知其他节点来接管客户端的请求。此外，系统可以提前为前端应用发出预警，应用会等待当前请求完成，但停止向该节点发送新的请求，这样垃圾回收可以在无干扰的情况下更加高效运行。这个技巧以某种方式对客户端隐藏垃圾回收，降低负面影响。目前一些对延迟敏感的系统已经采用了该方法。
	
	该方法的一个变种是，只对短期对象（可以快速回收）执行垃圾回收，然后在其变成长期存活对象之前，采取定期重启的策略从而避免对长期存活对象执行全面回收。每次选择一个节点重新启动，在重启之前，重新平衡节点之间的流量，思路与滚动升级类似。

- 虚拟机调度场景

	在虚拟化环境中，可能会暂停虚拟机（暂停所有执行进程并将内存状态保存到磁盘）然后继续（从内存中加载数据然后继续执行）。暂停可能发生进程运行的任一时刻，并且可能持续很长的时间。该功能通常用于实时迁移，即把虚拟机从一个主机迁移到另一个主机而不需要重启，这种情况下，暂停的长度主要取决于进程写入内存的速率。

- 操作系统调度

	当操作系统执行线程上下文切换时，或者虚拟机管理程序切换到另一个虚机时，正在运行的线程可能会在代码的任意位置被暂停。在虚拟机环境中，这种被其他虚拟机中断的 CPU 时间称为窃取时间。如果机器负载很高（即等待运行的线程很长），被暂停的线程可能需要一段时间之后才能再次运行。

- 磁盘 I/O 等待

	如果应用程序执行同步磁盘操作，则线程可能暂停并等待磁盘 I/O 完成。在许多语言中，即使代码并没有明确执行文件操作，也可能意外引入磁盘 I/O。例如， Java 类加载器在第一次使用类文件时会推迟加载，最终可能发生在执行时的任何时刻。I/O 暂停和 GC 暂停甚至可能会同时发生，从而进一步恶化情况。如果磁盘其实是个网络文件系统或网络块设备（如亚马逊的 EBS) , I/O 还要受到网络延迟变化的影响。

- 操作系统缺页

	如果操作系统配置了基于磁盘的内存交换分区，内存访问可能触发缺页中断，进而需要从磁盘中加载内存页。I/O 进行时（通常比较慢）线程为暂停。如果内存使用压力很大，还可能迫使更多的页面换出到磁盘。极端的情况下，操作系统可能会花费大量时间在页面换入换出上，而实际工作完成很少（所谓的颠簸）。为了避免此类问题，通常在服务器上禁用分页（宁愿杀死一些进程来释放内存而不是反复抖动）。

- 操作系统命令控制

	通过发送 SIGSTOP 信号来暂停 UNIX 进程，例如在 shell 中按下 Ctrl-Z。这个信号会立即停止进程避免其拿到更多的 CPU 周期，直到接下来收到信号 SIGCONT 之后才从停止的地方继续运行。另外也不排除 SIGSTOP 信号是由运维人员不小心意外发送。

- 人为控制

	运行在终端用户设备（如笔记本电脑）时，执行也可能发生暂停，例如用户关闭了笔记本电脑或休眠。

**所有上述情况都可能随时抢占一个正在运行的线程，然后在之后的某个时间点再恢复线程的执行，而线程自身却对此一无所知**。这个问题类似于在一台机器上运行多线程代码且保证线程安全。总之，你不能假定任何有关时间的事情，记住上下文切换和并行性可能随时可以发生。

# 4. 知识真相与谎言-分布式的共识与错误

分布式系统与单节点程序存在许多不同之处。例如，很少使用共享内存，通过不可靠网络传递消息且延迟不确定，可能遭受部分失效，不可靠的时钟以及进程暂停等。

网络中的一个节点无法确信信息，只能通过网络收到（或没有收到）的消息来猜测。节点只能通过消息交换来获得其他节点当前的状态（存储了哪些数据，是否正常工作等）。如果远程节点没有响应，由于没法区分网络本身的问题还是节点的问题，就无从知道节点究竟处于什么状态。

## 4.1. 真相由多数决定

节点不能根据自己的信息来判断自身的状态。由于节点可能随时会失效，可能会暂停－假死，甚至最终无法恢复，因此，分布式系统不能完全依赖于单个节点。目前，许多分布式算法都依靠法定票数，即在[[数据密集系统设计/数据密集型系统设计2：复制与分区#1 6 2 读写 quorum|节点之间进行投票]]。任何决策都需要来自多个节点的最小投票数，从而减少对特定节点的依赖。

这其中包括关于宣告节点失效的决定。如果有法定数量的节点声明另一个节点失效，即使该节点仍感觉活得很自在，那它也必须接受失效的裁定，所有个体节点必须遵循法定投票的决议然后离线。

最常见的法定票数是取系统节点半数以上（也有其他类型的法定人数）。如果某些节点发生故障， quorum 机制可以使系统继续工作（对于三个节点的系统，可以容忍一个节点失效；五个节点则可以容忍两个节点故障）。由于系统只可能存在一个多数，绝不会有两个多数在同时做出相互冲突的决定，因此系统的决议是可靠的， quorum 的使用还体现在[[数据密集型系统设计5：一致性与共识#1.1.4.1. 线性化与 quorum|一致性算法中]]。

## 4.2. 错误修复

有很多情况，我们需要在系统范围内只能有一个实例，例如主从模型中的主节点，互斥资源中的锁。当多数节点声明节点已失效，而该节点还继续充当唯一的那个实例时，如果系统设计不周就会导致负面后果。该节点会按照自认为正确的信息向其他节点发送消息，其他节点如果还选择相信它，那么系统就会出现错误的行为。

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220620233959.png)

这个问题属于前面进程暂停中的一种情况：持有租约的客户端被暂停太久直到租约到期。然后另一个客户端已经获得了文件的锁租约，并开始写文件。接下来，当暂停的客户端重新回来时，它仍然（错误地）认为合法持有锁并尝试写文件。结果导致客户 2 的文件写入被破坏。

当使用锁和租约机制来保护资源的并发访问时, 必须确保过期的唯一的那个节点不能影响其他正常部分。要实现这一目标，可以采用一种相当简单的技术 fencing (栅栏，隔离之意）。

我们假设每次锁服务在授予锁或租约时，还会同时返回一个 fencing 令牌，该令牌（数字）每授予一次就会递增（例如，由锁服务增加）。然后，要求客户端每次向存储系统发送写请求时，都必须包含所持有的 fencing 令牌。

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220620234121.png)

请注意，**只靠客户端自己检查锁状态是不够的，这种机制要求资源本身必须主动检查所持令牌信息**，如果发现已经处理过更高令牌的请求，要拒绝持有低令牌的所有写请求。如果资源不支持额外的令牌检查，可以采取一些临时技巧来绕过去（例如，对于访问文件存储服务的情况，可以将令牌信息内嵌在文件名中）。总之，为了避免在锁保护之外发生请求处理，需要进行额外的检查机制。

在服务器端检查令牌可能看起来有些复杂，但其实是推荐的正确做法： 系统服务不能假定所有的客户端都表现符合预期，事实上客户端通常由权限级别相对较低的人来操作运行, 因此存在一定的误用、滥用风险，从安全角度讲，服务端必须防范这种来自客户端的滥用。

## 4.3. 拜占庭故障

fencing 令牌可以检测并阻止那些无意的误操作（例如节点并没有发现其租约已经过期）。但是，如果节点故意试图破坏系统，在发送消息时可以简单地伪造令牌即可。

我们总是假设节点虽然不可靠但一定是诚实的： 它们尽管运行很慢或者由于故障而无法响应，或者状态可能已经过期（例如由于 GC 暂停或网络延迟），但一且做出了响应，则一定是完全基于其所知的全部信息和事先协议约定好的行为准则，响应代表了其所知的真相。

如果节点存在撒谎的情况（即故意发送错误的或破坏性的响应），那么分布式系统处理的难度就上了一个台阶。例如，节点明明没有收到某条消息，但却对外声称收到了。这种行为称为拜占庭故障，在这样不信任的环境中需要达成共识的问题也被称为拜占庭将军问题。

如果某个系统中即使发生部分节点故障，甚至不遵从协议，或者恶意攻击、干扰网络，但仍可继续正常运行，那么我们称之为拜占庭式容错系统。这些担忧在某些特定场景是合理的。
