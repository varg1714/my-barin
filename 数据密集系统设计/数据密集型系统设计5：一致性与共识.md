#数据库 #分布式 #分布式事务 #分布式一致性

# 1. 一致性保证

本章我们将讨论构建容错式分布式系统的相关算法和协议。这里假设[[数据密集系统设计/数据密集型系统设计4：分布式的挑战|分布式系统中所有的故障]]都可能发生，这包括网络数据包可能会丢失、顺序紊乱、重复发送或延迟，时钟也有一定偏差，节点可能发生暂停（例如由于垃圾回收）甚至随时崩溃。

我们尝试建立可以让分布式应用忽略内部各种问题的抽象机制。例如，**分布式系统最重要的抽象之一就是共识：所有的节点就某一项提议达成一致**。通过本章的介绍，最后你会发现面对各种网络故障和进程失效，可靠地达成共识是一件多么了不起的事情。一旦解决了共识问题，就可以服务于应用层很多的目标需求。

大多数多副本的数据库都至少提供了最终的一致性，这意味着如果停止更新数据库，并等待一段时间（长度未知）之后，最终所有读请求会返回相同的内容。换言之，**最终一致性意味着收敛，即预期所有的副本最终会收敛到相同的值**。

但是，这是一个非常弱的保证，它无法告诉我们系统何时会收敛。而**在收敛之前，读请求可能会返回任何值甚至读失败**。例如，如果完成一笔更新操作之后立即读取，由于读取可能会路由到不同的副本，系统不保证一定读到刚刚写入的值。对于应用开发入员而言，最终一致性会带来很大的处理挑战，这与普通的单线程程序中变量读写行为大相径庭。 

## 1.1. 可线性化

在最终一致性数据库中，同时查询两个不同的副本可能会得到两个不同的答案。这会使应用层感到困惑。如果数据库能够对上层提供只有单个副本的假象，情况会不会大为简化呢？这样让每个客户端都拥有相同的数据视图，而不必担心复制滞后。这就是可线性化 (也称为原子一致性, 强一致性等) 的思想。 

线性化的确切定义比较微妙，其基本的想法是**让一个系统看起来好像只有一个数据副本，且所有的操作都是原子的**。有了这个保证，应用程序就不需要关心系统内部的多个副本。

一个非线性化的例子如下，后发生的请求看到了更老的数据：

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220627002753.png)

### 1.1.1. 如何达到线性化？

可线性化背后的基本思想很简单：使系统看起来好像只有一个数据副本。 

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220627003008.png)

在一个可线性化的系统中，在写操作的开始与结束之间必定存在某个时间点，x 的值发生了从 0 到 1 的跳变。如果某个客户端的读取返回了新值 1，即使写操作尚未提交，那么所有后续的读取也必须全部返回新值。

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220627003126.png)

上图中的每个操作都有一条竖线，表示可能的执行时间点。这些标记以前后关系依次连接起来，最终的结果必须是一个有效的寄存器读写顺序，即每个读操作须返回最近写操作所设置的值。

可线性化要求，如果连接这些标记的竖线，它们必须总是按时间箭头（从左到右）向前移动，而不能向后移动。这个要求确保了之前所讨论的就近性保证： **一且新值被写入或读取，所有后续的读都看到的是最新的值，直到被再次覆盖。**

以上就是线性化背后的直觉含义。通过记录所有请求和响应的时序，然后检查它们是否可以顺序排列，可以用来测试系统是否可线性化（这里存在额外的计算开销）。

### 1.1.2. 可线性化与可串行化

可线性化 (Linearizability) 非常容易与[[数据密集系统设计/数据密集型系统设计3：事务#1 3 串行化|可串行化 (Serializability) ]] 发生混淆。两个词似乎都在表达类似可以按顺序排列的意思。但是它们完全不同，需要仔细区分。

- 可串行化
	可串行化是事务的隔离属性，其中每个事务可以读写多个对象（行，文档，记录等）。它**用来确保事务执行的结果与串行执行（即每次执行一个事务）的结果完全相同，即使串行执行的顺序可能与事务实际执行顺序不同**。
- 可线性化
	**可线性化是读写寄存器（单个对象）的最新值保证**。它并不要求将操作组合到事务中，因此无法避免[[数据密集系统设计/数据密集型系统设计3：事务#1 2 4 写倾斜与幻读|写倾斜]]等问题。除非采取其他额外措施（如实现[[数据密集系统设计/数据密集型系统设计3：事务#1 2 4 4 实体化冲突|实体化冲突]])。

数据库可以同时支持可串行化与线性化，这种组合又被称为严格的可串行化或者强的单副本可串行化 (strong one-copy serializabili ty, strong-lSR)。基于[[数据密集系统设计/数据密集型系统设计3：事务#1 3 2 两阶段加锁|两阶段加锁]]或者[[数据密集系统设计/数据密集型系统设计3：事务#1 3 1 实际串行化执行|实际以串行执行]]都是典型的可线性化。

但是，[[数据密集系统设计/数据密集型系统设计3：事务#1 3 3 可串行化的快照隔离|可串行化的快照隔离]]则不是线性化的：按照设计，它可以从一致性快照中读取，以避免读、写之间的竞争。**一致性快照的要点在于它里面不包括快照点创建时刻之后的写入数据**，因此从快照读取肯定不满足线性化。

### 1.1.3. 线性化的依赖条件

那什么情况下应该使用线性化呢? 上面足球比赛比分的例子只是个最简单的情况，结果存在几秒的延迟通常不会造成实质的伤害。然而，在有些场景下，线性化对于保证系统正确工作至关重要。

#### 1.1.3.1. 加锁与主节点选举

主从复制的系统需要确保有且只有一个主节点，否则会产生脑裂。选举新的主节点常见的方法是使用锁：即每个启动的节点都试图获得锁，其中只有一个可以成功即成为主节点。不管锁具体如何实现，它必须满足可线性化: **所有节点都必须同意那个节点持有锁**，否则就会出现问题。

提供协调者服务的系统如 Apache ZooKeeper 等通常用来实现分布式锁和主节点选举。它们都使用了支持容错的共识算法确保可线性化。归根结底，**线性化存储服务是所有这些协调服务的基础**。

#### 1.1.3.2. 约束与唯一性保证

唯一性约束在数据库中很常见。例如，用户名或电子邮件地址必须唯一标识一个用户，文件存储服务中两个文件不能具有相同的路径和文件名。如果要在写入数据时强制执行这些约束，则也需要线性化。

这种情况本质上与加锁非常类似: 用户注册等同于试图对用户名进行加锁操作。该操作也类似于原子比较和设置: 如果当前用户名尚未被使用，就设置用户名与客户 ID 进行关联。

当然在某些实际场合中，有时可以放宽这些限制。然而，硬性的唯一性约束，常见如关系型数据库中主键的约束，则需要线性化保证。其他如外键或属性约束，则并不要求一定线性化。

#### 1.1.3.3. 跨通道的时间依赖

在上面的例子中，如果 Alice 没有高呼比分， Bob 可能就不会知道他的查询结果是过期的。或许他会在几秒之后再次刷新页面，然后看到最终的比分。**线性化违例之所以被注意到，是因为系统中存在其他的通信渠道**（例如， Alice 对 Bob 发出的声音来传递信息）。

计算机系统也会出现类似的情况。例如，用户可以上传照片到某网站，有一个后台进程将照片调整为更低的分辨率 (即缩略图) 以方便更快下载。

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220627205827.png)

如果文件存储服务是可线性化的，那么系统应该可以正常工作。否则，这里就会引入竞争条件风险：消息队列可能比存储服务内部的复制执行更快。在这种情况下，当调整模块在读取图像 (步骤 5) 时，可能会看到图像的某个旧版本，或者根本读不到任何内容。

之所以出现这个问题是因为 Web 服务器和调整模块之间**存在两个不同的通信通道**: 文件存储器和消息队列。如果没有线性化的就近性保证，这两个通道之间存在竞争条件。

线性化并非避免这种竞争的唯一方法，但却是最容易理解的。如果可以控制某一个通信通道 (例如消息队列，但注意不适合 Alice 和 Bob 的例子，因为后者并非计算机系统)，可以尝试[[数据密集系统设计/数据密集型系统设计2：复制与分区#1 4 复制滞后问题|读自己的写]]方法，但会引入额外的复杂性。

### 1.1.4. 实现线性化系统

由于线性化本质上意味着表现得好像只有一个数据副本，且其上的所有操作都是原子的。所以最简单的方案自然是只用一个数据副本，但显然，该方法无法容错。

**系统容错最常见的方法就是采用复制机制**。对于多种的复制方式，我们看看哪些满足可线性化：

- 主从复制（部分支持可线性化）
	在主从复制的系统中只有主节点承担数据写入，从节点则在各自节点上维护数据的备份副本。如果从主节点或者同步更新的从节点上读取，则可以满足线性化。但并非每个主从复制的具体数据库实例都是可线性化的，主要是因为它们可能采用了快照隔离的设计，或者实现时存在并发方面的 bug。

	而从主节点上读取的前提是你确定知道哪个节点是主节点。正如在[[数据密集系统设计/数据密集型系统设计4：分布式的挑战#4 1 真相由多数决定|真相由多数决定]]中所讨论的，某节点可能自认为是主节点，但事实并非如此，这个自以为是的主节点如果对外提供服务，就会违反线性化。如果使用了异步复制，故障切换过程中甚至可能会[[数据密集系统设计/数据密集型系统设计2：复制与分区#1 2 3 1 数据丢失问题|丢失一些已提交的写入]]，结果是同时违反持久性和线性化。
- 共识算法（可线性化）
	我们本章稍后即将讨论的一些共识算法，与主从复制机制相似。不过共识协议通常内置一些措施来防止脑裂和过期的副本。正是由于这些专门的设计，共识算法可以安全地实现线性化存储，这些系统包括 Zookeeper 和 etcd 等。
- 多主复制（不可线性化）
	具有多主节点复制的系统通常无法线性化的，主要由于它们同时在多个节点上执行并发写入，并将数据异步复制到其他节点。因此它们可能会产生冲突的写入，需要额外的解决方案。
- 无主复制（可能不可线性化）
	对于无主节点复制的系统有些人认为只要配置法定读取和写入满足 ($w+r>n$) 就可以获得强一致性。但这完全取决于具体的 quorum 的配置，以及如何定义强一致性，它可能并不保证线性化。

	例如基于墙上时钟依赖于同步的时钟的最后写入获胜冲突解决方法几乎肯定是非线性化，因为这种时间戳无法保证与实际事件顺序一致（例如由于时钟偏移）。不规范的 quorum 也会破坏线性化。甚至即使是严格的 quorum，正如之后即将介绍的，也会发生违背线性化的情况。

#### 1.1.4.1. 线性化与 quorum

直觉上，对于 Dynamo 风格的复制模型，如果读写遵从了严格 quorum, 应该是可线性化的。然而如果遭遇不确定的网络延迟，就会出现竞争条件。

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220627213712.png)

x 的初始值为 0, 写客户端向所有三个副本 (n = 3, w = 3) 发送写请求将 x 更新为 1。与此同时，客户端 A 从两个节点 (r = 2) 读取数据，然后在其中一个节点上看到新值 1。与此同时，客户端 B 从两个节点的读取，两者在都返回了旧值 0。

我们发现它虽然满足了仲裁条件 ($w+r>n$), 但很明显这不是线性化的： B 的请求在 A 的请求完成之后才开始， A 返回了新值，但 B 却得到了旧值。这又类似 Alice 和 Bob 的情况。

有趣的是，**可以使 Dynamo 风格的复制系统以牺牲性能为代价来满足线性化： 读操作在返回结果给应用之前，必须同步执行[[数据密集系统设计/数据密集型系统设计2：复制与分区#1 6 1 1 读修复与反熵|读修复]]，而写操作在发送结果之前，必须读取 quorum 节点以获取最新值**。

此外，**这种方式只能实现线性化读、写操作，但无法支持线性化的比较和设置操作，后者需要共识算法的支持**。总而言之，最安全的假定是类似 Dynamo 风格的无主复制系统无法保证线性化。

### 1.1.5. 线性化的代价

由于有一部分复制方案能够保证线性化，而其他则无法保证，因此有必要更加深入地探讨线性化的优缺点。

多主复制非常适合多数据中心，如果两个数据中心之间发生网络中断，每个数据中心内都可以继续正常运行： 由于从一个数据中心到另一个数据中心的复制是异步，期间发生的写操作都暂存在本地队列，等网络恢复之后再继续同步。

与之对比，如果是主从复制，所有写请求和线性化读取都必须发送给主节点。因此，对于这样的主从复制系统，数据中心之间的网络一且中断，连接到从数据中心的客户端无法再联系上主节点，也就无法完成任何数据库写入和线性化读取。从节点可以提供读服务，但内容可能是过期的（非线性化保证）。所以，如果应用程序要求线性化读写，则网络中断一定会违背这样的要求。

#### 1.1.5.1. CAP 理论

不仅仅是主从复制和多主复制才有上面的问题，无论如何实现，任何可线性化的数据库都有这样问题。事实上，这个问题也不局限于多数据中心部署的情况，即使在一个数据中心内部，只要有不可靠的网络，都会发生违背线性化的风险。我们可以做以下的权衡考虑。

- 如果应用要求线性化，但由于网络方面的问题，某些副本与其他副本断开连接之后无法继续处理请求，就必须等待网络修复，或者直接返回错误。无论哪种方式，结果是服务不可用。

- 如果应用不要求线性化，那么断开连接之后，每个副本可独立处理请求例如写操作（多主复制）。此时，服务可用，但结果行为不符合线性化。

因此，不要求线性化的应用更能容忍网络故障。这种思路通常被称为[[分布式/分布式大纲#2 2 分布式事务缺陷之 CAP 定理|CAP定理]]。正式定义的 CAP 定理范围很窄，它只考虑了一种一致性模型（即线性化）和一种故障（网络分区节点仍处于活动状态但相互断开），而没有考虑网络延迟、节点失败或[其他需要折中的情况](https://blog.the-pans.com/cap/)。因此，尽管 CAP 在历史上具有重大的影响力，但对于一个具体的系统设计来说，它可能没有太大的实际价值。

分布式系统中还有很多有趣的研究结果，目前 CAP 已被更精确的研究成果所取代。

#### 1.1.5.2. 可线性化与网络延迟

虽然线性化是个很有用的保证，但实际上很少有系统真正满足线性化。例如，现代多核 CPU 上的内存甚至就是非线性化的，如果某个 CPU 核上运行的线程修改一个内存地址，紧接着另一个 CPU 核上的线程尝试读取，则系统无法保证可以读到刚刚写入的值，除非使用了内存屏障或 fence 指令。

为什么这样呢？首先， CAP 理论不适用于当今的多核－内存一致性模型：在计算机内部，我们通常假设通信是可靠的，例如我们不会假定一个 CPU 核在与其他核断开之后还能安然工作。**之所以放弃线性化的原因就是性能，而不是为了容错**。

许多分布式数据库也是类似，它们选择不支持线性化是为了提高性能，而不是为了保住容错特性。无论是否发生了网络故障，线性化对性能的影响都是巨大的。

那我们是否能找到一个更有效的线性化实现方案呢？目前看来答案是否定的。 Attiya 和 Welch 证明**如果想要满足线性化，那么读、写请求的响应时间至少要与网络中延迟成正比**。考虑到多数计符机网络高度不确定的网络延迟，线性化读写的性能势必非常差。

## 1.2. 顺序保证

我们之前曾说过，线性化寄存器对外呈现的好像只有一份数据拷贝，而且每一个操作似乎都是原子性生效。这意味着操作是按照某种顺序执行。

顺序是本书反复出现的主题，事实证明，排序、可线性化与共识之间存在着某种深刻的联系。之所以反复出现顺序问题，其中的一个原因是它**有助于保持因果关系**。

因果关系对所发生的事件施加了某种排序：发送消息先于收到消息；问题出现在答案之前等，或者就像在现实生活中一样，一件事情会导致另一件事情：一个节点根据读取的数据做出决定，然后写入结果，另一个节点读取写入的结果之后再写入新的内容，诸如此类。这些**因果关系的依赖链条定义了系统中的因果顺序，即某件事应该发生另一件事情之前**。

如果系统服从因果关系所规定的顺序，我们称之为因果一致性。例如，快照隔离提供了因果一致性： 当从数据库中读数据时，如果查询到了某些数据，也一定能看到触发该数据的前序事件（假设期间没有发生删除操作）。

### 1.2.1. 顺序与因果关系

#### 1.2.1.1. 因果关系并非全序

全序关系支持任何两个元素之间进行比较，即对于任意两个元素，总是可以指出哪个更大，哪个更小。 

但是，有些集合并不符合全序，例如集合 $\{a, b\}$ 大于集合 $\{b, c\}$ 么？因为它们都不是对方的子集，所以无法直接比较它们。我们称之为不可比较，数学集合只能是偏序。

全序和偏序的差异也会体现在不同的数据库一致性模型中：

- 可线性化
	在一个可线性化的系统中，存在全序操作关系。系统的行为就好像只有一个数据副本，且每个操作都是原子的，这意味着对于任何两个操作，我们总是可以指出哪个操作在先。 

	因此，根据这个定义，在可线性化数据存储中不存在并发操作（**并发意味着时间线会出现分支和合并，而不同分支上的橾作无法直接比较**），一定有一个时间线将所有操作都全序执行。可能存在多个请求处于等待处理的状态，但是数据存储保证了在特定的时间点执行特定的操作，所以是单个时间轴，单个数据副本，没有并发。
- 因果关系
	如果两个操作都没有发生在对方之前，那么这两个操作是[[数据密集系统设计/数据密集型系统设计2：复制与分区#1 6 3 2 Happens-before 关系和并发|并发关系]]。换言之，如果两个事件是因果关系（一个发生在另一个之前），那么这两个事件可以被排序；而并发的事件则无法排序比较。这表明因果关系至少可以定义为偏序，而非全序。

#### 1.2.1.2. 可线性化强于因果一致性

那么因果序和可线性化之间是什么关系呢？答案是**可线性化一定意味着因果关系**： 任何可线性化的系统都将正确地保证因果关系。特别是，如果系统存在多个[[数据密集系统设计/数据密集型系统设计5：一致性与共识#1.1.3.3. 跨通道的时间依赖|通信通道]]，可线性化确保了因果关系会自动全部保留，而不需要额外的工作（比如在不同组件之间的传递时间戳）。

可线性化可以确保因果性这一结论，使线性化系统更加简单易懂而富有吸引力。但是，正如在线性化的代价将要阐述的，线性化会显著降低性能和可用性，尤其是在严重网络延迟的情况下（例如多数据中心）。正因如此，一些分布式数据系统已经放弃了线性化，以换来更好的性能，但也存在可能无法正确工作的风险。

好消息是线性化并非是保证因果关系的唯一途径，还有其他方法使得系统可以满足因果一致性而免于线性化所带来的性能问题。事实上，**因果一致性可以认为是，不会由于网络延迟而显著影响性能，又能对网络故障提供容错的最强的一致性模型**。

在许多情况下，**许多看似需要线性化的系统实际上真正需要的是因果一致性**，后者的实现可以高效很多。

#### 1.2.1.3. 捕获因果依赖关系

为保持因果关系，需要知道哪个操作发生在前。这里只需偏序关系，或许并发操作会以任意顺序执行，但如果一个操作发生在另一个操作之前，那么每个副本都应该按照相同的顺序处理。因此，当某个副本在处理一个请求时，必须确保所有因果在前的请求都已完成处理；否则，后面的请求必须等待直到前序操作处理完毕。

确定请求的先后顺序与[[数据密集系统设计/数据密集型系统设计2：复制与分区#1 6 3 并发写检测|检测并发写]]中所讨论的技巧类似。后者针对的是无主复制中的因果关系，该场景需要去检测对同一个主键的并发写请求，从而避免更新丢失。因果一致性则要更进一步，它需要跟踪整个数据库请求的因果关系，而不仅仅是针对某个主键。版本向量技术可以推广为一种通用的解决方案。

为了确定因果关系，数据库需要知道应用程序读取的是哪个版本的数据。 SSI 的冲突检测也是类似想法，如[[数据密集系统设计/数据密集型系统设计3：事务#1 3 3 可串行化的快照隔离|可串行化的快照隔离]]所介绍的： 当事务提交时，数据库要检查事务曾经读取的数据版本现在是否仍是最新的。为此，数据库需要跟踪事务读取了哪些版本的数据。

### 1.2.2. 序列号排序

虽然因果关系很重要，但实际上跟踪所有的因果关系不切实际。在许多应用程序中，客户端在写入之前会先读取大批数据，系统无法了解之后的写入究竟是依赖于全部读取内容，还是仅仅是其中一小部分。但很明显，显式跟踪所有已读数据意味着巨大的运行开销。

这里还有一个更好的方法： 我们可以使用序列号或时间戳来排序事件。时间戳不一定来自墙上时钟（或者物理时钟）。它可以只是一个逻辑时钟，例如采用算法来产生一个数字序列用以识别操作，通常是递增的计数器。

特别是，我们可以按照与因果关系一致的顺序来创建序列号 : 保证如果操作 A 发生在 B 之前，那么 A 一定在全序中出现在 B 之前（即 A 的序列号更小）。并行操作的序列可能是任意的。这样的全局排序可以捕获所有的因果信息，但也强加了比因果关系更为严格的顺序性。

在主从复制数据库中，复制日志定义了与因果关系一致的写操作全序关系。主节点可以简单地为每个操作递增某个计数器，从而为复制日志中的每个操作赋值一个单调递增的序列号。从节点按照复制日志出现的顺序来应用写操作，那结果一定满足因果一致性（虽然从节点的数据可能会滞后于主节占）。

#### 1.2.2.1. 非因果序列发生器

如果系统不存在这样唯一的主节点（例如可能是多主或者无主类型的数据库，或者数据库本身是分区的），如何产生序列号就不是那么简单了。实践中可以采用以下方法：

- 每个节点都独立产生自己的一组序列号
    例如，如果有两个节点，则一个节点只生成奇数，而另一个节点只生成偶数。还可以在序列号中保留一些位用于嵌入所属节点的唯一标识符，确保不同的节点永远不会生成相同的序列号。
- 可以把墙上时间戳信息（物理时钟）附加到每个操作上
    时间戳可能是不连续的，但是只要它们有足够高的分辨率，就可以用来区分操作。最后写获胜的冲突解决方案也使用类似的方法。
- 可以预先分配序列号的区间范围
    例如，节点 A 负责区间 1~1000 的序列号，节点 B 负责 1001~2000。然后每个节点独立地从区间中分配序列号，当序列号出现紧张时就分配更多的区间。

上述三种思路都可行，**相比于把所有请求全部压给唯一的主节点具有更好的扩展性**。它们为每个操作生成一个唯一的、近似增加的序列号。不过，它们也都存在一个问题：**所产生的序列号与因果关系并不严格一致**。

所有这些序列号发生器都无法保证正确捕获跨节点操作的顺序，因而存在因果关系方面的问题：如节点处理速度不同导致节点生成序号差异过大，物理时钟受到时钟偏移的影响，区间分配器在多个节点上导致节点间序号与因果序不一致。

#### 1.2.2.2. Lamport 时间戳

刚才所描述的三个序列号发生器可能与因果关系存在不一致，但还有一个简单的方法可以产生与因果关系一致的序列号。它被称为 `兰伯特时间戳 (Lamport timestamp)`，由 Leslie Lamport 于 1978 年提出。

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220627225259.png)

该算法思想如下：

1. 每个请求对应一个时间戳，包含计数器，节点 ID
	首先每个节点都有一个唯一的标识符，且每个节点都有一个计数器来记录各自己处理的请求总数。 Lamport 时间戳是一个值对（计数器，节点 ID)。两个节点可能会有相同的计数器值，但时间戳中还包含节点 ID 信息，因此可以确保每个时间戳都是唯一的。
2. 收到请求时，对比时间戳从而更新计数器
	到目前为止，该思路与上一节所描述的奇数/偶数计数器并无本质不同。但是 Lamport 时间戳的核心亮点在于**使它们与因果性保持一致**，具体如下所示：每个节点以及每个客户端都跟踪迄今为止所见到的最大计数器值，并在每个请求中附带该最大计数器值。当节点收到某个请求（或者回复）时，如果发现请求内嵌的最大计数器值大于节点自身的计数器值，则它立即把自己的计数器修改为该最大值。
3. 对于相同计数器，采用节点 ID 比较大小
	Lamport 时间戳与物理墙上时钟并不存在直接对应关系，但它可以保证全序：给定两个 Lamport 时间戳，计数器较大那个时间戳大；如计数器值正好相同，则节点 ID 越大，时间戳越大。

#### 1.2.2.3. 时间戳排序依旧不够

虽然 Lamport 时间戳定义了与因果序一致的全序关系，但还不足以解决实际分布式系统中许多常见的问题。

例如，一个账户系统需要确保用户名唯一标识用户。即两个用户如果同时尝试使用相同的用户名创建账户时，确保其中一个成功，另一个必须失败。乍看之下，似乎全序关系（例如使用 `Lamport 时间戳`）应该可以解决问题： 如果有这样并发的操作，则选择时间戳较低的那个作为获胜者（先申请用户名的那个请求），而让时间戳大的请求失败。由于时间戳有序，所以这样的比较方法也应该是可行的。

但是，这种方法确定胜利者有这样一个前提条件：**需要收集系统中所有的用户创建请求，然后才可以比较它们的时间戳**。然而，当节点刚刚收到用户的创建请求时，它无法当时就做出决定该请求应该成功还是失败。此时，节点根本不知道是否有另一个节点在同时创建相同用户名（以及那个请求所附带的时间戳）。

而为了获得上述两点信息，系统就必须检查每个节点，询问它们在做什么。如果万一某个节点出现故障或者由于网络问题而无法连接，那么方法就无法正常运转。显然这不是我们所期望的容错系统。 

这里问题的关键是，只有在收集了所有的请求信息之后，才能清楚这些请求之间的全序关系。如果另一个节点执行了某些操作，但你无法知道那是什么，就无法构造出最终的请求序列。也许，该未知操作确实需要插入到全序集合中才能正确评估出下一步。

总而言之，**为了实现像用户名唯一性约束这样的目标，仅仅对操作进行全序排列还是不够的，还需要知道这些操作是否发生、何时确定等**。假如能够在创建用户名时，已经确定知道了没有其他节点正在执行相同用户名的创建，你大可以直接安全返回创建成功。

### 1.2.3. 全序关系广播

如果程序只运行在一个 CPU 核上，可以非常简单地定义出操作的全序关系，即在单核上执行的顺序。但是，在分布式系统中，让所有的节点就全序关系达成一致就面临巨大挑战。在前面一节中，我们讨论了按时间戳或序列号排序，发现它不如主从复制那么直接有效（如果使用时间戳排序来实现唯一性约束，会丧失容错性）。

如前所述，主从复制首先确定某一个节点作为主节点，然后在主节点上顺序执行操作。接下来的主要挑战在于，**如何扩展系统的吞吐量使之突破单一主节点的限制，以及如何处理主节点失效时的故障切换**。在分布式系统研究文献中，这些问题被称为全序关系广播或者原子广播。


全序关系广播通常指节点之间交换消息的某种协议。下面是一个非正式的定义，它要求满足两个基本安全属性：

1. 可靠发送
	没有消息丢失，如果消息发送到了某一个节点，则它一定要发送到所有节点。
2. 严格有序
	消息总是以相同的顺序发送给每个节点。

即使节点或网络出现了故障，全序关系广播算法的正确实现也必须保证上述两条。当然，网络中断时是不可能发送成功的，但算法要继续重试，直到最终网络修复，消息发送成功（且必须以正确的顺序发送）。

像 ZooKeeper 和 etcd 这样的共识服务实际上就实现了全关系序广播。这也暗示了全序关系广播与共识之间有着密切联系。

全序关系广播正是数据库复制所需要的：如果每条消息代表数据库写请求，并且每个副本都按相同的顺序处理这些写请求，那么所有副本可以保持一致（或许有些滞后）。该原则也被称为`状态机复制`。

可以使用全序关系广播来实现可串行化事务。如果每条消息表示一个确定性事务并且作为存储过程来执行，且每个节点都遵从相同的执行顺序，那么可以保证数据库各分区以及各副本之间的一致性。

全序关系广播另一个要点是**顺序在发送消息时已经确定，如果消息发送成功，节点不允许追溯地将某条消息插入到先前的某个位置上**。这一点使得全序关系广播比基于时间戳排序要求更强。

理解全序关系广播的另一种方式是将其视为日志（如复制日志，事务日志或预写日志）。传递消息就像追加方式更新日志。由于所有节点必须以相同的顺序发送消息，因此所有节点都可以读取日志并看到相同的消息序列。

全序关系广播对于提供 fencing 令牌的锁服务也很有用。每个获取锁的请求都作为消息附加到日志中，所有消息按照日志中的顺序依次编号。序列号还可以作为令牌，它符合单调递增要求。在 ZooKeeper 中，该序列号被称为 zxid。

##### 1.2.3.1. 使用全序关系广播实现线性化存储

在一个可线性化的系统中有全序操作集合。这是否意味着可线性化与全序关系广播是完全相同呢？不完全是，但两者之间有着密切的联系。

全序关系广播是基于异步模型：**保证消息以固定的顺序可靠地发送，但是不保证消息何时发送成功（因此某个接收者可能明显落后于其他接收者）。而可线性化则强调就近性：读取时保证能够看到最新的写入值**。

如果有了全序关系广播，就可以在其上构建线性化的存储系统。例如，确保用户名唯一标识一个用户。

设想一下，对于每一个可能的用户名，都可以有一个带有原子比较－设置操作的线性化寄存器。每个寄存器初始值为空（表示尚未使用）。当用户创建一个用户名时，对该用户名的寄存器执行比较设置操作： 仅当寄存器值为空时，将其设置为新的用户账号。如果多个用户试图同时获取相同的用户名，则只有一个原子比较－设置操作成功。

可以通过使用全序关系广播以追加日志的方式来实现线性化的原子比较－设置操作，步骤如下所示：

1. 在日志中追加一条消息，并指明想要的用户名。

2. 读取日志，将其广播给所有节点，并等待回复。

3. 检查是否有任何消息声称该用户名已被占用。
	- 如果第一条这样的回复来自于当前节点，那么就成功获得该用户名，可以提交该获取声明（也许附加另一条消息到日志）并返回给客户端。
	- 反之，如果声称占用的第一条回复消息来自其他节点，则中止操作。

由于日志条目以相同的顺序发送到所有节点，而如果存在多个并发写入，则所有节点将首先决定哪个请求在先。选择第一个写请求作为获胜者，并中止其他请求，以确保所有节点同意一个写请求最终要么提交成功要么中止。类似的方法还可以用来在日志之上实现可串行化的多对象事务。

虽然此过程可确保线性化写入，但它却无法保证线性化读取，即从异步日志更新的存储中读取数据时，可能是旧值。具体来说，这里只提供了顺序一致性, 有时也称为时间线一致性, 它弱于线性化保证。为了同时满足线性化读取，有以下几个方案：

- 读请求排序
	可以采用追加的方式把读请求排序、广播，然后各个节点获取该日志，当本节点收到消息时才执行真正的读操作。消息在日志中的位置已经决定了读取发生的时间点。 etcd 的 quorum 读取和这个思路有相似之处。
- 等待消息执行
	如果可以以线性化的方式获取当前最新日志中消息的位置，则查询位置，等待直到该位置之前的所有条目都已经发送给你，接下来再执行读取。这与 ZooKeeper 的 `sync ()` 操作思路相同。
- 读取最新副本
	可以从同步更新的副本上进行读取，这样确保总是读取最新值。这种技术可以用于链式复制。

##### 1.2.3.2. 使用线性化存储实现全序广播

假定已有了线性化的存储，如何在其上构建全序关系广播？ 最简单的方法是假设有一个线性化的寄存器来存储一个计数，然后使其支持原子自增读取操作或者原子比较－设置操作。

解法思路很简单： 对于每个要通过全序关系广播的消息，原子递增并读取该线性化的计数，然后将其作为序列号附加到消息中。接下来，将消息广播到所有节点（如果发生丢失，则重新发送），而接受者也严格按照序列化来发送回复消息。

请注意，与 Lamport 时间戳不同，通过递增线性化寄存器获得的数字不会存在任何间隙。因此，如果节点完成了消息 4 的发送，且接收到了序列化 6 的消息，那么在它对消息 6 回复之前必须等待消息 5。 Lamport 时间戳则不是这样，而这也是区别全序关系广播与基于时间戳排序的关键。

使用原子自增操作来创建线性化整数有多难呢？答案还是那样，如果不存在失效，就非常容易，甚至可以把它保存在某个节点的内存变量中。**难点在于处理节点的网络中断，以及节点失效时如何恢复该值**。事实上，如果对线性化的序列号发生器深思熟虑之后所得到的最终结果，往往亳无意外地指向了共识算法。

## 1.3. 分布式事务与共识

共识问题是分布式计算中最重要也是最基本的问题之一。有很多重要的场景都需要集群节点达成某种一致，例如：对于主节点选举需要所有节点达成一致避免出现脑裂；对于原子事务提交需要所有节点对事务的结果达成一致。

原子提交的形式化描述其实与共识还稍有不同，**原子事务只有在所有参与者都投票赞成的
情况下才能完成最终提交，如果有任何参与者中止，则必须中止。共识则可以就任何一位参与者的提议进行表决，达到多数即可通过**。原子提交与共识可以相互转化。而非阻塞的原子提交则比共识更难，具体请参阅笫本章后面的三阶段提交。

### 1.3.1. 原子提交与二阶段提交

原子性可以防止失败的事务破坏系统，避免形成部分成功夹杂着部分失败。对于在单个数据库节点上执行的事务，原子性通常由存储引擎来负责。当客户端请求数据库节点提交事务时，数据库首先使事务的写入持久化（通常保存在预写日志中), 然后把提交记录追加写入到磁盘的日志文件中。如果数据库在该过程中间发生了崩溃，那么当节点重启后，事务可以从日志中恢复： 如果在崩溃之前提交记录已成功写入磁盘，则认为事务已安全提交；否则，回滚该事务的所有写入。

因此，在单节点上，事务提交非常依赖于数据持久写入磁盘的顺序关系：先写入数据，然后再提交记录。事务提交（或中止）的关键点在于磁盘完成日志记录的时刻：在完成日志记录写之前如果发生了崩溃，则事务需要中止；如果在日志写入完成之后，即使发生崩溃，事务也被安全提交。这就是在单一设备上（某个特定的磁盘连接到一个特定的节点）上实现原子提交的核心思路。

但是，如果一个事务涉及多个节点呢？向所有节点简单地发送一个提交请求，然后各个节点独立执行事务提交是绝对不够的。这样做很容易发生部分节点提交成功，而其他一些节点发生失败，从而违反了原子性保证。

事务提交不可撤销，不能事后再改变主意（在提交之后再追溯去中止）。这些规则背后的深层原因是，**一且数据提交，就被其他事务可见，继而其他客户端会基于此做出相应的决策**。这个原则构成了[[数据密集系统设计/数据密集型系统设计3：事务#1 2 1 读提交|读－提交隔离]]级别的基础。如果允许事务在提交后还能中止，会违背之后所有读－提交的事务，进而被迫产生级联式的追溯和撤销。

当然已提交事务的效果可以被之后一笔新的事务来抵消掉，即补偿性事务。不过，从数据库的角度来看，前后两个事务完全互相独立。类似这种跨事务的正确性需要由应用层来负责。

#### 1.3.1.1. 二阶段提交


[[分布式/分布式大纲#2 4 3 XA 实现之 2PC 协议|两阶段提交]] (two-phase commit, 2PC) 是一种在多节点之间实现事务原子提交的算法，用来确保所有节点要么全部提交，要么全部中止。它是分布式数据库中的经典算法之一。

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220627234902.png)

2PC 引入了单节点事务所没有的一个新组件： 协调者（也称为事务管理器）。协调者通常实现为共享库，运行在请求事务相同进程中（例如嵌入在 JavaEE 容器中），但也可以是单独的进程或服务。常见协调者的例子包括 Narayana, JOTM, BTM 或 MSDTC。

通常， 2PC 事务从应用程序在多个数据库节点上执行数据读/写开始。我们将这些数据库节点称为事务中的参与者。当应用程序准备提交事务时，协调者开始阶段 1: 发送一个准备请求到所有节点，询问他们是否可以提交。协调者然后跟踪参与者的回应：

- 如果所有参与者回答是，表示他们已准备好提交，那么协调者接下来在阶段 2 会发出提交请求，提交开始实际执行。

- 如果有任何参与者回复否，则协调者在阶段 2 中向所有节点发送放弃请求。

#### 1.3.1.2. 三阶段提交

针对两阶段的一些问题，目前也有[[分布式/分布式大纲#2 4 4 2PC 改进之 3PC 协议|三阶段提交算法]]。但是三阶段提交依赖一个完美的故障检测器，这非常困难，因此目前还在普遍使用 2PC。

### 1.3.2. 实践中的分布式事务

分布式事务，尤其是那些通过两阶段提交所实现的事务，声誉混杂。一方面，它们被看作是提供了一个其他方案难以企及的重要的安全保证；但另一方面，他们由于操作上的缺陷、性能问题、承诺不可靠等问题而遭受诉病。

但是，我们不应该就这么直接地抛弃分布式事务，而应该更加审慎的对待，从中获取一些重要的经验教训。首先，我们还是要明确种截然不同的分布式事务概念：

- 数据库内部的分布式事务
	某些分布式数据库（例如那些标配支持复制和分区的数据库）支持跨数据库节点的内部事务。例如，VoltDB 和 MySQL Cluster 的 NDB 存储引擎就支持这样的内部分布式事务。此时，所有参与节点都运行着相同的数据库软件。
- 异构分布式事务
	在异构分布式事务中，存在两种或两种以上不同的参与者实现技术。例如来自不同供应商的数据库，甚至是非数据库系统（如消息中间件）。即使是完全不同的系统，跨系统的分布式事务业必须确保原子提交。

数据库内部事务由于不必考虑与其他系统的兼容，因此可以使用任何形式的内部协议并采取有针对性的优化。因此，数据库内部的分布式事务往往可行且工作不错，但异构环境的事务则充满了挑战。

#### 1.3.2.1. Exactly-once 消息处理

异构的分布式事务旨在无缝集成多种不同的系统。例如，当且仅当数据库中处理消息的事务成功提交，消息队列才会标记该消息已处理完毕。这个过程是通过自动提交消息确认和数据库写入来实现的。即使消息系统和数据库两种不同的技术运行在不同的节点上，采用分布式事务也能达到上述目标。

如果消息发送或数据库事务任何一个发生失败，则两者都须中止，消息队列可以在稍后再次重传消息。因此，通过自动提交消息和消息处理的结果，可以确保消息可以有效处理有且仅有一次（成功之前有可能需要重试）。而如果事务最后发生中止，则会放弃所有部分完成的结果。

需要指出，只有在所有受影响的系统都使用相同的原子提交协议的前提下，这种分布式事务才是可行。

#### 1.3.2.2. XA 交易

[[分布式/分布式大纲#2 4 1 刚性事务之 XA 模型|X/Open XA (eXtended Architecture, XA)]] 是异构环境下实施两阶段提交的一个工业标准, 于 1991 年推出并得到广泛推广。目前，许多传统关系数据库（包括 PostgreSQL, MySQL、 DB2 、 SQL Server 和 Oracle) 和消息队列（包括 ActiveMQ、 HornetQ、 MSMQ 和 IBM MQ) 都支持 XA。

XA 并不是一个网络协议，而是一个与事务协调者进行通信的 API。当然，它也支持其他语言的 API 绑定，例如 JavaEE 中，XA 事务是由 Java 事务 API (Java Transaction API, JTA) 来实现，JTA 可以支持非常多 JDBC (Java Databae Connectivity) 驱动和消息队列驱动（通过 Java 消息服务， JMS) 。

#### 1.3.2.3. 停顿时仍持有锁

为什么我们非常关注陷入停顿的参与者节点（即不确定该提交还是中止）呢？难道系统不能选择忽略（ 并最终清理）这些节点，这样系统不就可以继续工作么？

问题的关键在于锁。数据库事务通常持有待修改行的行级独占锁，用以防止脏写。此外，如果要使用可串行化的隔离，则两阶段锁的数据库还会对事务曾经读取的行持有读－共享锁。在事务提交（或中止）之前，数据库都不会释放这些锁。因此，在两阶段提交时，事务在整个停顿期间一直持有锁。换句话说，如果协调者崩溃并且需要 20 分钟才能重启恢复，那么这些对象将被锁定 20 分钟；如果协调者的日志由于某种原因而彻底丢失，这些数据对象将永久处于加锁状态，至少管理员采用手动方式解决之前只能如此。

数据处于加锁时，其他事务就无法执行修改。取决于数据库的具体实现，其他事务甚至无法读取这些行。因此，其他的事务事买上无法有效执行。这可能会导致很多上层应用基本处于不可用状态，所以必须解决处于停顿状态的那些事务。

#### 1.3.2.4. 从协调者故障中恢复

理论上，如果协调者崩溃之后重新启动，它应该可以从日志中恢复那些停顿的事务。然而，在实践中，孤立的不确定事务确实会发生。

无论何种原因，例如由于软件 bug 导致交易日志丢失或者损坏，最终协调者还是出现了恢复失败。那些悬而未决的事务无法自动解决，而是永远留在那里，而且还持有锁并阻止其他事务。

**即使重启那些处于停顿状态的数据库节点也无法解决这个问题**，这是由于 2PC 的正确实现要求即使发生了重启，也要继续保持重启之前事务的加锁（否则就会违背原子性保证）。所以，这的确非常棘手。

唯一的出路只能是让管理员手动决定究竟是执行提交还是回滚。管理员必须仔细检查每个有问题的参与者，确定是否有节点已经事实完成了提交（或中止），然后要将相同的结果一一应用于所有的参与者上。这种方案可能需要大量的手工操作，而且很可能处在关键生产环境的中断间隙，背负着巨大的压力和时间限制（要不然，为什么协调者容易出现这种问题）。

许多 XA 的实现都支持某种紧急避险措施称之为启发式决策： 这样参与者节点可以在紧急情况下单方面做出决定，放弃或者继续那些停顿的事务，而不需要等到协调者发出指令。需要说明的是，这里的启发式其实是可能破坏原子性的委婉说法，它的确违背了两阶段提交所做出的承诺。因此，这种启发式决策只是为了应急，不能作为常规手段来使用。

### 1.3.3. 分布式事务的限制

XA 事务解决了多个参与者之间如何达成一致这样一个非常现实而重要的问题，但正如上面所看到的，它也引入了不少操作方面的限制。特别是，核心的事务协调者本身就是一种数据库（存储事务的投票结果），因此需要和其他重要的数据库一样格外小心。

- 协调者的单点性能问题
	如果协调者不支持数据复制，而是在单节点上运行，那么它就是整个系统的单点故障（因为它的故障导致了很多应用阻塞在停顿事务所持有的锁上）。而现实情况是，有许多协调者的实现默认情况下并非高可用，或者只支持最基本的复制。
- 协调者的状态存储问题
	许多服务器端应用程序都倾向于无状态模式（因为更受 HTTP 的青睐），而所有的持久状态都保存在数据库中，这样应用服务器可以轻松地添加或删除实例。 

	但是，**当协调者就是应用服务器的一部分时，部署方式就发生了根本的变化**。突然间，协调者的日志成为可靠系统的重要组成部分，它要求与数据库本身一样重要 （需要协调者日志恢复那些有疑问的事务）。这样的应用服务器已经不再是无状态。
- 各个组件的兼容性问题
	由于 XA 需要与各种数据系统保持兼容，它最终其实是多系统可兼容的最低标准。例如，它**无法深入检测不同系统之间的死锁条件**（因为这就将需要另一个标准化协议，使得多个系统交换事务所等待的锁信息），而且不适用于 SSI，后者要求一个复杂的协议来识别不同系统间的写冲突。
- 分布式事务存在失败扩大的风险
	然而， 2PC 要成功提交事务还是存在潜在的限制，它要求必须所有参与者都投票赞成，如果有任何部分发生故障，整个事务只能失败。所以分布式事务有扩大事务失败的风险，这与我们构建容错系统的目标有些背道而驰。

### 1.3.4. 支持容错的共识

共识问题通常形式化描述如下： 一个或多个节点可以提议某些值，由共识算法来决定最终值。例如对于预约座位的例子，当多个顾客同时试图购买最后一个座位时，处理顾客请求的每个节点可以提议它所服务的顾客 ID，最后的决定则是关于由哪个顾客获得座位。

在这个描述中，共识募法必须满足以下性质: 

- 协商一致性 (Uniform agreement) 
	所有的节点都接受相同的决议。
- 诚实性 (Integrity) 
	所有节点不能反悔，即对一项提议不能有两次决定。
- 合法性 (Validity) 
	如果决定了值 V，则 V 一定是由某个节点所提议的。
- 可终止性 (Termination) 
	节点如果不崩溃则最终一定可以达成决议。

协商一致性和诚实性属性定义了共识的核心思想：决定一致的结果，一且决定，就不能改变。合法性属性主要是为了排除一些无意义的方案： 例如，无论什么建议，都可以有一个总是为空 (NULL) 的决定，虽然可以满足一致性和诚实性，但没有任何实际效果。

如果不关心容错，那么满足前三个属性很容易： 可以强行指定某个节点为独裁者，由它做出所有的决定。但是，如果该节点失败，系统就无法继续做出任何决定。其实这就是在两阶段提交时所看到的： 如果协调者失败了，那些处于不确定状态的参与者就无从知道下一步该做什么。

可终止性则引入了容错的思想。它重点强调**一个共识算法不能原地空转，永远不做事情，换句话说，它必须取得实质性进展。即使某些节点出现了故障，其他节点也必须最终做出决定**。可终止性属于一种活性，而另外三种则属于安全性方面的属性。

当然，如果所有的节点都崩溃了，那么无论何种算法都不可能继续做出决定。算法所能够容忍的失败次数和规模都有一定的限制。事实上，可以证明任何共识算法都需要至少大部分节点正确运行才能确保终止性。而这个多数就可以安全地构成 quorum。

因此，可终止性的前提是，发生崩溃或者不可用的节点数必须小于半数节点。即便是多数节点出现了故障或者存在严重的网络问题，现在有很多实现的共识系统也可以满足安全属性：协商一致性，诚实性和合法性。所以**大规摸的失效情况可能会导致系统无法处理请求，但不会破坏系统做出无效的决定**。

大多数共识算法都假定系统不存在拜占庭式错误。对于拜占庭式错误，即节点没有遵循协议（例如故意发送相互矛盾的消息），从而破坏协议的安全属性。研究表明，只要发生拜占庭故障的节点数小于三分之一, 也可以达成共识。

#### 1.3.4.1. 共识算法与全序广播

最著名的容错式共识算法包括 VSRl，Paxos, Raf 和 Zab。这些算法存在诸多相似之处，但又不完全相同。

这些算法大部分其实并不是直接使用上述的形式化模型（提议并决定某个值，同时满足上面 4 个属性）。相反，他们是决定了一系列值，然后采用全序关系广播算法。

全序关系广播的要点是，消息按照相同的顺序发送到所有节点，有且只有一次。如果仔细想想，**这其实相当于进行了多轮的共识过程：在每一轮，节点提出他们接下来想要发送的消息，然后决定下一个消息的全局顺序**。

所以，全序关系广播相当于持续的多轮共识（每一轮共识的决定对应于一条消息）：

- 由于协商一致性，所有节点决定以相同的顺序发送相同的消息。

- 由于诚实性，消息不能重复。

- 由于合法性，消息不会被破坏，也不是凭空捏造的。

- 由于可终止性，消息不会丢失。

VSR、Raft 和 Zab 都直接采取了全序关系广播，这比重复性的一轮共识只解决一个提议更加高效。而 Paxos 则有对应的优化版本称之为 Multi-Paxos。

#### 1.3.4.2. 主从复制与共识

主从复制所有的写入操作都由主节点负责，并以相同的顺序发送到从节点来保持副本更新。这不就是基本的全序关系广播么？那在主从复制时我们怎么没有考虑共识问题呢？

答案取决于如何选择主节点。如果主节点是由运营人员手动选择和配置的，那基本上就是一个独裁性质的一致性算法 ：只允许一个节点接受写入（并决定复制日志中的写入顺序），如果该节点发生故障，系统将无法写入，直到操作入员再手动配置新的节点成为主节点。这样的方案也能在实践中很好地发挥作用，但它需要人为干预才能取得进展，不满足共识的可终止性。

一些数据库支持自动选举主节点和故障切换，通过选举把某个从节点者提升为新的主节点。这样更接近容错式全序关系广播，从而达成共识。

#### 1.3.4.3. Epoch 和 Quorum

目前所讨论的所有共识协议在其内部都使用了某种形式的主节点，虽然主节点并不是固定的。相反，他们都采用了一种弱化的保证：协议定义了一个世代编号 (epoch number, 对应于 Paxos 中的 ballot number, VSP 中 view number, 以及 Raft 中的 term number) , 并保证在每个世代里，主节点是唯一确定的。

如果发现当前的主节点失效，节点就开始一轮投票选举新的主节点。选举会赋予一个单调递增的 epoch 号。如果出现了两个不同的主节点对应于不同 epoch 号码（例如，上一个 epoch 号码的主节点其实并没有真正挂掉），则**具有更高 epoch 号码的主节点将获胜**。

在主节点做出任何决定之前，它必须首先检查是否存在比它更高的 epoch 号码，否则就会产生冲突的决定。主节点如何知道它是否已被其他节点所取代了呢？节点不能依靠自己所掌握的信息来决策，例如自认为是主节点并不代表其他节点都接受了它的自认为。

相反，它必须从 quorum 节点中收集投票。主节点如果想要做出某个决定，须将提议发送给其他所有节点，等待 quorum 节点的响应。 quorum 通常（但不总是）由多数节点组成。并且，只有当没有发现更高 epoch 主节点存在时，节点才会对当前的提议（带有 epoch 号码）进行投票。

因此，这里面实际存在两轮不同的投票：首先是投票决定谁是主节点，然后是对主节点的提议进行投票。其中的关键一点是，参与两轮的 quorum 必须有重叠： 如果某个提议获得通过，那么其中参与投票的节点中必须至少有一个也参加了最近一次的主节点选举。换言之，如果在针对提议的投票中没有出现更高 epoch 号码，那么可以得出这样的结论： 因为没有发生更高 epoch 的主节点选举，当前的主节点地位没有改变，所以可以安全地就提议进行投票。

投票过程看起来很像两阶段提交 (2PC)。最大的区别是， 2PC 的协调者并不是依靠选举产生；另外**容错共识算法只需要收到多数节点的投票结果即可通过决议**，而 2PC 则要求每个参与者都必须做出”是“才能最终通过。此外，共识篇法还定义了恢复过程，出现故障之后，通过该过程节点可以选举出新的主节点然后进入一致的状态，确保总是能够满足安全属性。所有这些差异之处都是确保共识算法正确性和容错性的关键。

#### 1.3.4.4. 共识的局限性

**共识算法对于分布式系统来说绝对是一个巨大的突破，它为一切不确定的系统带来了明确的安全属性（一致性，完整性和有效性），此外它还可以支持容错（只要大多数节点还在工作和服务可达）。共识可以提供全序关系广播，以容错的方式实现线性化的原子操作。**不过，也不是所有系统都采用了共识，因为好处的背后都是有代价的。这包括：

- 共识算法投票是一个同步复制的过程
	在达成一致性决议之前，节点投票的过程是一个同步复制过程。数据库通常配置为异步复制，存在某些已提交的数据在故障切换时丢失的风险，即使这样，很多系统还是采用异步复制（而非同步复制），原因正是为了更好的性能。
- 共识体系需要严格的多数节点才能运行。
	这意味着需要至少三个节点才能容忍一个节点发生故蹄（剩下的三分之二形成多数），或者需要最少五个节点来容忍两个节点故障（其余五分之三形成多数）。如果由于网络故障切断了节点之间的连接，则只有多数节点所在的分区可以继续工作，剩下的少数节点分区则处于事实上的停顿状态)。
- 多数共识算法假定一组固定参与投票的节点集，这意味着不能动态添加或删除节点。
	动态成员资格的扩展特性可以在集群中的按需调整节点数，但相比于静态的成员组成，其理解程度和接受程度要低很多。
- 共识系统通常依靠超时机制来检测节点失效。
	在网络延迟高度不确定的环境中，特别是那些跨区域分布的系统，经常由于网络延迟的原因，导致节点错误地认为主节点发生了故障。虽然这种误判并不会损害安全属性，但频繁的主节点选举显著降低了性能，系统最终会花费更多的时间和资源在选举主节点上而不是原本的服务任务。
- 共识算法往往对网络问题特别敏感。 
	例如， Raft 已被发现存在不合理的边界条件处理 : 如果整个网络中存在某一条网络连接持续不可靠， Raft 会进入一种奇怪的状态： 它不断在两个节点之间反复切换主节点，当前主节点不断被赶下台，这最终导致系统根本无法安心提供服务。其他共识算法也有类似的问题，所以面对不可靠网络，如何设计更具鲁棒性的共识算法仍然是一个开放性的研究问题。

### 1.3.5. 成员与协调服务

ZooKeeper 或 etcd 这样的项目通常称为**分布式键值存储或协调与配置服务**。从它们对外提供服务的 API 来看则与数据库非常相像： 读取、写入对应主键的值，或者遍历主键。如果他们只是个普通数据库的话，为什么要花大力气实现一个共识算法呢？它们与其他数据库有何不同之处？

ZooKeeper 和 etcd 主要针对保存少量、可完全载入内存的数据（虽然它们最终仍要写入磁盘以支持持久性）而设计，所以不要用它们保存大量的数据。它们**通常采用容错的全序广播算法在所有节点上复制这些数据从而实现高可靠**。正如之前所讨论的，全序广播主要用来实现数据库复制： 每条消息代表的是数据库写请求，然后按照相同的顺序在多个节点上应用写操作，从而达到多副本之间的一致性。

ZooKeeper 的实现其实模仿了 Google 的 Chubby 分布式锁服务, 但它不仅实现全序广播（因此实现了共识），还提供了其他很多有趣的特性。所有这些特性在构建分布式系统时格外重要：

- 线性化的原子橾作
	使用原子比较－设置操作，可以实现加锁服务。例如如果多个节点同时尝试执行相同的操作，则确保其中只有一个会成功。共识协议保证了操作满足原子性和线性化，即使某些节点发生故障或网络随时被中断。分布式锁通常实现为一个带有到期时间的租约，这样万一某些客户端发生故障，可以最终释放锁。
- 橾作全序
	当资源被锁或者租约保护时，需要[[数据密集系统设计/数据密集型系统设计4：分布式的挑战#4 2 错误修复| fencing 令牌]]来防止某些客户端由于发生进程暂停而引起锁冲突。 fencing 令牌确保每次加锁时数字总是单调增加。 ZooKeeper 在实现该功能时，采用了对所有操作执行全局排序，然后为每个操作都赋予一个单调递增的事务 ID (zxid) 和版本号 (cversion)
- 故障检测
	客户端与 ZooKeeper 节点维护一个长期会话，客户端会周期性地与 ZooKeeper 服务节点互相交换心跳信息，以检查对方是否存活。即使连接出现闪断，或者某个 ZooKeeper 节点发生失效，会话仍处于活动状态。但是，如果长时间心跳停止且超过了会话超时设置， ZooKeeper 会声明会话失败。此时，所有该会话持有的锁资源可以配置为自动全部释放 (ZooKeeper 称之为 ephemeral nodes 即临时节点）。
- 更改通知
	客户端不仅可以读取其他客户端所创建的锁和键值，还可以监视它们的变化。因此，客户端可以知道其他客户端何时加入了集群（基于它写入 ZooKeeper 的值） 以及客户端是否发生了故障（会话超时导致节点消失）。通过订阅通知机制，客户端不需要频繁地轮询服务即可知道感兴趣对象的变化情况。

#### 1.3.5.1. 节点任务分配

Zoo Keeper 和 Chubby 系统非常适合的一个场景是，如果系统有多个流程或服务的实例，并且需求其中的一个实例充当主节点；而如果主节点失效，由其他某个节点来接管。显然，这非常吻合主从复制数据库，此外，它对于作业调度系统（或类似的有状态服务）也非常有用。

还有另一个场景，对于一些分区资源（可以是数据库，消息流，文件存储，分布式 actor system 等），需要决定将哪个分区分配给哪个节点。当有新节点加入集群时，需要将某些现有分区从当前节点迁移到新节点，从而实现[[数据密集系统设计/数据密集型系统设计2：复制与分区#2 3 分区与再平衡|负载动态平衡]])。而当节点移除或失败时，其他节点还需要接管失败节点。

上述场景中的任务，可以借助 ZooKeeper 中的原子操作，ephemeral nodes 和通知机制来实现。应用程序最初可能只运行在单节点，之后可能最终扩展到数于节点。试图在如此庞大的集群上进行多数者投票会非常低效。 ZooKeeper 通常是在固定数最的节点（通常三到五个）上运行投票，可以非常高效地支持大量的客户端。因此， ZooKeeper 其实提供了一种将跨节点协调服务（包括共识，操作排序和故障检测）专业外包的方式。

#### 1.3.5.2. 服务发现

此外，ZooKeeper、 etcd 和 Consul 还经常用于服务发现。例如需要某项服务时，应该连接到哪个 IP 地址等。在典型的云环境中，虚拟机可能会起起停停，这种动态变化的节点无法提前知道服务节点的 IP 地址，因此，可以这样配置服务，每当节点启动时将其网络端口信息向 ZooKeeper 等服务注册，然后其他人只需向 ZooKeeper 的注册表中询问即可。

即使服务发现不需要共识，但主节点选举则肯定需要。因此，如果共识系统已经明确知道哪一个是主节点，那它可以利用这些信息来帮助次级服务来发现各自的主节点。现在一些共识系统支持只读的缓存副本。这些副本异步地接收其他共识算法所达成的决议日志，但自身并不怎么参与投票，而主要是提供不需要支持线性化的读取服务。

#### 1.3.5.3. 成员服务

ZooKeeper 等还可以看作是成员服务范畴的一部分。关于成员服务的研究历史可以追溯到 20 世纪 80 年代，它对于构建高可靠的系统（例如空中交管）非常重要。

成员服务用来确定当前哪些节点处于活动状态并属于集群的有效成员。由于无限的网络延迟，无法可靠地检测一个节点究竟是否发生了故降。但是，**可以将故障检测与共识绑定在一起，让所有节点就节点的存活达成一致意见**。

这里依然存在发生误判的可能性，即节点其实处于活动状态却被错误地宣判为故障。即便这样，**系统就成员资格问题的决定是全体一致的，这是最重要的**。例如，选举主节点的方式可能是简单地投票选择编号最小的节点，一且节点对于当前包含哪些成员出现了不同意见，那么共识过程就无法继续。